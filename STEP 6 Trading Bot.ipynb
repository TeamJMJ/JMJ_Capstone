{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# University of Michigan \n",
    "# Master of Applied Data Science\n",
    "### SIAD 699: Capstone\n",
    "Team: James Yoon (jamyoon), Mario Feliciano (felicma), and James Tuccori (jtuccori)\n",
    "\n",
    "Date: April 2023\n",
    "____\n",
    "# Social Media Sentiment & Predicting Trading Signals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this notebook, we take our top performing model (Stacking_LR_model_JT2019to2022TEST_news_google), and then use it to create buy and sell signals(prediction) for us to invest on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "import random\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "import pandas as pd\n",
    "import mplfinance as mpf\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from alpaca_trade_api import REST, Stream\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pytz\n",
    "from dateutil import parser\n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from finta import TA\n",
    "import shap\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import schedule\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import schedule\n",
    "import time\n",
    "import datetime\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import ta\n",
    "from ta.volatility import AverageTrueRange\n",
    "from ta.trend import sma_indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hourly sampling extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make our hourly trading both successful we are implementing an \"Imcremental Data Update/Extract\" strategy. Our overall hourly bot needs about 57 hours of data to be able to successfully predict because our longest moving average is calculated at 57 hour moving average in STEP 2 of our pipeline.  If we extract the last 57 hours each time we need to make a prediction , it wont be possible because it takes about 2 hours and 24 minutes to extract these twitter information.  So this block of code below extracts the last 60 hours of data only once and store that information as  CSV file, which we then call upon this file and add only the last hour of twitter extracts in hourly increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets for the last 60 hours...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:   0%|                                                                                    | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:   2%|â–ˆâ–                                                                        | 1/60 [01:22<1:20:40, 82.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:   3%|â–ˆâ–ˆâ–                                                                      | 2/60 [03:39<1:51:04, 114.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:   5%|â–ˆâ–ˆâ–ˆâ–‹                                                                     | 3/60 [06:03<2:01:42, 128.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                    | 4/60 [08:52<2:14:34, 144.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:   8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                   | 5/60 [11:42<2:20:47, 153.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                 | 6/60 [13:59<2:12:58, 147.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                | 7/60 [16:15<2:07:16, 144.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                               | 8/60 [18:31<2:02:31, 141.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                              | 9/60 [21:10<2:04:47, 146.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 10/60 [23:32<2:01:09, 145.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 11/60 [25:52<1:57:31, 143.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 12/60 [28:11<1:53:55, 142.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                        | 13/60 [30:38<1:52:38, 143.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                       | 14/60 [32:58<1:49:19, 142.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                      | 15/60 [35:10<1:44:24, 139.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                    | 16/60 [37:31<1:42:40, 140.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                   | 17/60 [40:00<1:42:10, 142.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                  | 18/60 [42:39<1:43:13, 147.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                 | 19/60 [45:02<1:39:54, 146.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                | 20/60 [47:19<1:35:34, 143.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 21/60 [49:38<1:32:16, 141.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                             | 22/60 [51:51<1:28:16, 139.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                            | 23/60 [54:02<1:24:29, 137.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                           | 24/60 [56:29<1:23:57, 139.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 25/60 [58:57<1:23:04, 142.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 26/60 [1:01:22<1:21:05, 143.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                      | 27/60 [1:03:50<1:19:32, 144.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                     | 28/60 [1:06:04<1:15:28, 141.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                    | 29/60 [1:08:24<1:12:52, 141.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 30/60 [1:10:51<1:11:25, 142.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 31/60 [1:13:10<1:08:23, 141.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                | 32/60 [1:15:32<1:06:04, 141.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 33/60 [1:18:09<1:05:48, 146.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 34/60 [1:20:38<1:03:45, 147.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 35/60 [1:22:51<59:31, 142.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 36/60 [1:25:14<57:12, 143.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 37/60 [1:27:43<55:27, 144.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 38/60 [1:29:56<51:48, 141.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 39/60 [1:32:24<50:08, 143.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 40/60 [1:34:28<45:49, 137.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 41/60 [1:36:46<43:35, 137.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 42/60 [1:39:12<42:00, 140.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 43/60 [1:41:41<40:30, 142.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 44/60 [1:44:06<38:13, 143.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 45/60 [1:46:28<35:47, 143.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 46/60 [1:49:00<33:57, 145.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 47/60 [1:51:25<31:33, 145.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 48/60 [1:53:56<29:26, 147.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 49/60 [1:56:37<27:43, 151.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 50/60 [1:59:21<25:51, 155.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 51/60 [2:01:48<22:52, 152.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 52/60 [2:04:19<20:18, 152.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 53/60 [2:07:05<18:13, 156.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 54/60 [2:09:31<15:18, 153.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 55/60 [2:11:52<12:28, 149.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/60 [2:14:20<09:56, 149.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 57/60 [2:16:52<07:29, 149.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 58/60 [2:19:18<04:57, 148.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Hours:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/60 [2:21:49<02:29, 149.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hours: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [2:24:21<00:00, 144.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping tweets\n",
      "Wall time: 2h 24min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "##WORKING SO FAR V4\n",
    "PasthoursLongterm = 60\n",
    "start_time = datetime.now() - timedelta(hours=PasthoursLongterm)\n",
    "day1 = start_time.timestamp()\n",
    "\n",
    "number_of_tweets = 480\n",
    "random_sample = 0.8\n",
    "tweets_list2 = []\n",
    "\n",
    "print(\"Scraping tweets for the last 60 hours...\")\n",
    "for j in tqdm(range(PasthoursLongterm), desc=\"Hours\"):\n",
    "    print (j)\n",
    "    x = 0\n",
    "    for tweet in sntwitter.TwitterSearchScraper(\n",
    "        'bitcoin since_time:{} until_time:{}'.format(\n",
    "            int(day1),\n",
    "            int((start_time + timedelta(hours=j+1)).timestamp())\n",
    "        )\n",
    "    ).get_items():\n",
    "        if x == number_of_tweets:\n",
    "            break\n",
    "        if random.random() > random_sample:\n",
    "            tweets_list2.append(\n",
    "                [\n",
    "                    tweet.user.username,\n",
    "                    tweet.rawContent,\n",
    "                    tweet.date,\n",
    "                    tweet.likeCount,\n",
    "                    tweet.retweetCount,\n",
    "                    tweet.user.followersCount,\n",
    "                    tweet.user.friendsCount,\n",
    "                ]\n",
    "            )\n",
    "            x += 1\n",
    "\n",
    "tweets_df_last100hours = pd.DataFrame(tweets_list2, columns=['user', 'tweets', 'time', 'favorites', 'retweets', 'number_of_followers', 'following'])\n",
    "print(\"Finished scraping tweets\")\n",
    "\n",
    "tweets_df_last100hours.to_csv('assets/runs/Last100hours.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>tweets</th>\n",
       "      <th>time</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>number_of_followers</th>\n",
       "      <th>following</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30475</th>\n",
       "      <td>MarketJane</td>\n",
       "      <td>https://t.co/ck6QxiG0Ol\\n\\n#Bitcoin is the bes...</td>\n",
       "      <td>2023-04-04 13:56:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5177</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30476</th>\n",
       "      <td>Bitcoin24ORE</td>\n",
       "      <td>BREAKING: una legge anti-Bitcoin (n 1751) Ã¨ ap...</td>\n",
       "      <td>2023-04-04 13:56:28+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30477</th>\n",
       "      <td>BullSkyRocket</td>\n",
       "      <td>ðŸš¨ðŸš¨  BREAKING  ðŸš¨ðŸš¨\\nRalph Lauren To Accept #Bitc...</td>\n",
       "      <td>2023-04-04 13:56:21+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30478</th>\n",
       "      <td>l_Waqas_l</td>\n",
       "      <td>@OTC_Bitcoin Let me know when you're not sleep...</td>\n",
       "      <td>2023-04-04 13:56:10+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30479</th>\n",
       "      <td>DudeJLebowski</td>\n",
       "      <td>@CHAIRFORCE_BTC @ODELL Sounds like shitcoiners...</td>\n",
       "      <td>2023-04-04 13:56:09+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10665</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user                                             tweets  \\\n",
       "30475     MarketJane  https://t.co/ck6QxiG0Ol\\n\\n#Bitcoin is the bes...   \n",
       "30476   Bitcoin24ORE  BREAKING: una legge anti-Bitcoin (n 1751) Ã¨ ap...   \n",
       "30477  BullSkyRocket  ðŸš¨ðŸš¨  BREAKING  ðŸš¨ðŸš¨\\nRalph Lauren To Accept #Bitc...   \n",
       "30478      l_Waqas_l  @OTC_Bitcoin Let me know when you're not sleep...   \n",
       "30479  DudeJLebowski  @CHAIRFORCE_BTC @ODELL Sounds like shitcoiners...   \n",
       "\n",
       "                            time  favorites  retweets  number_of_followers  \\\n",
       "30475  2023-04-04 13:56:40+00:00          0         0                 5177   \n",
       "30476  2023-04-04 13:56:28+00:00          1         0                  250   \n",
       "30477  2023-04-04 13:56:21+00:00          0         0                  163   \n",
       "30478  2023-04-04 13:56:10+00:00          2         0                  152   \n",
       "30479  2023-04-04 13:56:09+00:00          4         0                10665   \n",
       "\n",
       "       following  \n",
       "30475        502  \n",
       "30476         81  \n",
       "30477         47  \n",
       "30478        528  \n",
       "30479       2756  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df_last100hours.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk9sUWCvT1yy"
   },
   "source": [
    "## 2. Hourly Trading Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tw1YmJ14rng1"
   },
   "source": [
    "#### This code block below runs through the whole pipelines in hourly increments and provides us a prediction in the for either a Buy , Sell or hold signal. \n",
    "\n",
    "#### Starting in code line 865, the code block could be uncommented to link it to a binance account, to allow real time buy/sell training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MULHceaQrl9d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping tweets for the last 1 hour...\n",
      "0\n",
      "Finished scraping tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleTrends Call 1 of 3 : Timeframe: 2023-03-16T00 2023-03-23T00 \n",
      "GoogleTrends Call 2 of 3 : Timeframe: 2023-03-23T00 2023-03-30T00 \n",
      "GoogleTrends Call 3 of 3 : Timeframe: 2023-03-30T00 2023-04-05T00 \n",
      "                  time  Prediction   Close_x\n",
      "0  2023-04-04 21:00:00    0.473491  28201.97\n",
      "2023-04-04 23:49:14.172536\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def run_program():\n",
    "    Pasthours = 1\n",
    "    ##WORKING SO FAR V4\n",
    "    start_time = datetime.now() - timedelta(hours=Pasthours)\n",
    "    day1 = start_time.timestamp()\n",
    "\n",
    "    number_of_tweets = 480\n",
    "    random_sample = 0.8\n",
    "    tweets_list2 = []\n",
    "\n",
    "    print(\"Scraping tweets for the last 1 hour...\")\n",
    "    for j in range(Pasthours):\n",
    "        print (j)\n",
    "        x = 0\n",
    "        for tweet in sntwitter.TwitterSearchScraper(\n",
    "            'bitcoin since_time:{} until_time:{}'.format(\n",
    "                int(day1),\n",
    "                int((start_time + timedelta(hours=j+1)).timestamp())  # Add 1 hour to the start_time\n",
    "            )\n",
    "        ).get_items():\n",
    "            if x == number_of_tweets:\n",
    "                break\n",
    "            if random.random() > random_sample:\n",
    "                tweets_list2.append(\n",
    "                    [\n",
    "                        tweet.user.username,\n",
    "                        tweet.rawContent,\n",
    "                        tweet.date,\n",
    "                        tweet.likeCount,\n",
    "                        tweet.retweetCount,\n",
    "                        tweet.user.followersCount,\n",
    "                        tweet.user.friendsCount,\n",
    "                    ]\n",
    "                )\n",
    "                x += 1\n",
    "\n",
    "    tweets_df_last1hours = pd.DataFrame(tweets_list2, columns=['user', 'tweets', 'time', 'favorites', 'retweets', 'number_of_followers', 'following'])\n",
    "    print(\"Finished scraping tweets\")\n",
    "\n",
    "\n",
    "    columns = ['user', 'tweets', 'time', 'favorites', 'retweets', 'number_of_followers', 'following']\n",
    "\n",
    "    tweets_df_last100hours = pd.read_csv('assets/runs/Last100hoursUpdate.csv')\n",
    "    tweets_df_last100hours = tweets_df_last100hours[columns]\n",
    "    tweets_df_last1hours = tweets_df_last1hours[columns]\n",
    "    df170 = pd.concat([tweets_df_last100hours, tweets_df_last1hours], ignore_index=True)\n",
    "    df170 = df170.drop_duplicates()\n",
    "    df170.to_csv('assets/runs/Last100hoursUpdate.csv')\n",
    "\n",
    "    apikey = 'Tei1cl6Qncsr03oPvcFN0eW5PfaYZGIf7ZtoX6TBKB7G8UvJTzQsYhB92vzi9NdN'\n",
    "    secret = 'kePezzaWGED3MYgzWYldFCfgNg69Tcm7vv1IEvFazKTKrT54fOyyTONualYiHnBF'\n",
    "\n",
    "\n",
    "    #Authenticating\n",
    "\n",
    "    client = Client(apikey, secret)# Get the date 1 week ago from today\n",
    "    one_week_ago = datetime.now() - timedelta(weeks=10)\n",
    "    one_week_ago_str = one_week_ago.strftime(\"%d %b %Y\")\n",
    "\n",
    "    #getting daily data\n",
    "    historical = client.get_historical_klines('BTCUSDT', Client.KLINE_INTERVAL_1HOUR, one_week_ago_str)\n",
    "    Original_df_historical = pd.DataFrame(historical)\n",
    "    df_historical= Original_df_historical.copy()\n",
    "\n",
    "    df_historical.columns = ['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time', 'Quote Asset Volume', \n",
    "                        'Number of Trades', 'TB Base Volume', 'TB Quote Volume', 'Ignore']\n",
    "\n",
    "    #changing time to standard units\n",
    "    df_historical['Open Time'] = pd.to_datetime(df_historical['Open Time']/1000, unit='s')\n",
    "    df_historical['Close Time'] = pd.to_datetime(df_historical['Close Time']/1000, unit='s')\n",
    "    df_historical['Close Time'] = df_historical['Close Time'].values.astype('datetime64[h]')\n",
    "\n",
    "    #changing numbers to integer\n",
    "    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume', 'TB Base Volume', 'TB Quote Volume']\n",
    "    df_historical[numeric_columns] = df_historical[numeric_columns].apply(pd.to_numeric, axis=1)\n",
    "    df_historical2_hourly =df_historical.set_index('Close Time')\n",
    "\n",
    "    #calculating Moving average\n",
    "    # Example 2: \n",
    "    numbers = df_historical2_hourly['Close']\n",
    "    window_size = 3\n",
    "\n",
    "    i = 0\n",
    "    moving_averages = []\n",
    "    for i in range(len(numbers)):\n",
    "        try:\n",
    "    #         this_window = numbers[i : i + window_size]\n",
    "            this_window = numbers[i - window_size: i]\n",
    "    #Get current window\n",
    "            window_average = sum(this_window) / window_size\n",
    "            moving_averages.append(window_average)\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            print(NaN)\n",
    "\n",
    "    #Adding the moving averages column\n",
    "    df_historical2_hourly['3MovingAverage'] = moving_averages\n",
    "\n",
    "\n",
    "    #calculating Moving average\n",
    "    # Example 2: \n",
    "    numbers = df_historical2_hourly['Close']\n",
    "    window_size = 5\n",
    "\n",
    "    i = 0\n",
    "    moving_averages = []\n",
    "    for i in range(len(numbers)):\n",
    "        try:\n",
    "    #         this_window = numbers[i : i + window_size]\n",
    "            this_window = numbers[i - window_size: i]\n",
    "    #Get current window\n",
    "            window_average = sum(this_window) / window_size\n",
    "            moving_averages.append(window_average)\n",
    "            i += 1\n",
    "        except ValueError:\n",
    "            print(NaN)\n",
    "    #print(moving_averages)\n",
    "\n",
    "    #Adding the moving averages column\n",
    "    df_historical2_hourly['5MovingAverage'] = moving_averages\n",
    "\n",
    "\n",
    "    #adding function to calcuate sentiment as postive, negative, or neutral. This is later used to calculate our bullratio score\n",
    "    def getSentiment(score):\n",
    "        if score < 0:\n",
    "            return 'Negative'\n",
    "        elif score == 0:\n",
    "            return 'Neutral'\n",
    "        else:\n",
    "            return 'Positive'\n",
    "\n",
    "\n",
    "    df170['followers_following_ratio'] = df170['number_of_followers'] / df170['following']\n",
    "    df170['2x_retweets_+_favorites'] = 2 * df170['retweets'] + df170['favorites']\n",
    "    df170['polarity'] = df170['tweets'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment.polarity))\n",
    "    df170['sentiment'] = df170['polarity'].apply(getSentiment)\n",
    "    np.seterr(divide = 'ignore') \n",
    "    df170['W1 Score'] = (np.log(df170['followers_following_ratio'])+1)* np.log(df170['2x_retweets_+_favorites'])*(df170['polarity'])\n",
    "    df170 = df170.fillna(0)\n",
    "    df170.replace([np.inf, -np.inf, -0], 0, inplace=True)\n",
    "\n",
    "    df170['time'] = pd.to_datetime(df170['time'], utc=True)\n",
    "\n",
    "    df170['time'] = pd.to_datetime(df170['time'])\n",
    "    df170['hour'] = df170['time'].dt.hour\n",
    "    df170['day'] = pd.to_datetime(df170['time'].dt.date)\n",
    "    df170['time'] = df170['day'] + pd.to_timedelta(df170['hour'], unit='h')\n",
    "\n",
    "\n",
    "    Bullratio_df = df170.groupby('time').sum()\n",
    "\n",
    "\n",
    "\n",
    "    bull_ratio_list = []\n",
    "    Bullratio_df = Bullratio_df.reset_index()\n",
    "\n",
    "    for day in Bullratio_df['time']:\n",
    "      #print(day)\n",
    "        filtered_df = df170.loc[(df170['time'] == day)]\n",
    "        try:\n",
    "            bull_ratio_list.append(filtered_df['sentiment'].value_counts()[1]/filtered_df['sentiment'].value_counts()[2]) #KEY NEED TO CHANGE TO [1] and [2] later\n",
    "        except:\n",
    "            bull_ratio_list.append(filtered_df['sentiment'].value_counts()[0]/filtered_df['sentiment'].value_counts()[0]) #KEY NEED TO CHANGE TO [1] and [2] later\n",
    "\n",
    "\n",
    "    Bullratio_df['Bull_Ratio'] = bull_ratio_list\n",
    "    df170['Bull_ratio'] = df170['time'].map(Bullratio_df.reset_index().set_index('time')['Bull_Ratio'])\n",
    "    df170['W Score With Bull Ratio'] = df170['W1 Score'] * df170['Bull_ratio']\n",
    "\n",
    "    Df_hourly = df170.groupby('time').mean()\n",
    "\n",
    "    Df_hourly_merge = pd.merge(Df_hourly, df_historical2_hourly, how ='left', left_index=True, right_index=True, indicator = True) \n",
    "\n",
    "    #calculating Moving average\n",
    "    # Example 2: \n",
    "    numbers = Df_hourly_merge['W Score With Bull Ratio']\n",
    "    n = 3\n",
    "    x = 0\n",
    "    JMJ_3Hmoving_averages = []\n",
    "\n",
    "    for i in range(len(numbers)):\n",
    "        try:\n",
    "            percentage = ((Df_hourly_merge['W Score With Bull Ratio'][x]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1])))*Df_hourly_merge['Close'][x] + ((Df_hourly_merge['W Score With Bull Ratio'][x-1]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1])))*Df_hourly_merge['Close'][x-1]+ ((Df_hourly_merge['W Score With Bull Ratio'][x-2]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1]))) *Df_hourly_merge['Close'][x-2]     \n",
    "            JMJ_3Hmoving_averages.append(percentage)\n",
    "            x = x + 1\n",
    "\n",
    "        except ValueError:\n",
    "            print(NaN)\n",
    "\n",
    "    #calculating Moving average\n",
    "    # Example 2: \n",
    "    numbers = Df_hourly_merge['W Score With Bull Ratio']\n",
    "    n = 5\n",
    "    x = 0\n",
    "    JMJ_5Hmoving_averages = []\n",
    "\n",
    "    for i in range(len(numbers)):\n",
    "      try:\n",
    "          percentage = ((Df_hourly_merge['W Score With Bull Ratio'][x]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1])))*Df_hourly_merge['Close'][x] + ((Df_hourly_merge['W Score With Bull Ratio'][x-1]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1])))*Df_hourly_merge['Close'][x-1] + ((Df_hourly_merge['W Score With Bull Ratio'][x-2]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1]))) *Df_hourly_merge['Close'][x-2] + ((Df_hourly_merge['W Score With Bull Ratio'][x-3]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1]))) *Df_hourly_merge['Close'][x-3] + ((Df_hourly_merge['W Score With Bull Ratio'][x-4]) / (sum(Df_hourly_merge['W Score With Bull Ratio'][x-n+1:x+1]))) *Df_hourly_merge['Close'][x-4]\n",
    "          JMJ_5Hmoving_averages.append(percentage)\n",
    "          x = x + 1\n",
    "\n",
    "      except ValueError:\n",
    "          print(NaN)\n",
    "\n",
    "\n",
    "    Df_hourly_merge['JMJ_3HMoving_averages'] = JMJ_3Hmoving_averages\n",
    "    Df_hourly_merge['JMJ_5HMoving_averages'] = JMJ_5Hmoving_averages\n",
    "    Df_hourly_merge.replace([np.inf, -np.inf, -0], 0, inplace=True)\n",
    "    # Df_hourly_merge.drop('Unnamed: 0', inplace=True, axis=1) #KEY may or may not need it\n",
    "\n",
    "    Df_hourly_merge['Signal'] = 0.0\n",
    "    Df_hourly_merge['Signal'] = np.where(Df_hourly_merge['3MovingAverage'] >Df_hourly_merge['5MovingAverage'], 1.0, 0.0)\n",
    "    Df_hourly_merge['Position'] = Df_hourly_merge['Signal'].diff()\n",
    "\n",
    "\n",
    "    Df_hourly_merge['Signal35JMJ'] = 0.0\n",
    "    Df_hourly_merge['Signal35JMJ'] = np.where(Df_hourly_merge['JMJ_3HMoving_averages'] >Df_hourly_merge['JMJ_5HMoving_averages'], 1.0, 0.0)\n",
    "    Df_hourly_merge['Position35JMJ'] = Df_hourly_merge['Signal35JMJ'].diff()\n",
    "\n",
    "\n",
    "    prediction = (Df_hourly_merge.shift(-1)['Close'] >= Df_hourly_merge['Close'])\n",
    "    prediction = prediction.iloc[:-1]\n",
    "    Df_hourly_merge['Actual_Label'] = prediction.astype(int)\n",
    "    Df_hourly_merge['Actual_Label'][-1]=3\n",
    "    Df_hourly_merge.to_csv('assets/Final_GO_Live_PCHourly20192022_ActualLabel.csv')                                     \n",
    "\n",
    "    # Calculate the start and end dates for the last day\n",
    "    end_date = datetime.now().date() + timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=5)\n",
    "\n",
    "    # Convert dates to string format\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')                                     \n",
    "\n",
    "\n",
    "    endpoint_url = \"https://paper-api.alpaca.markets\"\n",
    "    api = REST(\"PKN12PE5ALWR43EC685W\", \"qdu7NgYDh6gV2iGaFPIgoboplKKTjv98qlITxW8C\", endpoint_url)\n",
    "\n",
    "    crypto_news = api.get_news([\"BTCUSD\",\"BTC\",\"$BTC\",\"ETHUSD\",\"ETH\",\"$ETH\",\"DOGEUSD\",\"DOGE\",\"$DOGE\"], start_date_str, end_date_str, limit=41000)\n",
    "    mkt_news = api.get_news([\"SPY\",\"NASDAQ\",\"VIX^\",], start_date_str, end_date_str, limit=41000)\n",
    "\n",
    "    crypto_news_list = []\n",
    "\n",
    "    for story in crypto_news:\n",
    "        crypto_news_dict = {}\n",
    "        crypto_news_dict[\"time\"] = story.created_at\n",
    "        crypto_news_dict[\"Crypto Headline\"] = story.headline\n",
    "        crypto_news_list.append(crypto_news_dict)\n",
    "\n",
    "    crypto_news_df = pd.DataFrame(crypto_news_list)\n",
    "\n",
    "    mkt_news_list = []\n",
    "\n",
    "    for story in mkt_news:\n",
    "        mkt_news_dict = {}\n",
    "        mkt_news_dict[\"time\"] = story.created_at\n",
    "        mkt_news_dict[\"Mkt Headline\"] = story.headline\n",
    "        mkt_news_list.append(mkt_news_dict)\n",
    "\n",
    "    mkt_news_df = pd.DataFrame(mkt_news_list)\n",
    "\n",
    "    # Calculate sentiment scores for each headline\n",
    "    crypto_sentiments = []\n",
    "\n",
    "    for headline in crypto_news_df['Crypto Headline']:\n",
    "        blob = TextBlob(str(headline))\n",
    "        crypto_sentiment = blob.sentiment.polarity\n",
    "        crypto_sentiments.append(crypto_sentiment)\n",
    "\n",
    "    crypto_news_df['Crypto Sentiment'] = crypto_sentiments\n",
    "\n",
    "    mkt_sentiments = []\n",
    "\n",
    "    for headline in mkt_news_df['Mkt Headline']:\n",
    "        blob = TextBlob(str(headline))\n",
    "        mkt_sentiment = blob.sentiment.polarity\n",
    "        mkt_sentiments.append(mkt_sentiment)\n",
    "\n",
    "    mkt_news_df['Mkt Sentiment'] = mkt_sentiments\n",
    "\n",
    "    crypto_sentiment_df = crypto_news_df.resample('H', on='time').mean()[['Crypto Sentiment']].fillna(0)\n",
    "    mkt_sentiment_df = mkt_news_df.resample('H', on='time').mean()[['Mkt Sentiment']].fillna(0)\n",
    "\n",
    "    # Merge Dataframes & Filter\n",
    "    market_sentiment_df = pd.merge(mkt_sentiment_df, crypto_sentiment_df, how='outer', on='time').fillna(0)\n",
    "    market_sentiment_train_df = market_sentiment_df\n",
    "\n",
    "    #Good Until here\n",
    "    #days to hours\n",
    "\n",
    "    SMA_st=11\n",
    "    SMA_lt=21\n",
    "    WMA_st=25\n",
    "    WMA_lt=49\n",
    "    EMA_st=29\n",
    "    EMA_lt=57\n",
    "\n",
    "    time_period_1 =SMA_st*24\n",
    "    time_period_2 =SMA_lt*24\n",
    "    time_period_1a =WMA_st*24\n",
    "    time_period_2a =WMA_lt*24\n",
    "    time_period_1b =EMA_st*24\n",
    "    time_period_2b =EMA_lt*24\n",
    "\n",
    "\n",
    "    # BTC_Ticker = yf.Ticker(\"BTC-USD\")\n",
    "    # result_df_sma = BTC_Ticker.history(period=\"max\")   \n",
    "    result_df_sma = df_historical2_hourly.copy()                                  \n",
    "    result_df_sma[\"sma1\"] = result_df_sma[\"Close\"].rolling(window=time_period_1).mean()\n",
    "    result_df_sma[\"sma2\"] = result_df_sma[\"Close\"].rolling(window=time_period_2).mean()                                     \n",
    "\n",
    "    weights_1 = np.arange(1, time_period_1a + 1)\n",
    "    weights_2 = np.arange(1, time_period_2a + 1)\n",
    "\n",
    "    result_df_sma[\"wma1\"] = result_df_sma[\"Close\"].rolling(window=time_period_1a).apply(lambda x: np.sum(x * weights_1) / np.sum(weights_1), raw=True)\n",
    "    result_df_sma[\"wma2\"] = result_df_sma[\"Close\"].rolling(window=time_period_2a).apply(lambda x: np.sum(x * weights_2) / np.sum(weights_2), raw=True)\n",
    "\n",
    "\n",
    "    result_df_sma[\"ema1\"] = result_df_sma[\"Close\"].ewm(span=time_period_1b, adjust=False).mean()\n",
    "    result_df_sma[\"ema2\"] = result_df_sma[\"Close\"].ewm(span=time_period_2b, adjust=False).mean()\n",
    "\n",
    "    # result_df_sma = result_df_sma.drop(['Dividends', 'Stock Splits'], axis=1)\n",
    "\n",
    "    Final_Dataframe = result_df_sma.rename(columns={\n",
    "                                          \"sma1\": \"Historically Optimal SMA(s-t)\", \"sma2\": \"Historically Optimal SMA(l-t)\", \n",
    "                                         \"wma1\": \"Historically Optimal WMA(s-t)\", \"wma2\": \"Historically Optimal WMA(l-t)\", \n",
    "                                          \"ema1\": \"Historically Optimal EMA(s-t)\", \"ema2\": \"Historically Optimal EMA(l-t)\" \n",
    "                                         })\n",
    "\n",
    "    Final_Dataframe\n",
    "\n",
    "    # # resample the data to hourly frequency and interpolate missing values\n",
    "    # Final_Dataframe_hourly = Final_Dataframe.resample('H').interpolate()\n",
    "\n",
    "    # # calculate the hourly moving averages using a rolling window\n",
    "    # Final_Dataframe_hourly_ma = Final_Dataframe_hourly.rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "    # # remove the first 23 rows as they only have partial data for the first day\n",
    "    # Final_Dataframe_hourly_ma = Final_Dataframe_hourly_ma.iloc[23:]\n",
    "\n",
    "    df_1 = Final_Dataframe.reset_index()\n",
    "    df_1['Open Time'] = pd.to_datetime(df_1['Open Time']).dt.tz_localize('UTC')\n",
    "\n",
    "    filename = \"assets/Final_GO_Live_PCHourly20192022_ActualLabel.csv\"\n",
    "    df_2 = pd.read_csv(filename)\n",
    "    df_2 = df_2.reset_index()\n",
    "    df_2['time'] = pd.to_datetime(df_2['time']).dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "    #Merge the two dataframes\n",
    "    merged_df = pd.merge(df_1, df_2, left_on='Open Time', right_on='time')\n",
    "    merged_df = merged_df.drop(['Close Time','Open Time_y', 'Open_y', 'High_y', 'Low_y', 'Close_y', 'Volume_y',\n",
    "           'Quote Asset Volume_y', 'Number of Trades_y', 'TB Base Volume_y',\n",
    "           'TB Quote Volume_y', 'Ignore_y','Ignore_x', '3MovingAverage_y', '5MovingAverage_y'], axis=1)\n",
    "\n",
    "    merged_train_df = merged_df.rename(columns={'Open Time_x': 'Datetime','Open_x': 'Open', 'High_x': 'High', 'Low_x': 'Low', \n",
    "                 'Close_x': 'Close', 'Volume_x': 'Volume','Quote Asset Volume_x':'Quote Asset Volume', 'Signal': 'SMA Signal 35', \n",
    "                 'Signal35JMJ': 'JMJ Signal 35', 'Number of Trades_x': '# of Hourly Trades', 'TB Base Volume_x':'TB Base Volume',\n",
    "                  'TB Quote Volume_x':'TB Quote Volume',                             \n",
    "                 'JMJ_3HMoving_averages': 'JMJ 3 Hour MA', 'JMJ_5HMoving_averages': 'JMJ 5 Hour MA', \n",
    "                 'favorites': 'Twitter Hourly Favorites', 'retweets': 'Twitter Hourly Retweets', \n",
    "                 'number_of_followers': 'Twitter Hourly Follower Exposure', 'following': 'Twitter Hourly Following Exposure',\n",
    "                'followers_following_ratio': 'Twitter Hourly Follower to Following Ratio', '2x_retweets_+_favorites': \n",
    "                 'Twitter Hourly 2x Retweets + Favorites', 'polarity': 'Twitter Hourly Polarity Score', 'W1 Score': 'Twitter W1 Score',\n",
    "                'Bull_ratio': 'Twitter Hourly Bull Ratio', 'W Score With Bull Ratio': 'Twitter Hourly Weighted Bull Ratio', '3MovingAverage_x':\n",
    "                '3 Hour MA', '5MovingAverage_x': '5 Hour MA', 'Actual_Label': \"Actual Label\", 'Position': 'SMA Position 35', 'Position35JMJ': 'JMJ Position 35'})\n",
    "    merged_train_df = pd.merge(merged_train_df, market_sentiment_train_df, how='left', left_on='Datetime', right_on='time')\n",
    "\n",
    "    last_valid_index_mkt = merged_train_df['Mkt Sentiment'].last_valid_index()\n",
    "    last_valid_index_crypto = merged_train_df['Crypto Sentiment'].last_valid_index()\n",
    "\n",
    "    last_valid_index = max(last_valid_index_mkt, last_valid_index_crypto)\n",
    "\n",
    "    merged_train_df.loc[last_valid_index + 1:, ['Mkt Sentiment', 'Crypto Sentiment']] = merged_train_df.loc[last_valid_index + 1:, ['Mkt Sentiment', 'Crypto Sentiment']].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_moving_averages(df, columns, SMA_st, SMA_lt, WMA_st, WMA_lt, EMA_st, EMA_lt):\n",
    "        df_copy = df.copy() \n",
    "        for column in columns:\n",
    "            # Calculate moving average\n",
    "            df_copy[column+' SMA(s-t)'] = df_copy[column].rolling(window=(SMA_st)).mean()\n",
    "            df_copy[column+' SMA(l-t)'] = df_copy[column].rolling(window=(SMA_lt)).mean()\n",
    "\n",
    "            # Calculate weighted moving average\n",
    "            weights = np.arange(1, WMA_st+1)\n",
    "            df_copy[column+' WMA(s-t)'] = df_copy[column].rolling(window=(WMA_st)).apply(lambda x: np.dot(x, weights)/weights.sum(), raw=True)\n",
    "            weights = np.arange(1, WMA_lt+1)\n",
    "            df_copy[column+' WMA(l-t)'] = df_copy[column].rolling(window=(WMA_lt)).apply(lambda x: np.dot(x, weights)/weights.sum(), raw=True)\n",
    "\n",
    "            # Calculate exponential moving average\n",
    "            df_copy[column+' EMA(s-t)'] = df_copy[column].ewm(span=(EMA_st), adjust=False).mean()\n",
    "            df_copy[column+' EMA(l-t)'] = df_copy[column].ewm(span=(EMA_lt), adjust=False).mean()\n",
    "\n",
    "        return df_copy    \n",
    "\n",
    "    columns = ['Twitter Hourly Favorites',\n",
    "           'Twitter Hourly Retweets', 'Twitter Hourly Follower Exposure',\n",
    "           'Twitter Hourly Following Exposure',\n",
    "           'Twitter Hourly Follower to Following Ratio',\n",
    "           'Twitter Hourly 2x Retweets + Favorites',\n",
    "           'Twitter Hourly Polarity Score', 'Twitter W1 Score',\n",
    "           'Twitter Hourly Bull Ratio', 'Twitter Hourly Weighted Bull Ratio',\n",
    "           'Quote Asset Volume', '# of Hourly Trades', 'TB Base Volume',\n",
    "           'TB Quote Volume']\n",
    "\n",
    "    MA_train_DF = calculate_moving_averages(merged_train_df,columns, SMA_st, SMA_lt, WMA_st, WMA_lt, EMA_st, EMA_lt)\n",
    "\n",
    "    MA_train_DF.to_csv('assets/Final_Go_Live_Jesus_MA_train_dataframe.csv')\n",
    "\n",
    "\n",
    "    requests_args = {'headers': {\n",
    "        'authority': 'trends.google.com',\n",
    "        'method': 'GET',\n",
    "        'path': '/trends/api/autocomplete/BTC%20Bitcoin?hl=en-US&tz=420',\n",
    "        'scheme': 'https',\n",
    "        'accept': 'application/json, text/plain, */*',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'en-US,en;q=0.9,ko;q=0.8',\n",
    "        'cookie':'__utmc=10102256; __utmz=10102256.1679329027.1.1.utmcsr=trends.google.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utma=10102256.1496328958.1679329018.1679329027.1679329027.2; __utmt=1; __utmb=10102256.11.9.1679336351095; S=billing-ui-v3=pYYo7r79iXi-TZTh2uUj15DOGcTJrAws:billing-ui-v3-efe=pYYo7r79iXi-TZTh2uUj15DOGcTJrAws; __Secure-ENID=6.SE=flR0TrAbRYorkBrH_5FaKbGaC4eZvnQ0YV4cD9BZHC6M9N_ivtbL2cppn-2MZz9HutuIp_vUPDCzZ63XqJKAUHM49e7IZpaRRB1VKJRTB7w3U2poH9-LzgaFOtQeN9rlHHqgIDwHKHw7lW_Bw9ZOG2dqkdlid24uevth0tD23m_x42O1QOa5ffAHfDkViUGyrfKnyvyH2_hvtsTpyXoqvl2riS3K0GdsFVFmy113H-fWqJ8y4FNuqek16Bfw-ZHDk8Ni-Nln; ab.storage.deviceId.240e177d-4779-41c2-b484-3af37ffa8685=%7B%22g%22%3A%223f51bc2b-caa1-0f11-c497-746247a5f893%22%2C%22c%22%3A1678796013500%2C%22l%22%3A1678796013500%7D; SID=UQibUU0kksn5OWD6mOj4eVEi0VAphmoHoZvBUeJPUUBeTOHebW0faNP6Eukz0zkGHvp_QA.; __Secure-1PSID=UQibUU0kksn5OWD6mOj4eVEi0VAphmoHoZvBUeJPUUBeTOHeqtrfQSOugwhU9s5bk3dzyg.; __Secure-3PSID=UQibUU0kksn5OWD6mOj4eVEi0VAphmoHoZvBUeJPUUBeTOHef930e9OFbmwpbP-Yv2fKtA.; HSID=AMurgpyEkMg3Ewx9m; SSID=AhmJKGdjfaEjiJ5nZ; APISID=fDLYhRj3xasSGf5q/Ad1BdfZ2k2fOmltI8; SAPISID=Y6UjmWzruWzZmgT7/AE9ynEOWFAq-gJSTA; __Secure-1PAPISID=Y6UjmWzruWzZmgT7/AE9ynEOWFAq-gJSTA; __Secure-3PAPISID=Y6UjmWzruWzZmgT7/AE9ynEOWFAq-gJSTA; AEC=ARSKqsLvV86pgpia_Fui1LWKv-UWOM8xYLpt_dgafcs0X2yvdMnmWqujTg; SEARCH_SAMESITE=CgQI7JcB; _gid=GA1.3.575088293.1679329020; OTZ=6950417_84_88_104280_84_446940; NID=511=Vx7_pxhLT04OEwBWEBi5lQv2D4DuN2dVetQAteO_n0rfBn5w_8DSUp52HJhUUYm25ElyMVZAfmBvb-46sSRcdpJMpfRQOMRP___c4ZUn2fNSnbzvL1dezF4IPWd6G0WBOU-MwNxfGEHEfvzrLWAnZmnguulJd_5GNA2FMUxpHp8v5J74mMB2vECbvjh4xqBTx2ZbGb3DpTCurQuGijF-d08TIXyIyXr2NLBQgijSfJudwUiD1eUvPKYvyXWid7dL8XAzrl9ZihhYay5V4fYu_4pl4ZJgYFD8wdaZh0GUMfL1ByJh5uZjSlcktN-DzFNmaAzilTeE-qsWC3_AYXX_JCl0qtp1F8kzV4dFzzWryK_1cEnb6ehjFdO_RN6ac7McPsiPv6XJrFj9uhfEew; 1P_JAR=2023-3-20-18; _ga=GA1.3.1496328958.1679329018; _gat_gtag_UA_4401283=1; _ga_VWZPXDNJJB=GS1.1.1679335973.2.1.1679336393.0.0.0; SIDCC=AFvIBn9qaKwR_XVYdzqts09MTAMaYTUaTMvtbcG0EiOQH5JnqgLD-BnE1RLYCJIZYdavxbzROw; __Secure-1PSIDCC=AFvIBn8XCUaH5nBAAB8Gw5VOOfZDFeOHkiXzImYfcFEACQ_I3S1nAcAiHh_DEukR9UcF5SjwSNs; __Secure-3PSIDCC=AFvIBn_Iq0jwOb1MB3JgagxDjxWkw2wM4Q3-uE4Inahw31SC7jIXVWR5BqIT65rgnCeqguOIEPmf',\n",
    "        'referer': 'https://trends.google.com/trends/explore?date=2019-01-01%202022-10-01&q=BTC%20Bitcoin&hl=en',\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"111\", \"Not(A:Brand\";v=\"8\", \"Chromium\";v=\"111\"',\n",
    "        'sec-ch-ua-arch': '\"arm\"',\n",
    "        'sec-ch-ua-bitness': '\"64\"',\n",
    "        'sec-ch-ua-full-version': '\"111.0.5563.64\"',\n",
    "        'sec-ch-ua-full-version-list': '\"Google Chrome\";v=\"111.0.5563.64\", \"Not(A:Brand\";v=\"8.0.0.0\", \"Chromium\";v=\"111.0.5563.64\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-model': '',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'sec-ch-ua-platform-version': '\"12.0.1\"',\n",
    "        'sec-ch-ua-wow64': '?0',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "        'x-client-data': 'CLSVywE='}\n",
    "    }\n",
    "\n",
    "    \"\"\"Specify start and end date as well es the required keyword for your query\"\"\"\n",
    "    # end_date = datetime.now().date()\n",
    "    end_date = datetime.now().date() + timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=20)\n",
    "    keyword_list = [\"Bitcoin\", \"BTC\"] # If you add a second string, minor adjustments in the code have to be made\n",
    "\n",
    "    \"\"\"Since we want weekly data for our query, we will create lists which include \n",
    "    the weekly start and end date in the specified timeframe - 2018.01.01 to 2019.5.01\"\"\"\n",
    "\n",
    "    weekly_date_list = []\n",
    "\n",
    "    #Adds the start date as first entry in our weekly_date_list\n",
    "    start_date_temp = start_date\n",
    "    weekly_date_list.append(start_date_temp)\n",
    "\n",
    "    #This will return in list of weekly datetime.date objects - except the end date\n",
    "    while start_date_temp + timedelta(days=7) <= end_date:\n",
    "        start_date_temp += timedelta(days=7)\n",
    "        weekly_date_list.append(start_date_temp)\n",
    "\n",
    "    #This will add the end date to the weekly_date list. We now have a complete list in the specified timeframe\n",
    "    if start_date_temp + timedelta(days=7) > end_date:\n",
    "        weekly_date_list.append(end_date)\n",
    "\n",
    "\n",
    "    \"\"\"Now we can start to downloading the data via Google Trends API\n",
    "    therefore we have to specify a key which includes the start date\n",
    "    and the end-date with T00 as string for hourly data request\"\"\"\n",
    "\n",
    "    #This List will contain pandas Dataframes of weekly data with the features \"date\",\n",
    "    #\"keyword\"(which contains weekly scaling bettwen 0 and 100), \"isPartial\".\n",
    "    #Up to this point, the scaling is not correct.\n",
    "    interest_list = []\n",
    "\n",
    "    #Here we download the data and print the current status of the process\n",
    "    for i in range(len(weekly_date_list)-1):\n",
    "        requests_args = {'headers': {\n",
    "        'authority': 'trends.google.com',\n",
    "        'method': 'GET',\n",
    "        'path': '/trends/api/autocomplete/BTC%20Bitcoin?hl=en-US&tz=420',\n",
    "        'scheme': 'https',\n",
    "        'accept': 'application/json, text/plain, */*',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'en-US,en;q=0.9,ko;q=0.8',\n",
    "        'cookie':'__utmc=10102256; __utmz=10102256.1679329027.1.1.utmcsr=trends.google.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utma=10102256.1496328958.1679329018.1679329027.1679329027.2; __utmt=1; __utmb=10102256.11.9.1679336351095; S=billing-ui-v3=pYYo7r79iXi-TZTh2uUj15DOGcTJrAws:billing-ui-v3-efe=pYYo7r79iXi-TZTh2uUj15DOGcTJrAws; __Secure-ENID=6.SE=flR0TrAbRYorkBrH_5FaKbGaC4eZvnQ0YV4cD9BZHC6M9N_ivtbL2cppn-2MZz9HutuIp_vUPDCzZ63XqJKAUHM49e7IZpaRRB1VKJRTB7w3U2poH9-LzgaFOtQeN9rlHHqgIDwHKHw7lW_Bw9ZOG2dqkdlid24uevth0tD23m_x42O1QOa5ffAHfDkViUGyrfKnyvyH2_hvtsTpyXoqvl2riS3K0GdsFVFmy113H-fWqJ8y4FNuqek16Bfw-ZHDk8Ni-Nln; ab.storage.deviceId.240e177d-4779-41c2-b484-3af37ffa8685=%7B%22g%22%3A%223f51bc2b-caa1-0f11-c497-746247a5f893%22%2C%22c%22%3A1678796013500%2C%22l%22%3A1678796013500%7D; SID=UQibUU0kksn5OWD6mOj4eVEi0VAphmoHoZvBUeJPUUBeTOHebW0faNP6Eukz0zkGHvp_QA.; __Secure-1PSID=UQibUU0kksn5OWD6mOj4eVEi0VAphmoHoZvBUeJPUUBeTOHeqtrfQSOugwhU9s5bk3dzyg.; __Secure-3PSID=UQibUU0kksn5OWD6mOj4eVEi0VAphmoHoZvBUeJPUUBeTOHef930e9OFbmwpbP-Yv2fKtA.; HSID=AMurgpyEkMg3Ewx9m; SSID=AhmJKGdjfaEjiJ5nZ; APISID=fDLYhRj3xasSGf5q/Ad1BdfZ2k2fOmltI8; SAPISID=Y6UjmWzruWzZmgT7/AE9ynEOWFAq-gJSTA; __Secure-1PAPISID=Y6UjmWzruWzZmgT7/AE9ynEOWFAq-gJSTA; __Secure-3PAPISID=Y6UjmWzruWzZmgT7/AE9ynEOWFAq-gJSTA; AEC=ARSKqsLvV86pgpia_Fui1LWKv-UWOM8xYLpt_dgafcs0X2yvdMnmWqujTg; SEARCH_SAMESITE=CgQI7JcB; _gid=GA1.3.575088293.1679329020; OTZ=6950417_84_88_104280_84_446940; NID=511=Vx7_pxhLT04OEwBWEBi5lQv2D4DuN2dVetQAteO_n0rfBn5w_8DSUp52HJhUUYm25ElyMVZAfmBvb-46sSRcdpJMpfRQOMRP___c4ZUn2fNSnbzvL1dezF4IPWd6G0WBOU-MwNxfGEHEfvzrLWAnZmnguulJd_5GNA2FMUxpHp8v5J74mMB2vECbvjh4xqBTx2ZbGb3DpTCurQuGijF-d08TIXyIyXr2NLBQgijSfJudwUiD1eUvPKYvyXWid7dL8XAzrl9ZihhYay5V4fYu_4pl4ZJgYFD8wdaZh0GUMfL1ByJh5uZjSlcktN-DzFNmaAzilTeE-qsWC3_AYXX_JCl0qtp1F8kzV4dFzzWryK_1cEnb6ehjFdO_RN6ac7McPsiPv6XJrFj9uhfEew; 1P_JAR=2023-3-20-18; _ga=GA1.3.1496328958.1679329018; _gat_gtag_UA_4401283=1; _ga_VWZPXDNJJB=GS1.1.1679335973.2.1.1679336393.0.0.0; SIDCC=AFvIBn9qaKwR_XVYdzqts09MTAMaYTUaTMvtbcG0EiOQH5JnqgLD-BnE1RLYCJIZYdavxbzROw; __Secure-1PSIDCC=AFvIBn8XCUaH5nBAAB8Gw5VOOfZDFeOHkiXzImYfcFEACQ_I3S1nAcAiHh_DEukR9UcF5SjwSNs; __Secure-3PSIDCC=AFvIBn_Iq0jwOb1MB3JgagxDjxWkw2wM4Q3-uE4Inahw31SC7jIXVWR5BqIT65rgnCeqguOIEPmf',\n",
    "        'referer': 'https://trends.google.com/trends/explore?date=2019-01-01%202022-10-01&q=BTC%20Bitcoin&hl=en',\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"111\", \"Not(A:Brand\";v=\"8\", \"Chromium\";v=\"111\"',\n",
    "        'sec-ch-ua-arch': '\"arm\"',\n",
    "        'sec-ch-ua-bitness': '\"64\"',\n",
    "        'sec-ch-ua-full-version': '\"111.0.5563.64\"',\n",
    "        'sec-ch-ua-full-version-list': '\"Google Chrome\";v=\"111.0.5563.64\", \"Not(A:Brand\";v=\"8.0.0.0\", \"Chromium\";v=\"111.0.5563.64\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-model': '',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'sec-ch-ua-platform-version': '\"12.0.1\"',\n",
    "        'sec-ch-ua-wow64': '?0',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'sec-fetch-mode': 'cors',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "        'x-client-data': 'CLSVywE='}\n",
    "        }\n",
    "\n",
    "        key =str(weekly_date_list[i])+\"T00 \"+str(weekly_date_list[i+1])+\"T00\"\n",
    "        p = TrendReq(requests_args=requests_args)\n",
    "        p.build_payload(kw_list=keyword_list, timeframe=key)\n",
    "        interest=p.interest_over_time()\n",
    "        interest_list.append(interest)\n",
    "        print(\"GoogleTrends Call {} of {} : Timeframe: {} \"\\\n",
    "              .format(i+1, len(weekly_date_list)-1, key )\n",
    "              )   \n",
    "\n",
    "    df = pd.concat(interest_list)\n",
    "    df.drop(labels = \"isPartial\", axis = 1, inplace = True)\n",
    "    df = df.reset_index()\n",
    "    df.rename(columns={\"date\": \"Datetime\", \"Bitcoin\": \"Bitcoin_Google_Trend_Score\", \"BTC\": \"BTC_Google_Trend_Score\"}, inplace=True)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "\n",
    "    df_train = df\n",
    "\n",
    "\n",
    "    news_train_df = pd.read_csv('assets/Final_Go_Live_Jesus_MA_train_dataframe.csv')\n",
    "    news_train_df['Datetime'] = news_train_df['Datetime'].str[:-6]\n",
    "    news_train_df['Datetime'] = pd.to_datetime(news_train_df['Datetime'])\n",
    "    # news_train_df['Datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # news_train_df['Datetime'].dt.tz_localize(None)\n",
    "    news_train_df.set_index('Datetime', inplace=True)\n",
    "\n",
    "\n",
    "    df_train_merge = pd.merge(df_train, news_train_df, how ='left', left_index=True, right_index=True)\n",
    "\n",
    "    df_train_merge.to_csv('assets/Final_Go_Live_Jesus_google_trends_train_jan_01_2019_to_nov_30_2021.csv')\n",
    "\n",
    "\n",
    "    # Extracting milestone 2 dataframe\n",
    "    Df_hourly_merge = pd.read_csv('assets/Final_GO_Live_PCHourly20192022_ActualLabel.csv')\n",
    "\n",
    "    #Ensuring we have dat and time correct\n",
    "    Df_hourly_merge['time'] = pd.to_datetime(Df_hourly_merge['time'])\n",
    "    Df_hourly_merge =Df_hourly_merge.reset_index().set_index('time')\n",
    "\n",
    "    Df_hourly_merge = Df_hourly_merge[['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "           'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "           'W1 Score', 'hour', 'Bull_ratio', 'W Score With Bull Ratio',\n",
    "           'Open Time', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "           'Quote Asset Volume', 'Number of Trades', 'TB Base Volume',\n",
    "           'TB Quote Volume', 'Ignore', '3MovingAverage', '5MovingAverage',\n",
    "           'JMJ_3HMoving_averages', 'JMJ_5HMoving_averages', 'Signal',\n",
    "           'Position', 'Signal35JMJ', 'Position35JMJ', 'Actual_Label']]\n",
    "\n",
    "    Df_JT = pd.read_csv('assets/Final_Go_Live_Jesus_google_trends_train_jan_01_2019_to_nov_30_2021.csv')\n",
    "    #Convert date into hourly\n",
    "    Df_JT['time'] = pd.to_datetime(Df_JT['Datetime'])\n",
    "    Df_JT['hour'] = Df_JT['time'].dt.hour\n",
    "    Df_JT['day'] = pd.to_datetime(Df_JT['time'].dt.date)\n",
    "    Df_JT['time'] = Df_JT['day'] + pd.to_timedelta(Df_JT['hour'], unit='h')\n",
    "    Df_JT =Df_JT.reset_index().set_index('time')\n",
    "\n",
    "    Df_JT = Df_JT[['Bitcoin_Google_Trend_Score',\n",
    "           'BTC_Google_Trend_Score','Mkt Sentiment',\n",
    "           'Crypto Sentiment','Historically Optimal SMA(s-t)',\n",
    "           'Historically Optimal SMA(l-t)', \n",
    "           'Historically Optimal WMA(s-t)', 'Historically Optimal WMA(l-t)',\n",
    "            'Historically Optimal EMA(s-t)',\n",
    "           'Historically Optimal EMA(l-t)',  'Twitter Hourly Favorites SMA(s-t)',\n",
    "           'Twitter Hourly Favorites SMA(l-t)',\n",
    "           'Twitter Hourly Favorites WMA(s-t)',\n",
    "           'Twitter Hourly Favorites WMA(l-t)',\n",
    "           'Twitter Hourly Favorites EMA(s-t)',\n",
    "           'Twitter Hourly Favorites EMA(l-t)',\n",
    "           'Twitter Hourly Retweets SMA(s-t)',\n",
    "           'Twitter Hourly Retweets SMA(l-t)',\n",
    "           'Twitter Hourly Retweets WMA(s-t)',\n",
    "           'Twitter Hourly Retweets WMA(l-t)',\n",
    "           'Twitter Hourly Retweets EMA(s-t)',\n",
    "           'Twitter Hourly Retweets EMA(l-t)',\n",
    "           'Twitter Hourly Follower Exposure SMA(s-t)',\n",
    "           'Twitter Hourly Follower Exposure SMA(l-t)',\n",
    "           'Twitter Hourly Follower Exposure WMA(s-t)',\n",
    "           'Twitter Hourly Follower Exposure WMA(l-t)',\n",
    "           'Twitter Hourly Follower Exposure EMA(s-t)',\n",
    "           'Twitter Hourly Follower Exposure EMA(l-t)',\n",
    "           'Twitter Hourly Following Exposure SMA(s-t)',\n",
    "           'Twitter Hourly Following Exposure SMA(l-t)',\n",
    "           'Twitter Hourly Following Exposure WMA(s-t)',\n",
    "           'Twitter Hourly Following Exposure WMA(l-t)',\n",
    "           'Twitter Hourly Following Exposure EMA(s-t)',\n",
    "           'Twitter Hourly Following Exposure EMA(l-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio SMA(s-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio SMA(l-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio WMA(s-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio WMA(l-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio EMA(s-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio EMA(l-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites SMA(s-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites SMA(l-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites WMA(s-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites WMA(l-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites EMA(s-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites EMA(l-t)',\n",
    "           'Twitter Hourly Polarity Score SMA(s-t)',\n",
    "           'Twitter Hourly Polarity Score SMA(l-t)',\n",
    "           'Twitter Hourly Polarity Score WMA(s-t)',\n",
    "           'Twitter Hourly Polarity Score WMA(l-t)',\n",
    "           'Twitter Hourly Polarity Score EMA(s-t)',\n",
    "           'Twitter Hourly Polarity Score EMA(l-t)',\n",
    "           'Twitter W1 Score SMA(s-t)', 'Twitter W1 Score SMA(l-t)',\n",
    "           'Twitter W1 Score WMA(s-t)', 'Twitter W1 Score WMA(l-t)',\n",
    "           'Twitter W1 Score EMA(s-t)', 'Twitter W1 Score EMA(l-t)',\n",
    "           'Twitter Hourly Bull Ratio SMA(s-t)',\n",
    "           'Twitter Hourly Bull Ratio SMA(l-t)',\n",
    "           'Twitter Hourly Bull Ratio WMA(s-t)',\n",
    "           'Twitter Hourly Bull Ratio WMA(l-t)',\n",
    "           'Twitter Hourly Bull Ratio EMA(s-t)',\n",
    "           'Twitter Hourly Bull Ratio EMA(l-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio SMA(s-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio SMA(l-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio WMA(s-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio WMA(l-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio EMA(s-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio EMA(l-t)',\n",
    "           'Quote Asset Volume SMA(s-t)', 'Quote Asset Volume SMA(l-t)',\n",
    "           'Quote Asset Volume WMA(s-t)', 'Quote Asset Volume WMA(l-t)',\n",
    "           'Quote Asset Volume EMA(s-t)', 'Quote Asset Volume EMA(l-t)',\n",
    "           '# of Hourly Trades SMA(s-t)', '# of Hourly Trades SMA(l-t)',\n",
    "           '# of Hourly Trades WMA(s-t)', '# of Hourly Trades WMA(l-t)',\n",
    "           '# of Hourly Trades EMA(s-t)', '# of Hourly Trades EMA(l-t)',\n",
    "           'TB Base Volume SMA(s-t)', 'TB Base Volume SMA(l-t)',\n",
    "           'TB Base Volume WMA(s-t)', 'TB Base Volume WMA(l-t)',\n",
    "           'TB Base Volume EMA(s-t)', 'TB Base Volume EMA(l-t)',\n",
    "           'TB Quote Volume SMA(s-t)', 'TB Quote Volume SMA(l-t)',\n",
    "           'TB Quote Volume WMA(s-t)', 'TB Quote Volume WMA(l-t)',\n",
    "           'TB Quote Volume EMA(s-t)', 'TB Quote Volume EMA(l-t)']]\n",
    "\n",
    "    Df_hourly_mergewithJT = pd.merge(Df_hourly_merge, Df_JT, how ='left', left_index=True, right_index=True, indicator = True) \n",
    "\n",
    "    #Removing all Nana Columns to avoid errors later in Data modelling\n",
    "    #Df_hourly_mergewithJT = Df_hourly_mergewithJT.dropna() #Key removedropna\n",
    "\n",
    "    Df_hourly_mergewithJT.to_csv('assets/Final_Go_Live_Jesus_MergeJMJPCHourly2019202_NewsGoogleApi_ActualLabel.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## Prediction:\n",
    "    INDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']\n",
    "\n",
    "\n",
    "    testdata2022 ='assets/Final_Go_Live_Jesus_MergeJMJPCHourly2019202_NewsGoogleApi_ActualLabel.csv'\n",
    "\n",
    "\n",
    "    files= [testdata2022]\n",
    "\n",
    "    pkl_filename = 'model/Go_live_Stacking_LR_model_Final_80_20.pkl'\n",
    "    with open(pkl_filename, 'rb') as file:\n",
    "        Model2 = pickle.load(file)\n",
    "\n",
    "\n",
    "    def _exponential_smooth(data, alpha):\n",
    "        \"\"\"\n",
    "        Function that exponentially smooths dataset so values are less 'rigid'\n",
    "        :param alpha: weight factor to weight recent values more\n",
    "        \"\"\"\n",
    "\n",
    "        return data.ewm(alpha=alpha).mean()\n",
    "\n",
    "    def _get_indicator_data(data):\n",
    "        \"\"\"\n",
    "        Function that uses the finta API to calculate technical indicators used as the features\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        for indicator in INDICATORS:\n",
    "            ind_data = eval('TA.' + indicator + '(data)')\n",
    "            if not isinstance(ind_data, pd.DataFrame):\n",
    "                ind_data = ind_data.to_frame()\n",
    "            data = data.merge(ind_data, left_index=True, right_index=True)\n",
    "        data.rename(columns={\"14 period EMV.\": '14 period EMV'}, inplace=True)\n",
    "\n",
    "        # Also calculate moving averages for features\n",
    "        data['ema50'] = data['Close'] / data['Close'].ewm(50).mean()\n",
    "        data['ema21'] = data['Close'] / data['Close'].ewm(21).mean()\n",
    "        data['ema15'] = data['Close'] / data['Close'].ewm(14).mean()\n",
    "        data['ema5'] = data['Close'] / data['Close'].ewm(5).mean()\n",
    "\n",
    "        # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "        data['normVol'] = data['Volume'] / data['Volume'].ewm(5).mean()\n",
    "\n",
    "        return data\n",
    "\n",
    "    UpperThreshold = 0.5\n",
    "    LowerThresshold = 0.5\n",
    "\n",
    "    for file,x in zip(files, ['testdata2022']):\n",
    "        PredictDF= pd.read_csv(file)\n",
    "    #     prediction = (PredictDF.shift(-1)['Close'] >= PredictDF['Close'])\n",
    "    #     prediction = prediction.iloc[:-1]\n",
    "    #     PredictDF['Actual_Label'] = prediction.astype(int)\n",
    "    #     PredictDF= PredictDF.dropna()\n",
    "\n",
    "        Indicatordata = _exponential_smooth(PredictDF[['Close', 'Open','High','Low','Volume']], 0.65)\n",
    "\n",
    "        Indicatordatafinal = _get_indicator_data(Indicatordata)\n",
    "\n",
    "        Indicatordatafinal = Indicatordatafinal.drop(['Close', 'Open', 'High', 'Low', 'Volume'], axis = 1)\n",
    "        PredictDF = pd.merge(PredictDF, Indicatordatafinal, left_index=True, right_index=True)\n",
    "        PredictDF = PredictDF.drop(['time','hour','Open Time','_merge','Signal','Position', 'Signal35JMJ', 'Position35JMJ', 'Ignore'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        columns = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "               'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "               'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open', 'High',\n",
    "               'Low', 'Close', 'Volume', 'Quote Asset Volume', 'Number of Trades',\n",
    "               'TB Base Volume', 'TB Quote Volume', '3MovingAverage', '5MovingAverage',\n",
    "               'JMJ_3HMoving_averages', 'JMJ_5HMoving_averages', \n",
    "               '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "               '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV', '20 period CCI',\n",
    "               '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21', 'ema15', 'ema5']\n",
    "\n",
    "\n",
    "        def binary(value):\n",
    "            if value > 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        for column in columns: \n",
    "            PredictDF['Binary{}'.format(column)]  = (PredictDF[column] - PredictDF[column].shift(1)).apply(binary)\n",
    "\n",
    "\n",
    "        PredictDF = PredictDF.dropna()\n",
    "\n",
    "\n",
    "        feature_names_JMJ = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "           'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "           'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open',\n",
    "           'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume',\n",
    "           'Number of Trades', 'TB Base Volume', 'TB Quote Volume',\n",
    "           '3MovingAverage', '5MovingAverage', 'JMJ_3HMoving_averages',\n",
    "            'Bitcoin_Google_Trend_Score', 'BTC_Google_Trend_Score',\n",
    "           'JMJ_5HMoving_averages', 'Mkt Sentiment',\n",
    "           'Crypto Sentiment', 'Historically Optimal SMA(s-t)',\n",
    "           'Historically Optimal SMA(l-t)', 'Historically Optimal WMA(s-t)',\n",
    "           'Historically Optimal WMA(l-t)', 'Historically Optimal EMA(s-t)',\n",
    "           'Historically Optimal EMA(l-t)',\n",
    "           'Twitter Hourly Favorites SMA(s-t)',\n",
    "           'Twitter Hourly Favorites SMA(l-t)',\n",
    "           'Twitter Hourly Favorites WMA(s-t)',\n",
    "           'Twitter Hourly Favorites WMA(l-t)',\n",
    "           'Twitter Hourly Favorites EMA(s-t)',\n",
    "           'Twitter Hourly Favorites EMA(l-t)',\n",
    "           'Twitter Hourly Retweets SMA(s-t)',\n",
    "           'Twitter Hourly Retweets SMA(l-t)',\n",
    "           'Twitter Hourly Retweets WMA(s-t)',\n",
    "           'Twitter Hourly Retweets WMA(l-t)',\n",
    "           'Twitter Hourly Retweets EMA(s-t)',\n",
    "           'Twitter Hourly Retweets EMA(l-t)',\n",
    "           'Twitter Hourly Follower Exposure SMA(s-t)',\n",
    "           'Twitter Hourly Follower Exposure SMA(l-t)',\n",
    "           'Twitter Hourly Follower Exposure WMA(s-t)',\n",
    "           'Twitter Hourly Follower Exposure WMA(l-t)',\n",
    "           'Twitter Hourly Follower Exposure EMA(s-t)',\n",
    "           'Twitter Hourly Follower Exposure EMA(l-t)',\n",
    "           'Twitter Hourly Following Exposure SMA(s-t)',\n",
    "           'Twitter Hourly Following Exposure SMA(l-t)',\n",
    "           'Twitter Hourly Following Exposure WMA(s-t)',\n",
    "           'Twitter Hourly Following Exposure WMA(l-t)',\n",
    "           'Twitter Hourly Following Exposure EMA(s-t)',\n",
    "           'Twitter Hourly Following Exposure EMA(l-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio SMA(s-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio SMA(l-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio WMA(s-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio WMA(l-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio EMA(s-t)',\n",
    "           'Twitter Hourly Follower to Following Ratio EMA(l-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites SMA(s-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites SMA(l-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites WMA(s-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites WMA(l-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites EMA(s-t)',\n",
    "           'Twitter Hourly 2x Retweets + Favorites EMA(l-t)',\n",
    "           'Twitter Hourly Polarity Score SMA(s-t)',\n",
    "           'Twitter Hourly Polarity Score SMA(l-t)',\n",
    "           'Twitter Hourly Polarity Score WMA(s-t)',\n",
    "           'Twitter Hourly Polarity Score WMA(l-t)',\n",
    "           'Twitter Hourly Polarity Score EMA(s-t)',\n",
    "           'Twitter Hourly Polarity Score EMA(l-t)',\n",
    "           'Twitter W1 Score SMA(s-t)', 'Twitter W1 Score SMA(l-t)',\n",
    "           'Twitter W1 Score WMA(s-t)', 'Twitter W1 Score WMA(l-t)',\n",
    "           'Twitter W1 Score EMA(s-t)', 'Twitter W1 Score EMA(l-t)',\n",
    "           'Twitter Hourly Bull Ratio SMA(s-t)',\n",
    "           'Twitter Hourly Bull Ratio SMA(l-t)',\n",
    "           'Twitter Hourly Bull Ratio WMA(s-t)',\n",
    "           'Twitter Hourly Bull Ratio WMA(l-t)',\n",
    "           'Twitter Hourly Bull Ratio EMA(s-t)',\n",
    "           'Twitter Hourly Bull Ratio EMA(l-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio SMA(s-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio SMA(l-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio WMA(s-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio WMA(l-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio EMA(s-t)',\n",
    "           'Twitter Hourly Weighted Bull Ratio EMA(l-t)',\n",
    "           'Quote Asset Volume SMA(s-t)', 'Quote Asset Volume SMA(l-t)',\n",
    "           'Quote Asset Volume WMA(s-t)', 'Quote Asset Volume WMA(l-t)',\n",
    "           'Quote Asset Volume EMA(s-t)', 'Quote Asset Volume EMA(l-t)',\n",
    "           '# of Hourly Trades SMA(s-t)', '# of Hourly Trades SMA(l-t)',\n",
    "           '# of Hourly Trades WMA(s-t)', '# of Hourly Trades WMA(l-t)',\n",
    "           '# of Hourly Trades EMA(s-t)', '# of Hourly Trades EMA(l-t)',\n",
    "           'TB Base Volume SMA(s-t)', 'TB Base Volume SMA(l-t)',\n",
    "           'TB Base Volume WMA(s-t)', 'TB Base Volume WMA(l-t)',\n",
    "           'TB Base Volume EMA(s-t)', 'TB Base Volume EMA(l-t)',\n",
    "           'TB Quote Volume SMA(s-t)', 'TB Quote Volume SMA(l-t)',\n",
    "           'TB Quote Volume WMA(s-t)', 'TB Quote Volume WMA(l-t)',\n",
    "           'TB Quote Volume EMA(s-t)', 'TB Quote Volume EMA(l-t)',\n",
    "           '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "           '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV',\n",
    "           '20 period CCI', '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21',\n",
    "           'ema15', 'ema5', 'normVol', 'Binaryfavorites', 'Binaryretweets',\n",
    "           'Binarynumber_of_followers', 'Binaryfollowing',\n",
    "           'Binaryfollowers_following_ratio', 'Binary2x_retweets_+_favorites',\n",
    "           'Binarypolarity', 'BinaryW1 Score', 'BinaryBull_ratio',\n",
    "           'BinaryW Score With Bull Ratio', 'BinaryOpen', 'BinaryHigh',\n",
    "           'BinaryLow', 'BinaryClose', 'BinaryVolume',\n",
    "           'BinaryQuote Asset Volume', 'BinaryNumber of Trades',\n",
    "           'BinaryTB Base Volume', 'BinaryTB Quote Volume',\n",
    "           'Binary3MovingAverage', 'Binary5MovingAverage',\n",
    "           'BinaryJMJ_3HMoving_averages', 'BinaryJMJ_5HMoving_averages',\n",
    "           'Binary14 period RSI', 'BinaryMACD', 'BinarySIGNAL',\n",
    "           'Binary14 period STOCH %K', 'BinaryMFV', 'Binary14 period ATR',\n",
    "           'BinaryMOM', 'Binary14 period MFI', 'BinaryROC', 'BinaryOBV',\n",
    "           'Binary20 period CCI', 'Binary14 period EMV', 'BinaryVIm',\n",
    "           'BinaryVIp', 'Binaryema50', 'Binaryema21', 'Binaryema15',\n",
    "           'Binaryema5']\n",
    "\n",
    "        X_test = PredictDF[feature_names_JMJ]\n",
    "        y_test = PredictDF['Actual_Label']\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_test)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #     predict_y1= np.argmax(stacking_classifier.predict_proba(X_test_scaled),axis=0)\n",
    "        #predict_y=  Model2.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "        PredictDF['Prediction']= Model2.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "\n",
    "    dataframetime= pd.read_csv('assets/Final_Go_Live_Jesus_MergeJMJPCHourly2019202_NewsGoogleApi_ActualLabel.csv') \n",
    "    Final_prediction = pd.merge (dataframetime,PredictDF, left_index=True, right_index=True)\n",
    "    Final_prediction.to_csv(\"results.csv\")\n",
    "    FinalPrediction2 = Final_prediction[['time','Prediction','Close_x']].iloc[-1]\n",
    "    results_df = pd.DataFrame(columns=['time', 'Prediction', 'Close_x'])\n",
    "    result_output_df = pd.DataFrame([FinalPrediction2])\n",
    "    results_df = pd.concat([results_df, result_output_df], ignore_index=True)\n",
    "    print(results_df)\n",
    "    print(datetime.now())\n",
    "    \n",
    "    \n",
    "# If you  want to start using the information to trade directly in Binance, you can uncoment the code below and link it to your Binance API\n",
    "\n",
    "#     def signal(row):\n",
    "#         if float(row['Prediction']) >= 0.525:\n",
    "#             return 1\n",
    "#         elif float(row['Prediction']) < 0.475:\n",
    "#             return -1\n",
    "#         else:\n",
    "#             return 0\n",
    "\n",
    "#     PredictionBinance = signal(results_df)\n",
    "\n",
    "#     def max_buy_position():\n",
    "#         to_use = float(exchange.fetch_balance().get('USDT').get('free'))\n",
    "#         price = float(exchange.fetchTicker('BTC/USDT').get('last'))\n",
    "#         decide_position_to_use = (to_use / price)\n",
    "#         return decide_position_to_use, price\n",
    "\n",
    "#     def max_sell_position():\n",
    "#         to_use2 = float(exchange.fetch_balance().get('USDT').get('free'))\n",
    "#         decide_position_to_use2 = to_use2\n",
    "#         return decide_position_to_use2\n",
    "#     in_position = False #meaning I already have the asset\n",
    "#     bought_price = 27885.27 # will change, setup in beginning before running\n",
    "#     sold_price = 0 #will change, setup in beginning before running\n",
    "#     # Margin_Goal = 0.007 # 0.1%  to cover for binance fee\n",
    "\n",
    "\n",
    "#     def execute_buy_sell_signals(df):\n",
    "#         global in_position\n",
    "#         global bought_price\n",
    "#         global sold_price\n",
    "#         print ('checking for buy and sell signals')\n",
    "\n",
    "\n",
    "#         #BUY ORDER\n",
    "#         if PredictionBinance = 1\n",
    "#             print ('Recommendation to buy at probability of {}'.format(float(results_df['Prediction'])))\n",
    "#             if not in_position:\n",
    "#                 order = exchange.create_market_buy_order('BTC/USDT',max_buy_position()[0])\n",
    "#                 print (order)\n",
    "#                 print ('I have successfully bought')\n",
    "#                 in_position = True\n",
    "#                 bought_price = max_buy_position()[1]\n",
    "#             else:\n",
    "#                     print ('Already in position')\n",
    "\n",
    "#          #SELL ORDER        \n",
    "#         if PredictionBinance = -1\n",
    "#             print ('Recommendation to sell at probability of {}'.format(float(results_df['Prediction'])))\n",
    "#             if in_position:\n",
    "#                 order = exchange.create_market_sell_order('BTC/USDT',max_sell_position())\n",
    "#                 print (order)\n",
    "#                 print ('I have successfully sold')\n",
    "#                 in_position = False\n",
    "\n",
    "#             else:\n",
    "#                 print (\"You aren't in positin, nothing to sell\")    \n",
    "\n",
    "#         elif PredictionBinance = 0:\n",
    "#              print ('Recommendation to HOLD at probability {}'.format(float(results_df['Prediction'])))\n",
    "\n",
    "\n",
    "def threaded_run_program():\n",
    "    job_thread = threading.Thread(target=run_program)\n",
    "    job_thread.start()\n",
    "\n",
    "def schedule_one_minute_after_top_of_hour():\n",
    "    now = datetime.now()\n",
    "    next_hour = (now + timedelta(hours=1)).replace(minute=1, second=0, microsecond=0)\n",
    "    initial_delay = (next_hour - now).total_seconds()\n",
    "    \n",
    "    # Run the program once after the initial delay\n",
    "    time.sleep(initial_delay)\n",
    "    threaded_run_program()\n",
    "    \n",
    "    # Schedule the repeated runs, starting one hour after the initial run\n",
    "    schedule.every(1).hour.do(threaded_run_program)\n",
    "\n",
    "schedule_one_minute_after_top_of_hour()\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(3600)  # Check every hour"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Three and Five JMJ Moving Average Binance and Twitter. ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
