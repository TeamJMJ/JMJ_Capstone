{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709bba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from finta import TA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import optimizers, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947dd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']\n",
    "# Df_hourly_merge = pd.read_csv('assets/PCHourly2019202_ActualLabel.csv')\n",
    "Df_hourly_merge = pd.read_csv('assets/MergeJMJPCHourly2019202_NewsGoogleApi_ActualLabel.csv')\n",
    "\n",
    "def _exponential_smooth(data, alpha):\n",
    "    \"\"\"\n",
    "    Function that exponentially smooths dataset so values are less 'rigid'\n",
    "    :param alpha: weight factor to weight recent values more\n",
    "    \"\"\"\n",
    "    \n",
    "    return data.ewm(alpha=alpha).mean()\n",
    "\n",
    "Indicatordata = _exponential_smooth(Df_hourly_merge[['Close', 'Open','High','Low','Volume']], 0.65)\n",
    "def _get_indicator_data(data):\n",
    "    \"\"\"\n",
    "    Function that uses the finta API to calculate technical indicators used as the features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for indicator in INDICATORS:\n",
    "        ind_data = eval('TA.' + indicator + '(data)')\n",
    "        if not isinstance(ind_data, pd.DataFrame):\n",
    "            ind_data = ind_data.to_frame()\n",
    "        data = data.merge(ind_data, left_index=True, right_index=True)\n",
    "    data.rename(columns={\"14 period EMV.\": '14 period EMV'}, inplace=True)\n",
    "\n",
    "    # Also calculate moving averages for features\n",
    "    data['ema50'] = data['Close'] / data['Close'].ewm(50).mean()\n",
    "    data['ema21'] = data['Close'] / data['Close'].ewm(21).mean()\n",
    "    data['ema15'] = data['Close'] / data['Close'].ewm(14).mean()\n",
    "    data['ema5'] = data['Close'] / data['Close'].ewm(5).mean()\n",
    "\n",
    "    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "    data['normVol'] = data['Volume'] / data['Volume'].ewm(5).mean()\n",
    "    \n",
    "    return data\n",
    "\n",
    "Indicatordatafinal = _get_indicator_data(Indicatordata)\n",
    "\n",
    "Indicatordatafinal = Indicatordatafinal.drop(['Close', 'Open', 'High', 'Low', 'Volume'], axis = 1)\n",
    "Df_hourly_merge2 = pd.merge(Df_hourly_merge, Indicatordatafinal, left_index=True, right_index=True)\n",
    "\n",
    "Df_hourly_merge2 = Df_hourly_merge2.drop(['time','hour','Open Time','_merge','Signal','Position', 'Signal35JMJ', 'Position35JMJ', 'Ignore'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def binary(value):\n",
    "  if value > 0:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "columns = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "       'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "       'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open', 'High',\n",
    "       'Low', 'Close', 'Volume', 'Quote Asset Volume', 'Number of Trades',\n",
    "       'TB Base Volume', 'TB Quote Volume', '3MovingAverage', '5MovingAverage',\n",
    "       'JMJ_3HMoving_averages', 'JMJ_5HMoving_averages', \n",
    "       '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "       '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV', '20 period CCI',\n",
    "       '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21', 'ema15', 'ema5']\n",
    "\n",
    "\n",
    "for column in columns: \n",
    "    Df_hourly_merge2['Binary{}'.format(column)]  = (Df_hourly_merge2[column] - Df_hourly_merge2[column].shift(1)).apply(binary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca380c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25503"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Df_hourly_merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da43cdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>number_of_followers</th>\n",
       "      <th>following</th>\n",
       "      <th>followers_following_ratio</th>\n",
       "      <th>2x_retweets_+_favorites</th>\n",
       "      <th>polarity</th>\n",
       "      <th>W1 Score</th>\n",
       "      <th>Bull_ratio</th>\n",
       "      <th>W Score With Bull Ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>BinaryROC</th>\n",
       "      <th>BinaryOBV</th>\n",
       "      <th>Binary20 period CCI</th>\n",
       "      <th>Binary14 period EMV</th>\n",
       "      <th>BinaryVIm</th>\n",
       "      <th>BinaryVIp</th>\n",
       "      <th>Binaryema50</th>\n",
       "      <th>Binaryema21</th>\n",
       "      <th>Binaryema15</th>\n",
       "      <th>Binaryema5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.517857</td>\n",
       "      <td>1.276786</td>\n",
       "      <td>10592.354167</td>\n",
       "      <td>1652.068452</td>\n",
       "      <td>45.605159</td>\n",
       "      <td>9.071429</td>\n",
       "      <td>0.124241</td>\n",
       "      <td>0.264455</td>\n",
       "      <td>3.275000</td>\n",
       "      <td>0.866089</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.685230</td>\n",
       "      <td>0.479419</td>\n",
       "      <td>14341.610169</td>\n",
       "      <td>1852.799031</td>\n",
       "      <td>74.444302</td>\n",
       "      <td>3.644068</td>\n",
       "      <td>0.067950</td>\n",
       "      <td>0.097316</td>\n",
       "      <td>3.342105</td>\n",
       "      <td>0.325240</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.138107</td>\n",
       "      <td>0.670077</td>\n",
       "      <td>21769.074169</td>\n",
       "      <td>2449.731458</td>\n",
       "      <td>68.571009</td>\n",
       "      <td>3.478261</td>\n",
       "      <td>0.120056</td>\n",
       "      <td>0.144203</td>\n",
       "      <td>6.120000</td>\n",
       "      <td>0.882520</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.916462</td>\n",
       "      <td>0.670762</td>\n",
       "      <td>36958.090909</td>\n",
       "      <td>2790.968059</td>\n",
       "      <td>100.500076</td>\n",
       "      <td>3.257985</td>\n",
       "      <td>0.143717</td>\n",
       "      <td>0.110483</td>\n",
       "      <td>5.964286</td>\n",
       "      <td>0.658951</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.339394</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>13345.724242</td>\n",
       "      <td>3208.639394</td>\n",
       "      <td>66.105667</td>\n",
       "      <td>6.181818</td>\n",
       "      <td>0.136780</td>\n",
       "      <td>0.199699</td>\n",
       "      <td>4.607143</td>\n",
       "      <td>0.920041</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25498</th>\n",
       "      <td>5.783333</td>\n",
       "      <td>0.659524</td>\n",
       "      <td>12129.000000</td>\n",
       "      <td>1975.276190</td>\n",
       "      <td>108.150291</td>\n",
       "      <td>7.102381</td>\n",
       "      <td>0.077630</td>\n",
       "      <td>0.078660</td>\n",
       "      <td>3.022222</td>\n",
       "      <td>0.237728</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25499</th>\n",
       "      <td>3.011905</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>14180.595238</td>\n",
       "      <td>1765.121429</td>\n",
       "      <td>31.755382</td>\n",
       "      <td>3.797619</td>\n",
       "      <td>0.095683</td>\n",
       "      <td>0.135134</td>\n",
       "      <td>4.277778</td>\n",
       "      <td>0.578073</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25500</th>\n",
       "      <td>14.400000</td>\n",
       "      <td>2.407143</td>\n",
       "      <td>16161.992857</td>\n",
       "      <td>2084.061905</td>\n",
       "      <td>182.260151</td>\n",
       "      <td>19.214286</td>\n",
       "      <td>0.082271</td>\n",
       "      <td>0.107210</td>\n",
       "      <td>3.648649</td>\n",
       "      <td>0.391172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25501</th>\n",
       "      <td>21.570806</td>\n",
       "      <td>3.270153</td>\n",
       "      <td>11254.296296</td>\n",
       "      <td>1814.954248</td>\n",
       "      <td>428.484448</td>\n",
       "      <td>28.111111</td>\n",
       "      <td>0.070485</td>\n",
       "      <td>0.165014</td>\n",
       "      <td>1.887324</td>\n",
       "      <td>0.311435</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25502</th>\n",
       "      <td>7.505967</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>10556.818616</td>\n",
       "      <td>2143.973747</td>\n",
       "      <td>140.745063</td>\n",
       "      <td>9.262530</td>\n",
       "      <td>0.079407</td>\n",
       "      <td>0.135342</td>\n",
       "      <td>3.945946</td>\n",
       "      <td>0.534052</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25503 rows Ã— 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       favorites  retweets  number_of_followers    following  \\\n",
       "0       6.517857  1.276786         10592.354167  1652.068452   \n",
       "1       2.685230  0.479419         14341.610169  1852.799031   \n",
       "2       2.138107  0.670077         21769.074169  2449.731458   \n",
       "3       1.916462  0.670762         36958.090909  2790.968059   \n",
       "4       4.339394  0.921212         13345.724242  3208.639394   \n",
       "...          ...       ...                  ...          ...   \n",
       "25498   5.783333  0.659524         12129.000000  1975.276190   \n",
       "25499   3.011905  0.392857         14180.595238  1765.121429   \n",
       "25500  14.400000  2.407143         16161.992857  2084.061905   \n",
       "25501  21.570806  3.270153         11254.296296  1814.954248   \n",
       "25502   7.505967  0.878282         10556.818616  2143.973747   \n",
       "\n",
       "       followers_following_ratio  2x_retweets_+_favorites  polarity  W1 Score  \\\n",
       "0                      45.605159                 9.071429  0.124241  0.264455   \n",
       "1                      74.444302                 3.644068  0.067950  0.097316   \n",
       "2                      68.571009                 3.478261  0.120056  0.144203   \n",
       "3                     100.500076                 3.257985  0.143717  0.110483   \n",
       "4                      66.105667                 6.181818  0.136780  0.199699   \n",
       "...                          ...                      ...       ...       ...   \n",
       "25498                 108.150291                 7.102381  0.077630  0.078660   \n",
       "25499                  31.755382                 3.797619  0.095683  0.135134   \n",
       "25500                 182.260151                19.214286  0.082271  0.107210   \n",
       "25501                 428.484448                28.111111  0.070485  0.165014   \n",
       "25502                 140.745063                 9.262530  0.079407  0.135342   \n",
       "\n",
       "       Bull_ratio  W Score With Bull Ratio  ...  BinaryROC  BinaryOBV  \\\n",
       "0        3.275000                 0.866089  ...          0          0   \n",
       "1        3.342105                 0.325240  ...          0          0   \n",
       "2        6.120000                 0.882520  ...          0          0   \n",
       "3        5.964286                 0.658951  ...          0          0   \n",
       "4        4.607143                 0.920041  ...          0          0   \n",
       "...           ...                      ...  ...        ...        ...   \n",
       "25498    3.022222                 0.237728  ...          0          0   \n",
       "25499    4.277778                 0.578073  ...          0          0   \n",
       "25500    3.648649                 0.391172  ...          0          0   \n",
       "25501    1.887324                 0.311435  ...          0          1   \n",
       "25502    3.945946                 0.534052  ...          0          0   \n",
       "\n",
       "       Binary20 period CCI  Binary14 period EMV  BinaryVIm  BinaryVIp  \\\n",
       "0                        0                    0          0          0   \n",
       "1                        0                    0          0          0   \n",
       "2                        0                    0          0          0   \n",
       "3                        0                    0          0          0   \n",
       "4                        1                    0          0          0   \n",
       "...                    ...                  ...        ...        ...   \n",
       "25498                    1                    1          0          1   \n",
       "25499                    0                    1          0          1   \n",
       "25500                    0                    0          1          0   \n",
       "25501                    0                    0          1          0   \n",
       "25502                    0                    0          1          0   \n",
       "\n",
       "       Binaryema50  Binaryema21  Binaryema15  Binaryema5  \n",
       "0                0            0            0           0  \n",
       "1                1            1            1           1  \n",
       "2                0            0            0           0  \n",
       "3                0            0            0           0  \n",
       "4                0            0            0           0  \n",
       "...            ...          ...          ...         ...  \n",
       "25498            0            0            0           0  \n",
       "25499            0            0            0           0  \n",
       "25500            0            0            0           0  \n",
       "25501            1            1            1           1  \n",
       "25502            0            0            0           0  \n",
       "\n",
       "[25503 rows x 178 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_hourly_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8825a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['favorites', 'retweets', 'number_of_followers', 'following',\n",
       "       'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
       "       'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open',\n",
       "       'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume',\n",
       "       'Number of Trades', 'TB Base Volume', 'TB Quote Volume',\n",
       "       '3MovingAverage', '5MovingAverage', 'JMJ_3HMoving_averages',\n",
       "       'JMJ_5HMoving_averages', 'Actual_Label',\n",
       "       'Bitcoin_Google_Trend_Score', 'BTC_Google_Trend_Score',\n",
       "       'Mkt Sentiment', 'Crypto Sentiment',\n",
       "       'Historically Optimal SMA(s-t)', 'Historically Optimal SMA(l-t)',\n",
       "       'Historically Optimal WMA(s-t)', 'Historically Optimal WMA(l-t)',\n",
       "       'Historically Optimal EMA(s-t)', 'Historically Optimal EMA(l-t)',\n",
       "       'Twitter Hourly Favorites SMA(s-t)',\n",
       "       'Twitter Hourly Favorites SMA(l-t)',\n",
       "       'Twitter Hourly Favorites WMA(s-t)',\n",
       "       'Twitter Hourly Favorites WMA(l-t)',\n",
       "       'Twitter Hourly Favorites EMA(s-t)',\n",
       "       'Twitter Hourly Favorites EMA(l-t)',\n",
       "       'Twitter Hourly Retweets SMA(s-t)',\n",
       "       'Twitter Hourly Retweets SMA(l-t)',\n",
       "       'Twitter Hourly Retweets WMA(s-t)',\n",
       "       'Twitter Hourly Retweets WMA(l-t)',\n",
       "       'Twitter Hourly Retweets EMA(s-t)',\n",
       "       'Twitter Hourly Retweets EMA(l-t)',\n",
       "       'Twitter Hourly Follower Exposure SMA(s-t)',\n",
       "       'Twitter Hourly Follower Exposure SMA(l-t)',\n",
       "       'Twitter Hourly Follower Exposure WMA(s-t)',\n",
       "       'Twitter Hourly Follower Exposure WMA(l-t)',\n",
       "       'Twitter Hourly Follower Exposure EMA(s-t)',\n",
       "       'Twitter Hourly Follower Exposure EMA(l-t)',\n",
       "       'Twitter Hourly Following Exposure SMA(s-t)',\n",
       "       'Twitter Hourly Following Exposure SMA(l-t)',\n",
       "       'Twitter Hourly Following Exposure WMA(s-t)',\n",
       "       'Twitter Hourly Following Exposure WMA(l-t)',\n",
       "       'Twitter Hourly Following Exposure EMA(s-t)',\n",
       "       'Twitter Hourly Following Exposure EMA(l-t)',\n",
       "       'Twitter Hourly Follower to Following Ratio SMA(s-t)',\n",
       "       'Twitter Hourly Follower to Following Ratio SMA(l-t)',\n",
       "       'Twitter Hourly Follower to Following Ratio WMA(s-t)',\n",
       "       'Twitter Hourly Follower to Following Ratio WMA(l-t)',\n",
       "       'Twitter Hourly Follower to Following Ratio EMA(s-t)',\n",
       "       'Twitter Hourly Follower to Following Ratio EMA(l-t)',\n",
       "       'Twitter Hourly 2x Retweets + Favorites SMA(s-t)',\n",
       "       'Twitter Hourly 2x Retweets + Favorites SMA(l-t)',\n",
       "       'Twitter Hourly 2x Retweets + Favorites WMA(s-t)',\n",
       "       'Twitter Hourly 2x Retweets + Favorites WMA(l-t)',\n",
       "       'Twitter Hourly 2x Retweets + Favorites EMA(s-t)',\n",
       "       'Twitter Hourly 2x Retweets + Favorites EMA(l-t)',\n",
       "       'Twitter Hourly Polarity Score SMA(s-t)',\n",
       "       'Twitter Hourly Polarity Score SMA(l-t)',\n",
       "       'Twitter Hourly Polarity Score WMA(s-t)',\n",
       "       'Twitter Hourly Polarity Score WMA(l-t)',\n",
       "       'Twitter Hourly Polarity Score EMA(s-t)',\n",
       "       'Twitter Hourly Polarity Score EMA(l-t)',\n",
       "       'Twitter W1 Score SMA(s-t)', 'Twitter W1 Score SMA(l-t)',\n",
       "       'Twitter W1 Score WMA(s-t)', 'Twitter W1 Score WMA(l-t)',\n",
       "       'Twitter W1 Score EMA(s-t)', 'Twitter W1 Score EMA(l-t)',\n",
       "       'Twitter Hourly Bull Ratio SMA(s-t)',\n",
       "       'Twitter Hourly Bull Ratio SMA(l-t)',\n",
       "       'Twitter Hourly Bull Ratio WMA(s-t)',\n",
       "       'Twitter Hourly Bull Ratio WMA(l-t)',\n",
       "       'Twitter Hourly Bull Ratio EMA(s-t)',\n",
       "       'Twitter Hourly Bull Ratio EMA(l-t)',\n",
       "       'Twitter Hourly Weighted Bull Ratio SMA(s-t)',\n",
       "       'Twitter Hourly Weighted Bull Ratio SMA(l-t)',\n",
       "       'Twitter Hourly Weighted Bull Ratio WMA(s-t)',\n",
       "       'Twitter Hourly Weighted Bull Ratio WMA(l-t)',\n",
       "       'Twitter Hourly Weighted Bull Ratio EMA(s-t)',\n",
       "       'Twitter Hourly Weighted Bull Ratio EMA(l-t)',\n",
       "       'Quote Asset Volume SMA(s-t)', 'Quote Asset Volume SMA(l-t)',\n",
       "       'Quote Asset Volume WMA(s-t)', 'Quote Asset Volume WMA(l-t)',\n",
       "       'Quote Asset Volume EMA(s-t)', 'Quote Asset Volume EMA(l-t)',\n",
       "       '# of Hourly Trades SMA(s-t)', '# of Hourly Trades SMA(l-t)',\n",
       "       '# of Hourly Trades WMA(s-t)', '# of Hourly Trades WMA(l-t)',\n",
       "       '# of Hourly Trades EMA(s-t)', '# of Hourly Trades EMA(l-t)',\n",
       "       'TB Base Volume SMA(s-t)', 'TB Base Volume SMA(l-t)',\n",
       "       'TB Base Volume WMA(s-t)', 'TB Base Volume WMA(l-t)',\n",
       "       'TB Base Volume EMA(s-t)', 'TB Base Volume EMA(l-t)',\n",
       "       'TB Quote Volume SMA(s-t)', 'TB Quote Volume SMA(l-t)',\n",
       "       'TB Quote Volume WMA(s-t)', 'TB Quote Volume WMA(l-t)',\n",
       "       'TB Quote Volume EMA(s-t)', 'TB Quote Volume EMA(l-t)',\n",
       "       '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
       "       '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV',\n",
       "       '20 period CCI', '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21',\n",
       "       'ema15', 'ema5', 'normVol', 'Binaryfavorites', 'Binaryretweets',\n",
       "       'Binarynumber_of_followers', 'Binaryfollowing',\n",
       "       'Binaryfollowers_following_ratio', 'Binary2x_retweets_+_favorites',\n",
       "       'Binarypolarity', 'BinaryW1 Score', 'BinaryBull_ratio',\n",
       "       'BinaryW Score With Bull Ratio', 'BinaryOpen', 'BinaryHigh',\n",
       "       'BinaryLow', 'BinaryClose', 'BinaryVolume',\n",
       "       'BinaryQuote Asset Volume', 'BinaryNumber of Trades',\n",
       "       'BinaryTB Base Volume', 'BinaryTB Quote Volume',\n",
       "       'Binary3MovingAverage', 'Binary5MovingAverage',\n",
       "       'BinaryJMJ_3HMoving_averages', 'BinaryJMJ_5HMoving_averages',\n",
       "       'Binary14 period RSI', 'BinaryMACD', 'BinarySIGNAL',\n",
       "       'Binary14 period STOCH %K', 'BinaryMFV', 'Binary14 period ATR',\n",
       "       'BinaryMOM', 'Binary14 period MFI', 'BinaryROC', 'BinaryOBV',\n",
       "       'Binary20 period CCI', 'Binary14 period EMV', 'BinaryVIm',\n",
       "       'BinaryVIp', 'Binaryema50', 'Binaryema21', 'Binaryema15',\n",
       "       'Binaryema5'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_hourly_merge2.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b89263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 59.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = Df_hourly_merge2\n",
    "data = data.dropna()\n",
    "\n",
    "feature_names_JMJ = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "       'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "       'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open',\n",
    "       'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume',\n",
    "       'Number of Trades', 'TB Base Volume', 'TB Quote Volume',\n",
    "       '3MovingAverage', '5MovingAverage', 'JMJ_3HMoving_averages',\n",
    "        'Bitcoin_Google_Trend_Score', 'BTC_Google_Trend_Score',\n",
    "       'JMJ_5HMoving_averages', 'Mkt Sentiment',\n",
    "       'Crypto Sentiment', 'Historically Optimal SMA(s-t)',\n",
    "       'Historically Optimal SMA(l-t)', 'Historically Optimal WMA(s-t)',\n",
    "       'Historically Optimal WMA(l-t)', 'Historically Optimal EMA(s-t)',\n",
    "       'Historically Optimal EMA(l-t)',\n",
    "       'Twitter Hourly Favorites SMA(s-t)',\n",
    "       'Twitter Hourly Favorites SMA(l-t)',\n",
    "       'Twitter Hourly Favorites WMA(s-t)',\n",
    "       'Twitter Hourly Favorites WMA(l-t)',\n",
    "       'Twitter Hourly Favorites EMA(s-t)',\n",
    "       'Twitter Hourly Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Retweets SMA(s-t)',\n",
    "       'Twitter Hourly Retweets SMA(l-t)',\n",
    "       'Twitter Hourly Retweets WMA(s-t)',\n",
    "       'Twitter Hourly Retweets WMA(l-t)',\n",
    "       'Twitter Hourly Retweets EMA(s-t)',\n",
    "       'Twitter Hourly Retweets EMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(l-t)',\n",
    "       'Twitter W1 Score SMA(s-t)', 'Twitter W1 Score SMA(l-t)',\n",
    "       'Twitter W1 Score WMA(s-t)', 'Twitter W1 Score WMA(l-t)',\n",
    "       'Twitter W1 Score EMA(s-t)', 'Twitter W1 Score EMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(l-t)',\n",
    "       'Quote Asset Volume SMA(s-t)', 'Quote Asset Volume SMA(l-t)',\n",
    "       'Quote Asset Volume WMA(s-t)', 'Quote Asset Volume WMA(l-t)',\n",
    "       'Quote Asset Volume EMA(s-t)', 'Quote Asset Volume EMA(l-t)',\n",
    "       '# of Hourly Trades SMA(s-t)', '# of Hourly Trades SMA(l-t)',\n",
    "       '# of Hourly Trades WMA(s-t)', '# of Hourly Trades WMA(l-t)',\n",
    "       '# of Hourly Trades EMA(s-t)', '# of Hourly Trades EMA(l-t)',\n",
    "       'TB Base Volume SMA(s-t)', 'TB Base Volume SMA(l-t)',\n",
    "       'TB Base Volume WMA(s-t)', 'TB Base Volume WMA(l-t)',\n",
    "       'TB Base Volume EMA(s-t)', 'TB Base Volume EMA(l-t)',\n",
    "       'TB Quote Volume SMA(s-t)', 'TB Quote Volume SMA(l-t)',\n",
    "       'TB Quote Volume WMA(s-t)', 'TB Quote Volume WMA(l-t)',\n",
    "       'TB Quote Volume EMA(s-t)', 'TB Quote Volume EMA(l-t)',\n",
    "       '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "       '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV',\n",
    "       '20 period CCI', '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21',\n",
    "       'ema15', 'ema5', 'normVol', 'Binaryfavorites', 'Binaryretweets',\n",
    "       'Binarynumber_of_followers', 'Binaryfollowing',\n",
    "       'Binaryfollowers_following_ratio', 'Binary2x_retweets_+_favorites',\n",
    "       'Binarypolarity', 'BinaryW1 Score', 'BinaryBull_ratio',\n",
    "       'BinaryW Score With Bull Ratio', 'BinaryOpen', 'BinaryHigh',\n",
    "       'BinaryLow', 'BinaryClose', 'BinaryVolume',\n",
    "       'BinaryQuote Asset Volume', 'BinaryNumber of Trades',\n",
    "       'BinaryTB Base Volume', 'BinaryTB Quote Volume',\n",
    "       'Binary3MovingAverage', 'Binary5MovingAverage',\n",
    "       'BinaryJMJ_3HMoving_averages', 'BinaryJMJ_5HMoving_averages',\n",
    "       'Binary14 period RSI', 'BinaryMACD', 'BinarySIGNAL',\n",
    "       'Binary14 period STOCH %K', 'BinaryMFV', 'Binary14 period ATR',\n",
    "       'BinaryMOM', 'Binary14 period MFI', 'BinaryROC', 'BinaryOBV',\n",
    "       'Binary20 period CCI', 'Binary14 period EMV', 'BinaryVIm',\n",
    "       'BinaryVIp', 'Binaryema50', 'Binaryema21', 'Binaryema15',\n",
    "       'Binaryema5']\n",
    "\n",
    "X_JMJ = data[feature_names_JMJ]\n",
    "y_JMJ = data['Actual_Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71371fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names_JMJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b15bc",
   "metadata": {},
   "source": [
    "## BASE ESTIMATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce48f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def _LogisticRegression(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that uses random forest classifier to train the model\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new random forest classifier\n",
    "    #et = ExtraTreesClassifier()\n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    # Dictionary of all values we want to test for n_estimators\n",
    "    params_lr = {\n",
    "                    'C': [0.01, 0.1, 1, 10, 100], \n",
    "                    \"penalty\":['l2'],\n",
    "                    'solver': ['lbfgs'],\n",
    "                    'max_iter':[1000]\n",
    "        \n",
    "        \n",
    "        \n",
    "                    }\n",
    "    \n",
    "\n",
    "    \n",
    "    # Use gridsearch to test all values for n_estimators\n",
    "    lr_gs = GridSearchCV(lr, params_lr, cv=5)\n",
    "    \n",
    "    # Fit model to training data\n",
    "    lr_gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Save best model\n",
    "    lr_best = lr_gs.best_estimator_\n",
    "    \n",
    "    # Check best n_estimators value\n",
    "    print(lr_gs.best_params_)\n",
    "    \n",
    "    prediction = lr_best.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    \n",
    "    return lr_best\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aaaebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_AdaBoostClassifier(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that uses random forest classifier to train the model\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new random forest classifier\n",
    "    abc = AdaBoostClassifier()\n",
    "    \n",
    "    # Dictionary of all values we want to test for n_estimators\n",
    "    params_abc =  {\n",
    "                'n_estimators': [20, 50, 70, 100],\n",
    "                'learning_rate' : [0.001, 0.01, 0.1, 0.2]\n",
    "                }\n",
    "    \n",
    "    # Use gridsearch to test all values for n_estimators\n",
    "    abc_gs = GridSearchCV(abc, params_abc, cv=5)\n",
    "    \n",
    "    # Fit model to training data\n",
    "    abc_gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Save best model\n",
    "    abc_best = abc_gs.best_estimator_\n",
    "    \n",
    "    # Check best n_estimators value\n",
    "    print(abc_gs.best_params_)\n",
    "    \n",
    "    prediction = abc_best.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    \n",
    "    return abc_best\n",
    "    \n",
    "# abc_model = _train_AdaBoostClassifier(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d99641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def _train_LDA(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    \n",
    "    param_lda = {\n",
    "                'solver': ['svd', 'lsqr', 'eigen']\n",
    "                                  \n",
    "                 }\n",
    "    \n",
    "    # Use gridsearch to test all values for n_estimators\n",
    "    lda_gs = GridSearchCV(lda, param_lda, cv=5)\n",
    "    \n",
    "    # Fit model to training data\n",
    "    lda_gs.fit(X_train, y_train)\n",
    "              \n",
    "    # Save best model\n",
    "    lda_best = lda_gs.best_estimator_\n",
    "    \n",
    "    # Check best n_estimators value\n",
    "    print(lda_gs.best_params_)\n",
    "    \n",
    "    prediction = lda_best.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    \n",
    "    return lda_best\n",
    "    \n",
    "    \n",
    "# lda_model = _train_LDA(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc4fe796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def _GradientBoosingClassifier(X_train, y_train, X_test, y_test):\n",
    "\n",
    "\n",
    "    \n",
    "    # Create a ne\n",
    "    GBC = GradientBoostingClassifier()\n",
    "    \n",
    "    # Dictionary of all values we want to test for n_estimators\n",
    "    params_nnclf = {\n",
    "    \"loss\":[\"log_loss\"],\n",
    "    \"learning_rate\": [0.01, 0.1],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,5],\n",
    "    \"criterion\": [\"friedman_mse\"],\n",
    "    \"subsample\":[0.5, 1.0],\n",
    "    \"n_estimators\":[100]\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Use gridsearch to test all values for n_estimators\n",
    "    GBC_gs = GridSearchCV(GBC, params_nnclf, cv=5)\n",
    "    \n",
    "    # Fit model to training data\n",
    "    GBC_gs.fit(X_train, y_train)\n",
    "#     GBC.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    # Save best model\n",
    "    GBC_best = GBC_gs.best_estimator_\n",
    "    \n",
    "    # Check best n_estimators value\n",
    "    print(GBC_gs.best_params_)\n",
    "    \n",
    "    prediction = GBC_best.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, prediction))\n",
    "    print(confusion_matrix(y_test, prediction))\n",
    "    \n",
    "    return GBC_best\n",
    "    \n",
    "# GBC_model = _GradientBoosingClassifier(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75264452",
   "metadata": {},
   "source": [
    "## FINAL MODEL SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11fdf156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 2316 2317 2318] [2319 2320 2321 ... 4633 4634 4635]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.35      0.42      1084\n",
      "         1.0       0.57      0.75      0.64      1233\n",
      "\n",
      "    accuracy                           0.56      2317\n",
      "   macro avg       0.56      0.55      0.53      2317\n",
      "weighted avg       0.56      0.56      0.54      2317\n",
      "\n",
      "[[375 709]\n",
      " [310 923]]\n",
      "{'learning_rate': 0.01, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.56      0.54      1084\n",
      "         1.0       0.58      0.53      0.55      1233\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.55      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[612 472]\n",
      " [581 652]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 64 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 27 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 66 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.52480077 0.52480077        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.25      0.33      1084\n",
      "         1.0       0.53      0.75      0.62      1233\n",
      "\n",
      "    accuracy                           0.52      2317\n",
      "   macro avg       0.50      0.50      0.47      2317\n",
      "weighted avg       0.50      0.52      0.48      2317\n",
      "\n",
      "[[270 814]\n",
      " [306 927]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.46      0.49      1084\n",
      "         1.0       0.58      0.64      0.61      1233\n",
      "\n",
      "    accuracy                           0.56      2317\n",
      "   macro avg       0.55      0.55      0.55      2317\n",
      "weighted avg       0.55      0.56      0.55      2317\n",
      "\n",
      "[[498 586]\n",
      " [439 794]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.46      0.49      1084\n",
      "         1.0       0.57      0.63      0.60      1233\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.54      0.54      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[497 587]\n",
      " [457 776]]\n",
      "ABC Accuracy = 0.5455330168321105 , LDA Accuracy = 0.5166163141993958 , LR Accuracy = 0.5602071644367717 , GBC Accuracy = 0.5576176089771256 , LTSM Accuracy = 0.5494173500215797\n",
      " \n",
      "[   0    1    2 ... 4633 4634 4635] [4636 4637 4638 ... 6950 6951 6952]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.33      0.42      1178\n",
      "         1.0       0.53      0.77      0.63      1139\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.56      0.55      0.53      2317\n",
      "weighted avg       0.56      0.55      0.52      2317\n",
      "\n",
      "[[386 792]\n",
      " [257 882]]\n",
      "{'learning_rate': 0.01, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.47      0.53      1178\n",
      "         1.0       0.55      0.68      0.61      1139\n",
      "\n",
      "    accuracy                           0.57      2317\n",
      "   macro avg       0.58      0.57      0.57      2317\n",
      "weighted avg       0.58      0.57      0.57      2317\n",
      "\n",
      "[[553 625]\n",
      " [370 769]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 66 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.55004255 0.5476693         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.04      0.08      1178\n",
      "         1.0       0.50      0.98      0.66      1139\n",
      "\n",
      "    accuracy                           0.50      2317\n",
      "   macro avg       0.57      0.51      0.37      2317\n",
      "weighted avg       0.57      0.50      0.36      2317\n",
      "\n",
      "[[  48 1130]\n",
      " [  27 1112]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.30      0.40      1178\n",
      "         1.0       0.53      0.80      0.64      1139\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.57      0.55      0.52      2317\n",
      "weighted avg       0.57      0.55      0.52      2317\n",
      "\n",
      "[[353 825]\n",
      " [223 916]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.31      0.41      1178\n",
      "         1.0       0.53      0.79      0.63      1139\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.57      0.55      0.52      2317\n",
      "weighted avg       0.57      0.55      0.52      2317\n",
      "\n",
      "[[371 807]\n",
      " [241 898]]\n",
      "ABC Accuracy = 0.570565386275356 , LDA Accuracy = 0.5006473888649116 , LR Accuracy = 0.5472593871385412 , GBC Accuracy = 0.5476909797151489 , LTSM Accuracy = 0.5476909797151489\n",
      " \n",
      "[   0    1    2 ... 6950 6951 6952] [6953 6954 6955 ... 9267 9268 9269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.52      0.54      1144\n",
      "         1.0       0.57      0.63      0.60      1173\n",
      "\n",
      "    accuracy                           0.57      2317\n",
      "   macro avg       0.57      0.57      0.57      2317\n",
      "weighted avg       0.57      0.57      0.57      2317\n",
      "\n",
      "[[590 554]\n",
      " [432 741]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.59      0.58      1144\n",
      "         1.0       0.58      0.55      0.57      1173\n",
      "\n",
      "    accuracy                           0.57      2317\n",
      "   macro avg       0.57      0.57      0.57      2317\n",
      "weighted avg       0.57      0.57      0.57      2317\n",
      "\n",
      "[[675 469]\n",
      " [522 651]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 27 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 66 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 64 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.49876917 0.49920103        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lsqr'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.61      0.58      1144\n",
      "         1.0       0.58      0.53      0.56      1173\n",
      "\n",
      "    accuracy                           0.57      2317\n",
      "   macro avg       0.57      0.57      0.57      2317\n",
      "weighted avg       0.57      0.57      0.57      2317\n",
      "\n",
      "[[695 449]\n",
      " [550 623]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.57      0.57      1144\n",
      "         1.0       0.58      0.57      0.58      1173\n",
      "\n",
      "    accuracy                           0.57      2317\n",
      "   macro avg       0.57      0.57      0.57      2317\n",
      "weighted avg       0.57      0.57      0.57      2317\n",
      "\n",
      "[[656 488]\n",
      " [500 673]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.53      0.55      1144\n",
      "         1.0       0.57      0.60      0.58      1173\n",
      "\n",
      "    accuracy                           0.57      2317\n",
      "   macro avg       0.57      0.57      0.57      2317\n",
      "weighted avg       0.57      0.57      0.57      2317\n",
      "\n",
      "[[606 538]\n",
      " [468 705]]\n",
      "ABC Accuracy = 0.5722917565817868 , LDA Accuracy = 0.5688390159689254 , LR Accuracy = 0.5744497194648253 , GBC Accuracy = 0.5735865343116099 , LTSM Accuracy = 0.5658178679326715\n",
      " \n",
      "[   0    1    2 ... 9267 9268 9269] [ 9270  9271  9272 ... 11584 11585 11586]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.55      0.54      1127\n",
      "         1.0       0.55      0.53      0.54      1190\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.54      0.54      0.54      2317\n",
      "weighted avg       0.54      0.54      0.54      2317\n",
      "\n",
      "[[623 504]\n",
      " [564 626]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.61      0.56      1127\n",
      "         1.0       0.57      0.48      0.52      1190\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.55      0.55      0.54      2317\n",
      "weighted avg       0.55      0.54      0.54      2317\n",
      "\n",
      "[[685 442]\n",
      " [613 577]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 64 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.52556634 0.52535059        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.53      0.53      1127\n",
      "         1.0       0.56      0.56      0.56      1190\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.55      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[600 527]\n",
      " [524 666]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.52      0.53      1127\n",
      "         1.0       0.56      0.57      0.57      1190\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.55      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[588 539]\n",
      " [509 681]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.57      0.55      1127\n",
      "         1.0       0.56      0.51      0.53      1190\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.54      0.54      0.54      2317\n",
      "weighted avg       0.54      0.54      0.54      2317\n",
      "\n",
      "[[644 483]\n",
      " [580 610]]\n",
      "ABC Accuracy = 0.5446698316788952 , LDA Accuracy = 0.5463962019853259 , LR Accuracy = 0.5390591281829953 , GBC Accuracy = 0.5476909797151489 , LTSM Accuracy = 0.5412170910660337\n",
      " \n",
      "[    0     1     2 ... 11584 11585 11586] [11587 11588 11589 ... 13901 13902 13903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.58      0.56      1125\n",
      "         1.0       0.58      0.55      0.56      1192\n",
      "\n",
      "    accuracy                           0.56      2317\n",
      "   macro avg       0.56      0.56      0.56      2317\n",
      "weighted avg       0.56      0.56      0.56      2317\n",
      "\n",
      "[[648 477]\n",
      " [542 650]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.65      0.59      1125\n",
      "         1.0       0.59      0.47      0.52      1192\n",
      "\n",
      "    accuracy                           0.56      2317\n",
      "   macro avg       0.56      0.56      0.55      2317\n",
      "weighted avg       0.56      0.56      0.55      2317\n",
      "\n",
      "[[736 389]\n",
      " [637 555]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.53525739 0.53534371        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lsqr'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.67      0.59      1125\n",
      "         1.0       0.58      0.42      0.49      1192\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.55      0.55      0.54      2317\n",
      "weighted avg       0.55      0.54      0.54      2317\n",
      "\n",
      "[[757 368]\n",
      " [691 501]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.55      0.54      1125\n",
      "         1.0       0.56      0.55      0.55      1192\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.55      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[617 508]\n",
      " [542 650]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.63      0.58      1125\n",
      "         1.0       0.59      0.51      0.54      1192\n",
      "\n",
      "    accuracy                           0.56      2317\n",
      "   macro avg       0.57      0.57      0.56      2317\n",
      "weighted avg       0.57      0.56      0.56      2317\n",
      "\n",
      "[[704 421]\n",
      " [589 603]]\n",
      "ABC Accuracy = 0.5571860164005179 , LDA Accuracy = 0.5429434613724644 , LR Accuracy = 0.5602071644367717 , GBC Accuracy = 0.5468277945619335 , LTSM Accuracy = 0.5640914976262408\n",
      " \n",
      "[    0     1     2 ... 13901 13902 13903] [13904 13905 13906 ... 16218 16219 16220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.48      0.51      1120\n",
      "         1.0       0.56      0.60      0.58      1197\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.54      0.54      2317\n",
      "weighted avg       0.55      0.55      0.54      2317\n",
      "\n",
      "[[543 577]\n",
      " [474 723]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.62      0.57      1120\n",
      "         1.0       0.57      0.46      0.51      1197\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.54      0.54      0.54      2317\n",
      "weighted avg       0.54      0.54      0.54      2317\n",
      "\n",
      "[[699 421]\n",
      " [647 550]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.529921   0.52984908        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.57      0.55      1120\n",
      "         1.0       0.56      0.52      0.54      1197\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.54      0.54      0.54      2317\n",
      "weighted avg       0.55      0.54      0.54      2317\n",
      "\n",
      "[[641 479]\n",
      " [579 618]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.44      0.49      1120\n",
      "         1.0       0.55      0.65      0.60      1197\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.54      2317\n",
      "weighted avg       0.55      0.55      0.54      2317\n",
      "\n",
      "[[497 623]\n",
      " [423 774]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.50      0.51      1120\n",
      "         1.0       0.56      0.59      0.58      1197\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.54      0.54      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[555 565]\n",
      " [486 711]]\n",
      "ABC Accuracy = 0.5390591281829953 , LDA Accuracy = 0.543375053949072 , LR Accuracy = 0.5463962019853259 , GBC Accuracy = 0.5485541648683643 , LTSM Accuracy = 0.5463962019853259\n",
      " \n",
      "[    0     1     2 ... 16218 16219 16220] [16221 16222 16223 ... 18535 18536 18537]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.84      0.62      1100\n",
      "         1.0       0.58      0.20      0.29      1217\n",
      "\n",
      "    accuracy                           0.50      2317\n",
      "   macro avg       0.53      0.52      0.46      2317\n",
      "weighted avg       0.54      0.50      0.45      2317\n",
      "\n",
      "[[927 173]\n",
      " [978 239]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.65      0.57      1100\n",
      "         1.0       0.57      0.43      0.49      1217\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.54      0.54      0.53      2317\n",
      "weighted avg       0.54      0.53      0.53      2317\n",
      "\n",
      "[[714 386]\n",
      " [698 519]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 64 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 66 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.54466388 0.54491049        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lsqr'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.56      0.54      1100\n",
      "         1.0       0.57      0.53      0.55      1217\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.54      0.54      0.54      2317\n",
      "weighted avg       0.54      0.54      0.54      2317\n",
      "\n",
      "[[614 486]\n",
      " [575 642]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.42      0.46      1100\n",
      "         1.0       0.55      0.65      0.60      1217\n",
      "\n",
      "    accuracy                           0.54      2317\n",
      "   macro avg       0.54      0.54      0.53      2317\n",
      "weighted avg       0.54      0.54      0.53      2317\n",
      "\n",
      "[[460 640]\n",
      " [423 794]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.49      0.86      0.62      1100\n",
      "         1.0       0.59      0.18      0.28      1217\n",
      "\n",
      "    accuracy                           0.50      2317\n",
      "   macro avg       0.54      0.52      0.45      2317\n",
      "weighted avg       0.54      0.50      0.44      2317\n",
      "\n",
      "[[950 150]\n",
      " [999 218]]\n",
      "ABC Accuracy = 0.5321536469572723 , LDA Accuracy = 0.542080276219249 , LR Accuracy = 0.5032369443245576 , GBC Accuracy = 0.5412170910660337 , LTSM Accuracy = 0.5041001294777729\n",
      " \n",
      "[    0     1     2 ... 18535 18536 18537] [18538 18539 18540 ... 20852 20853 20854]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.52      0.52      1139\n",
      "         1.0       0.53      0.54      0.54      1178\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.53      0.53      0.53      2317\n",
      "weighted avg       0.53      0.53      0.53      2317\n",
      "\n",
      "[[587 552]\n",
      " [543 635]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.58      0.55      1139\n",
      "         1.0       0.54      0.48      0.51      1178\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.53      0.53      0.53      2317\n",
      "weighted avg       0.53      0.53      0.53      2317\n",
      "\n",
      "[[661 478]\n",
      " [608 570]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 64 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.54957383 0.54984351        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lsqr'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.86      0.63      1139\n",
      "         1.0       0.56      0.18      0.27      1178\n",
      "\n",
      "    accuracy                           0.51      2317\n",
      "   macro avg       0.53      0.52      0.45      2317\n",
      "weighted avg       0.53      0.51      0.45      2317\n",
      "\n",
      "[[975 164]\n",
      " [967 211]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.40      0.45      1139\n",
      "         1.0       0.53      0.65      0.58      1178\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.53      0.52      0.52      2317\n",
      "weighted avg       0.53      0.53      0.52      2317\n",
      "\n",
      "[[454 685]\n",
      " [411 767]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.57      0.54      1139\n",
      "         1.0       0.54      0.48      0.51      1178\n",
      "\n",
      "    accuracy                           0.52      2317\n",
      "   macro avg       0.53      0.53      0.52      2317\n",
      "weighted avg       0.53      0.52      0.52      2317\n",
      "\n",
      "[[645 494]\n",
      " [607 571]]\n",
      "ABC Accuracy = 0.5312904618040569 , LDA Accuracy = 0.5118687958567113 , LR Accuracy = 0.5274061286145878 , GBC Accuracy = 0.5269745360379802 , LTSM Accuracy = 0.5248165731549418\n",
      " \n",
      "[    0     1     2 ... 20852 20853 20854] [20855 20856 20857 ... 23169 23170 23171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.43      0.47      1151\n",
      "         1.0       0.53      0.62      0.57      1166\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.53      0.53      0.52      2317\n",
      "weighted avg       0.53      0.53      0.52      2317\n",
      "\n",
      "[[495 656]\n",
      " [440 726]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.55      0.54      1151\n",
      "         1.0       0.53      0.50      0.51      1166\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.53      0.53      0.53      2317\n",
      "weighted avg       0.53      0.53      0.53      2317\n",
      "\n",
      "[[637 514]\n",
      " [584 582]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 64 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.54054184 0.54011029        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.51      0.87      0.64      1151\n",
      "         1.0       0.56      0.17      0.26      1166\n",
      "\n",
      "    accuracy                           0.52      2317\n",
      "   macro avg       0.53      0.52      0.45      2317\n",
      "weighted avg       0.53      0.52      0.45      2317\n",
      "\n",
      "[[1000  151]\n",
      " [ 972  194]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.37      0.44      1151\n",
      "         1.0       0.53      0.69      0.60      1166\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.54      0.53      0.52      2317\n",
      "weighted avg       0.54      0.53      0.52      2317\n",
      "\n",
      "[[429 722]\n",
      " [360 806]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.44      0.48      1151\n",
      "         1.0       0.53      0.61      0.57      1166\n",
      "\n",
      "    accuracy                           0.53      2317\n",
      "   macro avg       0.53      0.53      0.53      2317\n",
      "weighted avg       0.53      0.53      0.53      2317\n",
      "\n",
      "[[512 639]\n",
      " [453 713]]\n",
      "ABC Accuracy = 0.5261113508847648 , LDA Accuracy = 0.5153215364695727 , LR Accuracy = 0.5269745360379802 , GBC Accuracy = 0.5330168321104877 , LTSM Accuracy = 0.5287009063444109\n",
      " \n",
      "[    0     1     2 ... 23169 23170 23171] [23172 23173 23174 ... 25486 25487 25488]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.57      0.57      1166\n",
      "         1.0       0.56      0.54      0.55      1151\n",
      "\n",
      "    accuracy                           0.56      2317\n",
      "   macro avg       0.56      0.56      0.56      2317\n",
      "weighted avg       0.56      0.56      0.56      2317\n",
      "\n",
      "[[669 497]\n",
      " [530 621]]\n",
      "{'learning_rate': 0.001, 'n_estimators': 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.57      0.56      1166\n",
      "         1.0       0.55      0.53      0.54      1151\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.55      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[662 504]\n",
      " [542 609]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 6 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 605, in fit\n",
      "    self._solve_eigen(\n",
      "  File \"C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\discriminant_analysis.py\", line 445, in _solve_eigen\n",
      "    evals, evecs = linalg.eigh(Sb, Sw)\n",
      "  File \"C:\\Users\\mario\\anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp.py\", line 578, in eigh\n",
      "    raise LinAlgError('The leading minor of order {} of B is not '\n",
      "numpy.linalg.LinAlgError: The leading minor of order 65 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\mario\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.53823458 0.53797562        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'svd'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.46      0.51      1166\n",
      "         1.0       0.54      0.65      0.59      1151\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.56      0.55      0.55      2317\n",
      "weighted avg       0.56      0.55      0.55      2317\n",
      "\n",
      "[[540 626]\n",
      " [408 743]]\n",
      "{'criterion': 'friedman_mse', 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.44      0.49      1166\n",
      "         1.0       0.54      0.66      0.59      1151\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.54      2317\n",
      "weighted avg       0.55      0.55      0.54      2317\n",
      "\n",
      "[[510 656]\n",
      " [390 761]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.56      0.56      1166\n",
      "         1.0       0.55      0.54      0.55      1151\n",
      "\n",
      "    accuracy                           0.55      2317\n",
      "   macro avg       0.55      0.55      0.55      2317\n",
      "weighted avg       0.55      0.55      0.55      2317\n",
      "\n",
      "[[653 513]\n",
      " [525 626]]\n",
      "ABC Accuracy = 0.5485541648683643 , LDA Accuracy = 0.5537332757876564 , LR Accuracy = 0.5567544238239103 , GBC Accuracy = 0.5485541648683643 , LTSM Accuracy = 0.5520069054812258\n",
      " \n",
      " \n",
      "ABC Accuracy = 0.546741476046612\n",
      "LDA Accuracy = 0.5341821320673285\n",
      "LR Accuracy = 0.5441950798446266\n",
      "GBC Accuracy = 0.5471730686232197\n",
      "LTSM Accuracy = 0.5424255502805352\n",
      " \n",
      "CPU times: total: 7h 57min 6s\n",
      "Wall time: 7h 38min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "LTSM_RESULTS = []\n",
    "abc_RESULTS = []\n",
    "lda_RESULTS = []\n",
    "lr_RESULTS =[]\n",
    "GBC_RESULTS = []\n",
    "    \n",
    "for train_index, test_index in tscv.split(X_JMJ):\n",
    "    print(train_index, test_index)\n",
    "    \n",
    "    X_train, X_test = X_JMJ.iloc[train_index], X_JMJ.iloc[test_index]\n",
    "    y_train, y_test = y_JMJ.iloc[train_index], y_JMJ.iloc[test_index]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #Setting up the Base Estimators\n",
    "    lr_model = _LogisticRegression(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    abc_model = _train_AdaBoostClassifier(X_train_scaled, y_train,  X_test_scaled, y_test)\n",
    "    lda_model = _train_LDA(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    GBC_model = _GradientBoosingClassifier(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    # Define the stacking classifier\n",
    "    stacking_classifier = StackingClassifier(\n",
    "        estimators=[('lr', lr_model),('abc', abc_model),('lda', lda_model), ('GBC',GBC_model)], \n",
    "        final_estimator=lr_model,\n",
    "        cv=5,\n",
    "        passthrough=True,\n",
    "        stack_method='predict_proba'\n",
    "    )\n",
    "    \n",
    "    # Train the stacking classifier on the training data\n",
    "    stacking_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = stacking_classifier.predict(X_test_scaled)\n",
    "    testy_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    abc_prediction = abc_model.predict(X_test_scaled)\n",
    "    lda_prediction = lda_model.predict(X_test_scaled)\n",
    "    lr_prediction = lr_model.predict(X_test_scaled)\n",
    "    GBC_prediction = GBC_model.predict(X_test_scaled)\n",
    "\n",
    "    \n",
    "    abc_accuracy = accuracy_score(y_test.values, abc_prediction)\n",
    "    lda_accuracy = accuracy_score(y_test.values, lda_prediction)\n",
    "    lr_accuracy = accuracy_score(y_test.values, lr_prediction)\n",
    "    GBC_accuracy = accuracy_score(y_test.values, GBC_prediction)\n",
    "    LTSMaccurary = accuracy_score (y_test, testy_pred)   \n",
    "   \n",
    "\n",
    "\n",
    "    abc_RESULTS.append(abc_accuracy)\n",
    "    lda_RESULTS.append(lda_accuracy)\n",
    "    lr_RESULTS.append(lr_accuracy)\n",
    "    GBC_RESULTS.append(GBC_accuracy)\n",
    "    LTSM_RESULTS.append(LTSMaccurary)\n",
    "    \n",
    "#     # Print classification report\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "#     print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, testy_pred))\n",
    "    print(confusion_matrix(y_test, testy_pred))\n",
    "    \n",
    "    print('ABC Accuracy = ' + str(abc_accuracy),', LDA Accuracy = ' + str(lda_accuracy),', LR Accuracy = ' + str(lr_accuracy),', GBC Accuracy = ' + str(GBC_accuracy),', LTSM Accuracy = ' + str(LTSMaccurary))#,GBC_accuracy, ensemble_accuracy)    \n",
    "    print(' ')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "print(' ')\n",
    "print('ABC Accuracy = ' + str( sum(abc_RESULTS) / len(abc_RESULTS)))\n",
    "print('LDA Accuracy = ' + str( sum(lda_RESULTS) / len(lda_RESULTS)))\n",
    "print('LR Accuracy = ' + str( sum(lr_RESULTS) / len(lr_RESULTS)))\n",
    "print('GBC Accuracy = ' + str( sum(GBC_RESULTS) / len(GBC_RESULTS)))\n",
    "print('LTSM Accuracy = ' + str( sum(LTSM_RESULTS) / len(LTSM_RESULTS)))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65e739e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(cv=5,\n",
       "                   estimators=[(&#x27;lr&#x27;,\n",
       "                                LogisticRegression(C=0.01, max_iter=1000)),\n",
       "                               (&#x27;abc&#x27;,\n",
       "                                AdaBoostClassifier(learning_rate=0.001,\n",
       "                                                   n_estimators=20)),\n",
       "                               (&#x27;lda&#x27;, LinearDiscriminantAnalysis()),\n",
       "                               (&#x27;GBC&#x27;,\n",
       "                                GradientBoostingClassifier(learning_rate=0.01))],\n",
       "                   final_estimator=LogisticRegression(C=0.01, max_iter=1000),\n",
       "                   passthrough=True, stack_method=&#x27;predict_proba&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(cv=5,\n",
       "                   estimators=[(&#x27;lr&#x27;,\n",
       "                                LogisticRegression(C=0.01, max_iter=1000)),\n",
       "                               (&#x27;abc&#x27;,\n",
       "                                AdaBoostClassifier(learning_rate=0.001,\n",
       "                                                   n_estimators=20)),\n",
       "                               (&#x27;lda&#x27;, LinearDiscriminantAnalysis()),\n",
       "                               (&#x27;GBC&#x27;,\n",
       "                                GradientBoostingClassifier(learning_rate=0.01))],\n",
       "                   final_estimator=LogisticRegression(C=0.01, max_iter=1000),\n",
       "                   passthrough=True, stack_method=&#x27;predict_proba&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01, max_iter=1000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>abc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(learning_rate=0.001, n_estimators=20)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lda</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>GBC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01, max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('lr',\n",
       "                                LogisticRegression(C=0.01, max_iter=1000)),\n",
       "                               ('abc',\n",
       "                                AdaBoostClassifier(learning_rate=0.001,\n",
       "                                                   n_estimators=20)),\n",
       "                               ('lda', LinearDiscriminantAnalysis()),\n",
       "                               ('GBC',\n",
       "                                GradientBoostingClassifier(learning_rate=0.01))],\n",
       "                   final_estimator=LogisticRegression(C=0.01, max_iter=1000),\n",
       "                   passthrough=True, stack_method='predict_proba')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585825d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier(cv=5,\n",
      "                   estimators=[('lr',\n",
      "                                LogisticRegression(C=0.01, max_iter=1000)),\n",
      "                               ('abc',\n",
      "                                AdaBoostClassifier(learning_rate=0.001,\n",
      "                                                   n_estimators=20)),\n",
      "                               ('lda', LinearDiscriminantAnalysis()),\n",
      "                               ('GBC',\n",
      "                                GradientBoostingClassifier(learning_rate=0.01))],\n",
      "                   final_estimator=LogisticRegression(C=0.01, max_iter=1000),\n",
      "                   passthrough=True, stack_method='predict_proba')\n"
     ]
    }
   ],
   "source": [
    "print(stacking_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56493c1e",
   "metadata": {},
   "source": [
    "#### SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "150d57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#saving model\n",
    "pkl_filename = 'model/Stacking_LR_model_JT2019to2022TEST_news_google.pkl'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(stacking_classifier, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03316101",
   "metadata": {},
   "source": [
    "#### LOADING MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "pkl_filename = 'model/Stacking_LR_model_JT2019to2022TEST_news_google.pkl'\n",
    "with open(pkl_filename, 'rb') as file: \n",
    "    Model2 = pickle.load(file)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be14ad0",
   "metadata": {},
   "source": [
    "# TESTING RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f4a398",
   "metadata": {},
   "source": [
    "### Testing with Whole Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7d9b5b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.41      0.48      3625\n",
      "         1.0       0.54      0.68      0.60      3629\n",
      "\n",
      "    accuracy                           0.55      7254\n",
      "   macro avg       0.55      0.55      0.54      7254\n",
      "weighted avg       0.55      0.55      0.54      7254\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']\n",
    "\n",
    "testdata2022 ='assets/TEST_MergeJMJPCHourly2019202_NewsGoogleApi_ActualLabel.csv'\n",
    "\n",
    "files= [testdata2022]\n",
    "\n",
    "def _exponential_smooth(data, alpha):\n",
    "    \"\"\"\n",
    "    Function that exponentially smooths dataset so values are less 'rigid'\n",
    "    :param alpha: weight factor to weight recent values more\n",
    "    \"\"\"\n",
    "    \n",
    "    return data.ewm(alpha=alpha).mean()\n",
    "\n",
    "def _get_indicator_data(data):\n",
    "    \"\"\"\n",
    "    Function that uses the finta API to calculate technical indicators used as the features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for indicator in INDICATORS:\n",
    "        ind_data = eval('TA.' + indicator + '(data)')\n",
    "        if not isinstance(ind_data, pd.DataFrame):\n",
    "            ind_data = ind_data.to_frame()\n",
    "        data = data.merge(ind_data, left_index=True, right_index=True)\n",
    "    data.rename(columns={\"14 period EMV.\": '14 period EMV'}, inplace=True)\n",
    "\n",
    "    # Also calculate moving averages for features\n",
    "    data['ema50'] = data['Close'] / data['Close'].ewm(50).mean()\n",
    "    data['ema21'] = data['Close'] / data['Close'].ewm(21).mean()\n",
    "    data['ema15'] = data['Close'] / data['Close'].ewm(14).mean()\n",
    "    data['ema5'] = data['Close'] / data['Close'].ewm(5).mean()\n",
    "\n",
    "    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "    data['normVol'] = data['Volume'] / data['Volume'].ewm(5).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "for file,x in zip(files, ['testdata2022']):\n",
    "    PredictDF= pd.read_csv(file)\n",
    "    prediction = (PredictDF.shift(-1)['Close'] >= PredictDF['Close'])\n",
    "    prediction = prediction.iloc[:-1]\n",
    "    PredictDF['Actual_Label'] = prediction.astype(int)\n",
    "    PredictDF= PredictDF.dropna()\n",
    "    \n",
    "    Indicatordata = _exponential_smooth(PredictDF[['Close', 'Open','High','Low','Volume']], 0.65)\n",
    "\n",
    "    Indicatordatafinal = _get_indicator_data(Indicatordata)\n",
    "\n",
    "    Indicatordatafinal = Indicatordatafinal.drop(['Close', 'Open', 'High', 'Low', 'Volume'], axis = 1)\n",
    "    PredictDF = pd.merge(PredictDF, Indicatordatafinal, left_index=True, right_index=True)\n",
    "    PredictDF = PredictDF.drop(['time','hour','Open Time','_merge','Signal','Position', 'Signal35JMJ', 'Position35JMJ', 'Ignore'], axis = 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "           'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "           'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open', 'High',\n",
    "           'Low', 'Close', 'Volume', 'Quote Asset Volume', 'Number of Trades',\n",
    "           'TB Base Volume', 'TB Quote Volume', '3MovingAverage', '5MovingAverage',\n",
    "           'JMJ_3HMoving_averages', 'JMJ_5HMoving_averages', \n",
    "           '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "           '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV', '20 period CCI',\n",
    "           '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21', 'ema15', 'ema5']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for column in columns: \n",
    "        PredictDF['Binary{}'.format(column)]  = (PredictDF[column] - PredictDF[column].shift(1)).apply(binary)\n",
    "    \n",
    "\n",
    "    PredictDF = PredictDF.dropna()\n",
    "   \n",
    "    \n",
    "    feature_names_JMJ = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "       'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "       'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open',\n",
    "       'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume',\n",
    "       'Number of Trades', 'TB Base Volume', 'TB Quote Volume',\n",
    "       '3MovingAverage', '5MovingAverage', 'JMJ_3HMoving_averages',\n",
    "        'Bitcoin_Google_Trend_Score', 'BTC_Google_Trend_Score',\n",
    "       'JMJ_5HMoving_averages', 'Mkt Sentiment',\n",
    "       'Crypto Sentiment', 'Historically Optimal SMA(s-t)',\n",
    "       'Historically Optimal SMA(l-t)', 'Historically Optimal WMA(s-t)',\n",
    "       'Historically Optimal WMA(l-t)', 'Historically Optimal EMA(s-t)',\n",
    "       'Historically Optimal EMA(l-t)',\n",
    "       'Twitter Hourly Favorites SMA(s-t)',\n",
    "       'Twitter Hourly Favorites SMA(l-t)',\n",
    "       'Twitter Hourly Favorites WMA(s-t)',\n",
    "       'Twitter Hourly Favorites WMA(l-t)',\n",
    "       'Twitter Hourly Favorites EMA(s-t)',\n",
    "       'Twitter Hourly Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Retweets SMA(s-t)',\n",
    "       'Twitter Hourly Retweets SMA(l-t)',\n",
    "       'Twitter Hourly Retweets WMA(s-t)',\n",
    "       'Twitter Hourly Retweets WMA(l-t)',\n",
    "       'Twitter Hourly Retweets EMA(s-t)',\n",
    "       'Twitter Hourly Retweets EMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(l-t)',\n",
    "       'Twitter W1 Score SMA(s-t)', 'Twitter W1 Score SMA(l-t)',\n",
    "       'Twitter W1 Score WMA(s-t)', 'Twitter W1 Score WMA(l-t)',\n",
    "       'Twitter W1 Score EMA(s-t)', 'Twitter W1 Score EMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(l-t)',\n",
    "       'Quote Asset Volume SMA(s-t)', 'Quote Asset Volume SMA(l-t)',\n",
    "       'Quote Asset Volume WMA(s-t)', 'Quote Asset Volume WMA(l-t)',\n",
    "       'Quote Asset Volume EMA(s-t)', 'Quote Asset Volume EMA(l-t)',\n",
    "       '# of Hourly Trades SMA(s-t)', '# of Hourly Trades SMA(l-t)',\n",
    "       '# of Hourly Trades WMA(s-t)', '# of Hourly Trades WMA(l-t)',\n",
    "       '# of Hourly Trades EMA(s-t)', '# of Hourly Trades EMA(l-t)',\n",
    "       'TB Base Volume SMA(s-t)', 'TB Base Volume SMA(l-t)',\n",
    "       'TB Base Volume WMA(s-t)', 'TB Base Volume WMA(l-t)',\n",
    "       'TB Base Volume EMA(s-t)', 'TB Base Volume EMA(l-t)',\n",
    "       'TB Quote Volume SMA(s-t)', 'TB Quote Volume SMA(l-t)',\n",
    "       'TB Quote Volume WMA(s-t)', 'TB Quote Volume WMA(l-t)',\n",
    "       'TB Quote Volume EMA(s-t)', 'TB Quote Volume EMA(l-t)',\n",
    "       '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "       '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV',\n",
    "       '20 period CCI', '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21',\n",
    "       'ema15', 'ema5', 'normVol', 'Binaryfavorites', 'Binaryretweets',\n",
    "       'Binarynumber_of_followers', 'Binaryfollowing',\n",
    "       'Binaryfollowers_following_ratio', 'Binary2x_retweets_+_favorites',\n",
    "       'Binarypolarity', 'BinaryW1 Score', 'BinaryBull_ratio',\n",
    "       'BinaryW Score With Bull Ratio', 'BinaryOpen', 'BinaryHigh',\n",
    "       'BinaryLow', 'BinaryClose', 'BinaryVolume',\n",
    "       'BinaryQuote Asset Volume', 'BinaryNumber of Trades',\n",
    "       'BinaryTB Base Volume', 'BinaryTB Quote Volume',\n",
    "       'Binary3MovingAverage', 'Binary5MovingAverage',\n",
    "       'BinaryJMJ_3HMoving_averages', 'BinaryJMJ_5HMoving_averages',\n",
    "       'Binary14 period RSI', 'BinaryMACD', 'BinarySIGNAL',\n",
    "       'Binary14 period STOCH %K', 'BinaryMFV', 'Binary14 period ATR',\n",
    "       'BinaryMOM', 'Binary14 period MFI', 'BinaryROC', 'BinaryOBV',\n",
    "       'Binary20 period CCI', 'Binary14 period EMV', 'BinaryVIm',\n",
    "       'BinaryVIp', 'Binaryema50', 'Binaryema21', 'Binaryema15',\n",
    "       'Binaryema5']\n",
    "\n",
    "    X_test = PredictDF[feature_names_JMJ]\n",
    "    y_test = PredictDF['Actual_Label']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    predict_y=  stacking_classifier.predict(X_test_scaled)\n",
    "    PredictDF['Predicted_Label']= predict_y\n",
    "\n",
    "    print(classification_report(y_test, predict_y))   \n",
    "    \n",
    "    # confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "    confusion_mc = confusion_matrix(PredictDF['Actual_Label'], PredictDF['Predicted_Label'])\n",
    "    df_cm = pd.DataFrame(confusion_mc, \n",
    "                         index = [i for i in range(0,2)], columns = [i for i in range(0,2)])\n",
    "\n",
    "\n",
    "    PredictDF['ValueActual'] =0\n",
    "    PredictDF['BTCValueActual'] =0\n",
    "    if PredictDF.loc[PredictDF.index[0],'Actual_Label']==0.0:\n",
    "        PredictDF.loc[PredictDF.index[0],'ValueActual']=1000\n",
    "    else:\n",
    "        PredictDF.loc[PredictDF.index[0],'BTCValueActual']=round((1000/PredictDF.loc[PredictDF.index[0],'Close']),5)\n",
    "\n",
    "\n",
    "    for current in range(1, len(PredictDF.index)):\n",
    "        previous = current - 1\n",
    "\n",
    "        if PredictDF.loc[PredictDF.index[current],'Actual_Label']==0 and PredictDF.loc[PredictDF.index[previous],'BTCValueActual']==0:\n",
    "            PredictDF.loc[PredictDF.index[current],'ValueActual']=PredictDF.loc[PredictDF.index[previous],'ValueActual']\n",
    "        elif PredictDF.loc[PredictDF.index[current],'Actual_Label']==1 and PredictDF.loc[PredictDF.index[previous],'ValueActual'] ==0 :\n",
    "            PredictDF.loc[PredictDF.index[current],'ValueActual']= 0\n",
    "            PredictDF.loc[PredictDF.index[current],'BTCValueActual']=round(PredictDF.loc[PredictDF.index[previous],'BTCValueActual'],3)\n",
    "        elif PredictDF.loc[PredictDF.index[current],'Actual_Label']==1:\n",
    "            PredictDF.loc[PredictDF.index[current],'BTCValueActual'] = round((PredictDF.loc[PredictDF.index[previous],'ValueActual']/PredictDF.loc[PredictDF.index[current],'Close']),5)\n",
    "        else:\n",
    "            PredictDF.loc[PredictDF.index[current],'ValueActual'] = round((PredictDF.loc[PredictDF.index[previous],'BTCValueActual'] *PredictDF.loc[PredictDF.index[current],'Close']),3)\n",
    "\n",
    "\n",
    "    PredictDF['ValuePredicted'] =0\n",
    "    PredictDF['BTCValuePredicted'] =0\n",
    "    if PredictDF.loc[PredictDF.index[0],'Predicted_Label']==0.0:\n",
    "        PredictDF.loc[PredictDF.index[0],'ValuePredicted']=1000\n",
    "    else:\n",
    "        PredictDF.loc[PredictDF.index[0],'BTCValuePredicted']=round((1000/PredictDF.loc[PredictDF.index[0],'Close']),5)\n",
    "\n",
    "\n",
    "    for current in range(1, len(PredictDF.index)):\n",
    "        previous = current - 1\n",
    "\n",
    "        if PredictDF.loc[PredictDF.index[current],'Predicted_Label']==0 and PredictDF.loc[PredictDF.index[previous],'BTCValuePredicted']==0:\n",
    "            PredictDF.loc[PredictDF.index[current],'ValuePredicted']=PredictDF.loc[PredictDF.index[previous],'ValuePredicted']\n",
    "        elif PredictDF.loc[PredictDF.index[current],'Predicted_Label']==1 and PredictDF.loc[PredictDF.index[previous],'ValuePredicted'] ==0 :\n",
    "            PredictDF.loc[PredictDF.index[current],'ValuePredicted']= 0\n",
    "            PredictDF.loc[PredictDF.index[current],'BTCValuePredicted']=round(PredictDF.loc[PredictDF.index[previous],'BTCValuePredicted'],3)\n",
    "        elif PredictDF.loc[PredictDF.index[current],'Predicted_Label']==1:\n",
    "            PredictDF.loc[PredictDF.index[current],'BTCValuePredicted'] = round((PredictDF.loc[PredictDF.index[previous],'ValuePredicted']/PredictDF.loc[PredictDF.index[current],'Close']),5)\n",
    "        else:\n",
    "            PredictDF.loc[PredictDF.index[current],'ValuePredicted'] = round((PredictDF.loc[PredictDF.index[previous],'BTCValuePredicted'] *PredictDF.loc[PredictDF.index[current],'Close']),3)\n",
    "    df = PredictDF.mask(PredictDF==0).ffill().iloc[[-1]]\n",
    "    LastPredictvalue = df['ValuePredicted'].values[0]\n",
    "    LastPredictBTC = df['BTCValuePredicted'].values[0]\n",
    "    # LastPredictvalue= PredictDF['ValuePredicted'][PredictDF['ValuePredicted'].to_numpy().nonzero()[0][-1]+1]\n",
    "    # LastPredictBTC= PredictDF['BTCValuePredicted'][PredictDF['BTCValuePredicted'].to_numpy().nonzero()[0][-1]+1]\n",
    "\n",
    "    #     print('Total USD Value and BTC Benchmark Actual Label: ${:,.2f} and {:,.2f} BTC'.format (max(PredictDF['ValueActual']),max(PredictDF['BTCValueActual'])))#, )       \n",
    "    #     print('Total USD Value and BTC Benchmark Actual Label: ${:,.2f} and {:,.2f} BTC'.format (LastPredictvalue,LastPredictBTC))#, )       \n",
    "    #     print (' ')\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(2,2))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "    plt.title('Assessment for: {}\\nAccuracy:{:.3f}\\nPrecision:{:.3f}\\nRecall:{:.3f}\\nF1:{:.3f}\\nBenchmark USD Value and BTC  ${:,.2f} and {:,.2f}\\nPredicted USD Value and BTC ${:,.2f} and {:,.2f}'.format(x, accuracy_score(PredictDF['Actual_Label'],PredictDF['Predicted_Label']),\n",
    "                                                                        precision_score(PredictDF['Actual_Label'],PredictDF['Predicted_Label']),\n",
    "                                                                                    recall_score(PredictDF['Actual_Label'], PredictDF['Predicted_Label']),\n",
    "                                                                                    f1_score(PredictDF['Actual_Label'], PredictDF['Predicted_Label']),\n",
    "                                                                                     max(PredictDF['ValueActual']),max(PredictDF['BTCValueActual']),\n",
    "                                                                                     LastPredictvalue,LastPredictBTC\n",
    "                                                                                    ))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcbfac",
   "metadata": {},
   "source": [
    "## Trying with Thesholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9941c008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.11      0.18       643\n",
      "         1.0       0.57      0.96      0.71       801\n",
      "\n",
      "    accuracy                           0.58      1444\n",
      "   macro avg       0.61      0.53      0.45      1444\n",
      "weighted avg       0.61      0.58      0.48      1444\n",
      "\n",
      "[[ 69 574]\n",
      " [ 36 765]]\n"
     ]
    }
   ],
   "source": [
    "INDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']\n",
    "\n",
    "testdata2022 ='assets/newfeatures/TEST_MergeJMJPCHourly2019202_NewsGoogleApi_ActualLabel.csv'\n",
    "\n",
    "files= [testdata2022]\n",
    "\n",
    "def _exponential_smooth(data, alpha):\n",
    "    \"\"\"\n",
    "    Function that exponentially smooths dataset so values are less 'rigid'\n",
    "    :param alpha: weight factor to weight recent values more\n",
    "    \"\"\"\n",
    "    \n",
    "    return data.ewm(alpha=alpha).mean()\n",
    "\n",
    "def _get_indicator_data(data):\n",
    "    \"\"\"\n",
    "    Function that uses the finta API to calculate technical indicators used as the features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for indicator in INDICATORS:\n",
    "        ind_data = eval('TA.' + indicator + '(data)')\n",
    "        if not isinstance(ind_data, pd.DataFrame):\n",
    "            ind_data = ind_data.to_frame()\n",
    "        data = data.merge(ind_data, left_index=True, right_index=True)\n",
    "    data.rename(columns={\"14 period EMV.\": '14 period EMV'}, inplace=True)\n",
    "\n",
    "    # Also calculate moving averages for features\n",
    "    data['ema50'] = data['Close'] / data['Close'].ewm(50).mean()\n",
    "    data['ema21'] = data['Close'] / data['Close'].ewm(21).mean()\n",
    "    data['ema15'] = data['Close'] / data['Close'].ewm(14).mean()\n",
    "    data['ema5'] = data['Close'] / data['Close'].ewm(5).mean()\n",
    "\n",
    "    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "    data['normVol'] = data['Volume'] / data['Volume'].ewm(5).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "UpperThreshold = 0.6\n",
    "LowerThresshold = 0.4\n",
    "\n",
    "for file,x in zip(files, ['testdata2022']):\n",
    "    PredictDF= pd.read_csv(file)\n",
    "    prediction = (PredictDF.shift(-1)['Close'] >= PredictDF['Close'])\n",
    "    prediction = prediction.iloc[:-1]\n",
    "    PredictDF['Actual_Label'] = prediction.astype(int)\n",
    "    PredictDF= PredictDF.dropna()\n",
    "    \n",
    "    Indicatordata = _exponential_smooth(PredictDF[['Close', 'Open','High','Low','Volume']], 0.65)\n",
    "\n",
    "    Indicatordatafinal = _get_indicator_data(Indicatordata)\n",
    "\n",
    "    Indicatordatafinal = Indicatordatafinal.drop(['Close', 'Open', 'High', 'Low', 'Volume'], axis = 1)\n",
    "    PredictDF = pd.merge(PredictDF, Indicatordatafinal, left_index=True, right_index=True)\n",
    "    PredictDF = PredictDF.drop(['time','hour','Open Time','_merge','Signal','Position', 'Signal35JMJ', 'Position35JMJ', 'Ignore'], axis = 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "           'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "           'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open', 'High',\n",
    "           'Low', 'Close', 'Volume', 'Quote Asset Volume', 'Number of Trades',\n",
    "           'TB Base Volume', 'TB Quote Volume', '3MovingAverage', '5MovingAverage',\n",
    "           'JMJ_3HMoving_averages', 'JMJ_5HMoving_averages', \n",
    "           '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "           '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV', '20 period CCI',\n",
    "           '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21', 'ema15', 'ema5']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for column in columns: \n",
    "        PredictDF['Binary{}'.format(column)]  = (PredictDF[column] - PredictDF[column].shift(1)).apply(binary)\n",
    "    \n",
    "\n",
    "    PredictDF = PredictDF.dropna()\n",
    "    \n",
    "    feature_names_JMJ = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "       'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "       'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open',\n",
    "       'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume',\n",
    "       'Number of Trades', 'TB Base Volume', 'TB Quote Volume',\n",
    "       '3MovingAverage', '5MovingAverage', 'JMJ_3HMoving_averages',\n",
    "        'Bitcoin_Google_Trend_Score', 'BTC_Google_Trend_Score',\n",
    "       'JMJ_5HMoving_averages', 'Mkt Sentiment',\n",
    "       'Crypto Sentiment', 'Historically Optimal SMA(s-t)',\n",
    "       'Historically Optimal SMA(l-t)', 'Historically Optimal WMA(s-t)',\n",
    "       'Historically Optimal WMA(l-t)', 'Historically Optimal EMA(s-t)',\n",
    "       'Historically Optimal EMA(l-t)',\n",
    "       'Twitter Hourly Favorites SMA(s-t)',\n",
    "       'Twitter Hourly Favorites SMA(l-t)',\n",
    "       'Twitter Hourly Favorites WMA(s-t)',\n",
    "       'Twitter Hourly Favorites WMA(l-t)',\n",
    "       'Twitter Hourly Favorites EMA(s-t)',\n",
    "       'Twitter Hourly Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Retweets SMA(s-t)',\n",
    "       'Twitter Hourly Retweets SMA(l-t)',\n",
    "       'Twitter Hourly Retweets WMA(s-t)',\n",
    "       'Twitter Hourly Retweets WMA(l-t)',\n",
    "       'Twitter Hourly Retweets EMA(s-t)',\n",
    "       'Twitter Hourly Retweets EMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(l-t)',\n",
    "       'Twitter W1 Score SMA(s-t)', 'Twitter W1 Score SMA(l-t)',\n",
    "       'Twitter W1 Score WMA(s-t)', 'Twitter W1 Score WMA(l-t)',\n",
    "       'Twitter W1 Score EMA(s-t)', 'Twitter W1 Score EMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(l-t)',\n",
    "       'Quote Asset Volume SMA(s-t)', 'Quote Asset Volume SMA(l-t)',\n",
    "       'Quote Asset Volume WMA(s-t)', 'Quote Asset Volume WMA(l-t)',\n",
    "       'Quote Asset Volume EMA(s-t)', 'Quote Asset Volume EMA(l-t)',\n",
    "       '# of Hourly Trades SMA(s-t)', '# of Hourly Trades SMA(l-t)',\n",
    "       '# of Hourly Trades WMA(s-t)', '# of Hourly Trades WMA(l-t)',\n",
    "       '# of Hourly Trades EMA(s-t)', '# of Hourly Trades EMA(l-t)',\n",
    "       'TB Base Volume SMA(s-t)', 'TB Base Volume SMA(l-t)',\n",
    "       'TB Base Volume WMA(s-t)', 'TB Base Volume WMA(l-t)',\n",
    "       'TB Base Volume EMA(s-t)', 'TB Base Volume EMA(l-t)',\n",
    "       'TB Quote Volume SMA(s-t)', 'TB Quote Volume SMA(l-t)',\n",
    "       'TB Quote Volume WMA(s-t)', 'TB Quote Volume WMA(l-t)',\n",
    "       'TB Quote Volume EMA(s-t)', 'TB Quote Volume EMA(l-t)',\n",
    "       '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "       '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV',\n",
    "       '20 period CCI', '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21',\n",
    "       'ema15', 'ema5', 'normVol', 'Binaryfavorites', 'Binaryretweets',\n",
    "       'Binarynumber_of_followers', 'Binaryfollowing',\n",
    "       'Binaryfollowers_following_ratio', 'Binary2x_retweets_+_favorites',\n",
    "       'Binarypolarity', 'BinaryW1 Score', 'BinaryBull_ratio',\n",
    "       'BinaryW Score With Bull Ratio', 'BinaryOpen', 'BinaryHigh',\n",
    "       'BinaryLow', 'BinaryClose', 'BinaryVolume',\n",
    "       'BinaryQuote Asset Volume', 'BinaryNumber of Trades',\n",
    "       'BinaryTB Base Volume', 'BinaryTB Quote Volume',\n",
    "       'Binary3MovingAverage', 'Binary5MovingAverage',\n",
    "       'BinaryJMJ_3HMoving_averages', 'BinaryJMJ_5HMoving_averages',\n",
    "       'Binary14 period RSI', 'BinaryMACD', 'BinarySIGNAL',\n",
    "       'Binary14 period STOCH %K', 'BinaryMFV', 'Binary14 period ATR',\n",
    "       'BinaryMOM', 'Binary14 period MFI', 'BinaryROC', 'BinaryOBV',\n",
    "       'Binary20 period CCI', 'Binary14 period EMV', 'BinaryVIm',\n",
    "       'BinaryVIp', 'Binaryema50', 'Binaryema21', 'Binaryema15',\n",
    "       'Binaryema5']\n",
    "\n",
    "\n",
    "    X_test = PredictDF[feature_names_JMJ]\n",
    "    y_test = PredictDF['Actual_Label']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#     predict_y1= np.argmax(stacking_classifier.predict_proba(X_test_scaled),axis=0)\n",
    "    predict_y=  Model2.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "    #PredictDF['Predicted_Label']= predict_y\n",
    "\n",
    "#     print(classification_report(y_test, predict_y))   \n",
    "\n",
    "    testy_predThreshold = np.where(predict_y > UpperThreshold, 1,np.where(predict_y < LowerThresshold, 0,2))\n",
    "    testy_predThreshold = testy_predThreshold.squeeze()\n",
    "    dataset = pd.DataFrame({'Y_TEST': y_test, 'Y_PREDICTED': testy_predThreshold}, columns=['Y_TEST', 'Y_PREDICTED'])\n",
    "    dataset2 = dataset.drop(dataset[dataset.Y_PREDICTED == 2].index)\n",
    "   \n",
    "    try:\n",
    "#         LTSMaccuracy = accuracy_score(dataset2['Y_TEST'].values, dataset2['Y_PREDICTED'].values)\n",
    "#         LTSM_RESULTS.append(LTSMaccuracy)\n",
    "    \n",
    "#         # Print classification report\n",
    "        print(classification_report(dataset2['Y_TEST'].values, dataset2['Y_PREDICTED'].values))\n",
    "        print(confusion_matrix(dataset2['Y_TEST'].values, dataset2['Y_PREDICTED'].values))\n",
    "    except:\n",
    "        print(\"No values\")\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad7d889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.41      0.48      3625\n",
      "         1.0       0.54      0.68      0.60      3629\n",
      "\n",
      "    accuracy                           0.55      7254\n",
      "   macro avg       0.55      0.55      0.54      7254\n",
      "weighted avg       0.55      0.55      0.54      7254\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86198c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c22f5b",
   "metadata": {},
   "source": [
    "### Exporting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d9f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']\n",
    "\n",
    "testdata2022 ='assets/TEST_MergeJMJPCHourly2019202_NewsGoogleApi_ActualLabel.csv'\n",
    "\n",
    "files= [testdata2022]\n",
    "\n",
    "def _exponential_smooth(data, alpha):\n",
    "    \"\"\"\n",
    "    Function that exponentially smooths dataset so values are less 'rigid'\n",
    "    :param alpha: weight factor to weight recent values more\n",
    "    \"\"\"\n",
    "    \n",
    "    return data.ewm(alpha=alpha).mean()\n",
    "\n",
    "def _get_indicator_data(data):\n",
    "    \"\"\"\n",
    "    Function that uses the finta API to calculate technical indicators used as the features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    for indicator in INDICATORS:\n",
    "        ind_data = eval('TA.' + indicator + '(data)')\n",
    "        if not isinstance(ind_data, pd.DataFrame):\n",
    "            ind_data = ind_data.to_frame()\n",
    "        data = data.merge(ind_data, left_index=True, right_index=True)\n",
    "    data.rename(columns={\"14 period EMV.\": '14 period EMV'}, inplace=True)\n",
    "\n",
    "    # Also calculate moving averages for features\n",
    "    data['ema50'] = data['Close'] / data['Close'].ewm(50).mean()\n",
    "    data['ema21'] = data['Close'] / data['Close'].ewm(21).mean()\n",
    "    data['ema15'] = data['Close'] / data['Close'].ewm(14).mean()\n",
    "    data['ema5'] = data['Close'] / data['Close'].ewm(5).mean()\n",
    "\n",
    "    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n",
    "    data['normVol'] = data['Volume'] / data['Volume'].ewm(5).mean()\n",
    "\n",
    "    return data\n",
    "\n",
    "UpperThreshold = 0.5\n",
    "LowerThresshold = 0.5\n",
    "\n",
    "for file,x in zip(files, ['testdata2022']):\n",
    "    PredictDF= pd.read_csv(file)\n",
    "    prediction = (PredictDF.shift(-1)['Close'] >= PredictDF['Close'])\n",
    "    prediction = prediction.iloc[:-1]\n",
    "    PredictDF['Actual_Label'] = prediction.astype(int)\n",
    "    PredictDF= PredictDF.dropna()\n",
    "    \n",
    "    Indicatordata = _exponential_smooth(PredictDF[['Close', 'Open','High','Low','Volume']], 0.65)\n",
    "\n",
    "    Indicatordatafinal = _get_indicator_data(Indicatordata)\n",
    "\n",
    "    Indicatordatafinal = Indicatordatafinal.drop(['Close', 'Open', 'High', 'Low', 'Volume'], axis = 1)\n",
    "    PredictDF = pd.merge(PredictDF, Indicatordatafinal, left_index=True, right_index=True)\n",
    "    PredictDF = PredictDF.drop(['time','hour','Open Time','_merge','Signal','Position', 'Signal35JMJ', 'Position35JMJ', 'Ignore'], axis = 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    columns = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "           'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "           'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open', 'High',\n",
    "           'Low', 'Close', 'Volume', 'Quote Asset Volume', 'Number of Trades',\n",
    "           'TB Base Volume', 'TB Quote Volume', '3MovingAverage', '5MovingAverage',\n",
    "           'JMJ_3HMoving_averages', 'JMJ_5HMoving_averages', \n",
    "           '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "           '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV', '20 period CCI',\n",
    "           '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21', 'ema15', 'ema5']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for column in columns: \n",
    "        PredictDF['Binary{}'.format(column)]  = (PredictDF[column] - PredictDF[column].shift(1)).apply(binary)\n",
    "    \n",
    "\n",
    "    PredictDF = PredictDF.dropna()\n",
    "    \n",
    "    feature_names_JMJ = ['favorites', 'retweets', 'number_of_followers', 'following',\n",
    "       'followers_following_ratio', '2x_retweets_+_favorites', 'polarity',\n",
    "       'W1 Score', 'Bull_ratio', 'W Score With Bull Ratio', 'Open',\n",
    "       'High', 'Low', 'Close', 'Volume', 'Quote Asset Volume',\n",
    "       'Number of Trades', 'TB Base Volume', 'TB Quote Volume',\n",
    "       '3MovingAverage', '5MovingAverage', 'JMJ_3HMoving_averages',\n",
    "        'Bitcoin_Google_Trend_Score', 'BTC_Google_Trend_Score',\n",
    "       'JMJ_5HMoving_averages', 'Mkt Sentiment',\n",
    "       'Crypto Sentiment', 'Historically Optimal SMA(s-t)',\n",
    "       'Historically Optimal SMA(l-t)', 'Historically Optimal WMA(s-t)',\n",
    "       'Historically Optimal WMA(l-t)', 'Historically Optimal EMA(s-t)',\n",
    "       'Historically Optimal EMA(l-t)',\n",
    "       'Twitter Hourly Favorites SMA(s-t)',\n",
    "       'Twitter Hourly Favorites SMA(l-t)',\n",
    "       'Twitter Hourly Favorites WMA(s-t)',\n",
    "       'Twitter Hourly Favorites WMA(l-t)',\n",
    "       'Twitter Hourly Favorites EMA(s-t)',\n",
    "       'Twitter Hourly Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Retweets SMA(s-t)',\n",
    "       'Twitter Hourly Retweets SMA(l-t)',\n",
    "       'Twitter Hourly Retweets WMA(s-t)',\n",
    "       'Twitter Hourly Retweets WMA(l-t)',\n",
    "       'Twitter Hourly Retweets EMA(s-t)',\n",
    "       'Twitter Hourly Retweets EMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Follower Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure SMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure WMA(l-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(s-t)',\n",
    "       'Twitter Hourly Following Exposure EMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Follower to Following Ratio EMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites SMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites WMA(l-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(s-t)',\n",
    "       'Twitter Hourly 2x Retweets + Favorites EMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score SMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score WMA(l-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(s-t)',\n",
    "       'Twitter Hourly Polarity Score EMA(l-t)',\n",
    "       'Twitter W1 Score SMA(s-t)', 'Twitter W1 Score SMA(l-t)',\n",
    "       'Twitter W1 Score WMA(s-t)', 'Twitter W1 Score WMA(l-t)',\n",
    "       'Twitter W1 Score EMA(s-t)', 'Twitter W1 Score EMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Bull Ratio EMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio SMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio WMA(l-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(s-t)',\n",
    "       'Twitter Hourly Weighted Bull Ratio EMA(l-t)',\n",
    "       'Quote Asset Volume SMA(s-t)', 'Quote Asset Volume SMA(l-t)',\n",
    "       'Quote Asset Volume WMA(s-t)', 'Quote Asset Volume WMA(l-t)',\n",
    "       'Quote Asset Volume EMA(s-t)', 'Quote Asset Volume EMA(l-t)',\n",
    "       '# of Hourly Trades SMA(s-t)', '# of Hourly Trades SMA(l-t)',\n",
    "       '# of Hourly Trades WMA(s-t)', '# of Hourly Trades WMA(l-t)',\n",
    "       '# of Hourly Trades EMA(s-t)', '# of Hourly Trades EMA(l-t)',\n",
    "       'TB Base Volume SMA(s-t)', 'TB Base Volume SMA(l-t)',\n",
    "       'TB Base Volume WMA(s-t)', 'TB Base Volume WMA(l-t)',\n",
    "       'TB Base Volume EMA(s-t)', 'TB Base Volume EMA(l-t)',\n",
    "       'TB Quote Volume SMA(s-t)', 'TB Quote Volume SMA(l-t)',\n",
    "       'TB Quote Volume WMA(s-t)', 'TB Quote Volume WMA(l-t)',\n",
    "       'TB Quote Volume EMA(s-t)', 'TB Quote Volume EMA(l-t)',\n",
    "       '14 period RSI', 'MACD', 'SIGNAL', '14 period STOCH %K', 'MFV',\n",
    "       '14 period ATR', 'MOM', '14 period MFI', 'ROC', 'OBV',\n",
    "       '20 period CCI', '14 period EMV', 'VIm', 'VIp', 'ema50', 'ema21',\n",
    "       'ema15', 'ema5', 'normVol', 'Binaryfavorites', 'Binaryretweets',\n",
    "       'Binarynumber_of_followers', 'Binaryfollowing',\n",
    "       'Binaryfollowers_following_ratio', 'Binary2x_retweets_+_favorites',\n",
    "       'Binarypolarity', 'BinaryW1 Score', 'BinaryBull_ratio',\n",
    "       'BinaryW Score With Bull Ratio', 'BinaryOpen', 'BinaryHigh',\n",
    "       'BinaryLow', 'BinaryClose', 'BinaryVolume',\n",
    "       'BinaryQuote Asset Volume', 'BinaryNumber of Trades',\n",
    "       'BinaryTB Base Volume', 'BinaryTB Quote Volume',\n",
    "       'Binary3MovingAverage', 'Binary5MovingAverage',\n",
    "       'BinaryJMJ_3HMoving_averages', 'BinaryJMJ_5HMoving_averages',\n",
    "       'Binary14 period RSI', 'BinaryMACD', 'BinarySIGNAL',\n",
    "       'Binary14 period STOCH %K', 'BinaryMFV', 'Binary14 period ATR',\n",
    "       'BinaryMOM', 'Binary14 period MFI', 'BinaryROC', 'BinaryOBV',\n",
    "       'Binary20 period CCI', 'Binary14 period EMV', 'BinaryVIm',\n",
    "       'BinaryVIp', 'Binaryema50', 'Binaryema21', 'Binaryema15',\n",
    "       'Binaryema5']\n",
    "\n",
    "\n",
    "    X_test = PredictDF[feature_names_JMJ]\n",
    "    y_test = PredictDF['Actual_Label']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#     predict_y1= np.argmax(stacking_classifier.predict_proba(X_test_scaled),axis=0)\n",
    "    #predict_y=  Model2.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "    PredictDF['Prediction']= Model2.predict_proba(X_test_scaled)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d875d627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(cv=5,\n",
       "                   estimators=[(&#x27;lr&#x27;,\n",
       "                                LogisticRegression(C=0.01, max_iter=1000)),\n",
       "                               (&#x27;abc&#x27;,\n",
       "                                AdaBoostClassifier(learning_rate=0.001,\n",
       "                                                   n_estimators=20)),\n",
       "                               (&#x27;lda&#x27;, LinearDiscriminantAnalysis()),\n",
       "                               (&#x27;GBC&#x27;,\n",
       "                                GradientBoostingClassifier(learning_rate=0.01))],\n",
       "                   final_estimator=LogisticRegression(C=0.01, max_iter=1000),\n",
       "                   passthrough=True, stack_method=&#x27;predict_proba&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(cv=5,\n",
       "                   estimators=[(&#x27;lr&#x27;,\n",
       "                                LogisticRegression(C=0.01, max_iter=1000)),\n",
       "                               (&#x27;abc&#x27;,\n",
       "                                AdaBoostClassifier(learning_rate=0.001,\n",
       "                                                   n_estimators=20)),\n",
       "                               (&#x27;lda&#x27;, LinearDiscriminantAnalysis()),\n",
       "                               (&#x27;GBC&#x27;,\n",
       "                                GradientBoostingClassifier(learning_rate=0.01))],\n",
       "                   final_estimator=LogisticRegression(C=0.01, max_iter=1000),\n",
       "                   passthrough=True, stack_method=&#x27;predict_proba&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01, max_iter=1000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>abc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(learning_rate=0.001, n_estimators=20)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lda</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>GBC</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01, max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('lr',\n",
       "                                LogisticRegression(C=0.01, max_iter=1000)),\n",
       "                               ('abc',\n",
       "                                AdaBoostClassifier(learning_rate=0.001,\n",
       "                                                   n_estimators=20)),\n",
       "                               ('lda', LinearDiscriminantAnalysis()),\n",
       "                               ('GBC',\n",
       "                                GradientBoostingClassifier(learning_rate=0.01))],\n",
       "                   final_estimator=LogisticRegression(C=0.01, max_iter=1000),\n",
       "                   passthrough=True, stack_method='predict_proba')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e22440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>number_of_followers</th>\n",
       "      <th>following</th>\n",
       "      <th>followers_following_ratio</th>\n",
       "      <th>2x_retweets_+_favorites</th>\n",
       "      <th>polarity</th>\n",
       "      <th>W1 Score</th>\n",
       "      <th>Bull_ratio</th>\n",
       "      <th>W Score With Bull Ratio</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Quote Asset Volume</th>\n",
       "      <th>Number of Trades</th>\n",
       "      <th>TB Base Volume</th>\n",
       "      <th>TB Quote Volume</th>\n",
       "      <th>3MovingAverage</th>\n",
       "      <th>5MovingAverage</th>\n",
       "      <th>JMJ_3HMoving_averages</th>\n",
       "      <th>JMJ_5HMoving_averages</th>\n",
       "      <th>Actual_Label</th>\n",
       "      <th>Bitcoin_Google_Trend_Score</th>\n",
       "      <th>BTC_Google_Trend_Score</th>\n",
       "      <th>Mkt Sentiment</th>\n",
       "      <th>Crypto Sentiment</th>\n",
       "      <th>Historically Optimal SMA(s-t)</th>\n",
       "      <th>Historically Optimal SMA(l-t)</th>\n",
       "      <th>Historically Optimal WMA(s-t)</th>\n",
       "      <th>Historically Optimal WMA(l-t)</th>\n",
       "      <th>Historically Optimal EMA(s-t)</th>\n",
       "      <th>Historically Optimal EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Favorites SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Favorites SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Favorites WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Favorites WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Favorites EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Favorites EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Retweets SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Retweets SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Retweets WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Retweets WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Retweets EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Retweets EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Follower Exposure SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Follower Exposure SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Follower Exposure WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Follower Exposure WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Follower Exposure EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Follower Exposure EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Following Exposure SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Following Exposure SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Following Exposure WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Following Exposure WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Following Exposure EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Following Exposure EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Follower to Following Ratio SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Follower to Following Ratio SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Follower to Following Ratio WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Follower to Following Ratio WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Follower to Following Ratio EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Follower to Following Ratio EMA(l-t)</th>\n",
       "      <th>Twitter Hourly 2x Retweets + Favorites SMA(s-t)</th>\n",
       "      <th>Twitter Hourly 2x Retweets + Favorites SMA(l-t)</th>\n",
       "      <th>Twitter Hourly 2x Retweets + Favorites WMA(s-t)</th>\n",
       "      <th>Twitter Hourly 2x Retweets + Favorites WMA(l-t)</th>\n",
       "      <th>Twitter Hourly 2x Retweets + Favorites EMA(s-t)</th>\n",
       "      <th>Twitter Hourly 2x Retweets + Favorites EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Polarity Score SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Polarity Score SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Polarity Score WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Polarity Score WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Polarity Score EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Polarity Score EMA(l-t)</th>\n",
       "      <th>Twitter W1 Score SMA(s-t)</th>\n",
       "      <th>Twitter W1 Score SMA(l-t)</th>\n",
       "      <th>Twitter W1 Score WMA(s-t)</th>\n",
       "      <th>Twitter W1 Score WMA(l-t)</th>\n",
       "      <th>Twitter W1 Score EMA(s-t)</th>\n",
       "      <th>Twitter W1 Score EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Bull Ratio SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Bull Ratio SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Bull Ratio WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Bull Ratio WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Bull Ratio EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Bull Ratio EMA(l-t)</th>\n",
       "      <th>Twitter Hourly Weighted Bull Ratio SMA(s-t)</th>\n",
       "      <th>Twitter Hourly Weighted Bull Ratio SMA(l-t)</th>\n",
       "      <th>Twitter Hourly Weighted Bull Ratio WMA(s-t)</th>\n",
       "      <th>Twitter Hourly Weighted Bull Ratio WMA(l-t)</th>\n",
       "      <th>Twitter Hourly Weighted Bull Ratio EMA(s-t)</th>\n",
       "      <th>Twitter Hourly Weighted Bull Ratio EMA(l-t)</th>\n",
       "      <th>Quote Asset Volume SMA(s-t)</th>\n",
       "      <th>Quote Asset Volume SMA(l-t)</th>\n",
       "      <th>Quote Asset Volume WMA(s-t)</th>\n",
       "      <th>Quote Asset Volume WMA(l-t)</th>\n",
       "      <th>Quote Asset Volume EMA(s-t)</th>\n",
       "      <th>Quote Asset Volume EMA(l-t)</th>\n",
       "      <th># of Hourly Trades SMA(s-t)</th>\n",
       "      <th># of Hourly Trades SMA(l-t)</th>\n",
       "      <th># of Hourly Trades WMA(s-t)</th>\n",
       "      <th># of Hourly Trades WMA(l-t)</th>\n",
       "      <th># of Hourly Trades EMA(s-t)</th>\n",
       "      <th># of Hourly Trades EMA(l-t)</th>\n",
       "      <th>TB Base Volume SMA(s-t)</th>\n",
       "      <th>TB Base Volume SMA(l-t)</th>\n",
       "      <th>TB Base Volume WMA(s-t)</th>\n",
       "      <th>TB Base Volume WMA(l-t)</th>\n",
       "      <th>TB Base Volume EMA(s-t)</th>\n",
       "      <th>TB Base Volume EMA(l-t)</th>\n",
       "      <th>TB Quote Volume SMA(s-t)</th>\n",
       "      <th>TB Quote Volume SMA(l-t)</th>\n",
       "      <th>TB Quote Volume WMA(s-t)</th>\n",
       "      <th>TB Quote Volume WMA(l-t)</th>\n",
       "      <th>TB Quote Volume EMA(s-t)</th>\n",
       "      <th>TB Quote Volume EMA(l-t)</th>\n",
       "      <th>14 period RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>SIGNAL</th>\n",
       "      <th>14 period STOCH %K</th>\n",
       "      <th>MFV</th>\n",
       "      <th>14 period ATR</th>\n",
       "      <th>MOM</th>\n",
       "      <th>14 period MFI</th>\n",
       "      <th>ROC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>20 period CCI</th>\n",
       "      <th>14 period EMV</th>\n",
       "      <th>VIm</th>\n",
       "      <th>VIp</th>\n",
       "      <th>ema50</th>\n",
       "      <th>ema21</th>\n",
       "      <th>ema15</th>\n",
       "      <th>ema5</th>\n",
       "      <th>normVol</th>\n",
       "      <th>Binaryfavorites</th>\n",
       "      <th>Binaryretweets</th>\n",
       "      <th>Binarynumber_of_followers</th>\n",
       "      <th>Binaryfollowing</th>\n",
       "      <th>Binaryfollowers_following_ratio</th>\n",
       "      <th>Binary2x_retweets_+_favorites</th>\n",
       "      <th>Binarypolarity</th>\n",
       "      <th>BinaryW1 Score</th>\n",
       "      <th>BinaryBull_ratio</th>\n",
       "      <th>BinaryW Score With Bull Ratio</th>\n",
       "      <th>BinaryOpen</th>\n",
       "      <th>BinaryHigh</th>\n",
       "      <th>BinaryLow</th>\n",
       "      <th>BinaryClose</th>\n",
       "      <th>BinaryVolume</th>\n",
       "      <th>BinaryQuote Asset Volume</th>\n",
       "      <th>BinaryNumber of Trades</th>\n",
       "      <th>BinaryTB Base Volume</th>\n",
       "      <th>BinaryTB Quote Volume</th>\n",
       "      <th>Binary3MovingAverage</th>\n",
       "      <th>Binary5MovingAverage</th>\n",
       "      <th>BinaryJMJ_3HMoving_averages</th>\n",
       "      <th>BinaryJMJ_5HMoving_averages</th>\n",
       "      <th>Binary14 period RSI</th>\n",
       "      <th>BinaryMACD</th>\n",
       "      <th>BinarySIGNAL</th>\n",
       "      <th>Binary14 period STOCH %K</th>\n",
       "      <th>BinaryMFV</th>\n",
       "      <th>Binary14 period ATR</th>\n",
       "      <th>BinaryMOM</th>\n",
       "      <th>Binary14 period MFI</th>\n",
       "      <th>BinaryROC</th>\n",
       "      <th>BinaryOBV</th>\n",
       "      <th>Binary20 period CCI</th>\n",
       "      <th>Binary14 period EMV</th>\n",
       "      <th>BinaryVIm</th>\n",
       "      <th>BinaryVIp</th>\n",
       "      <th>Binaryema50</th>\n",
       "      <th>Binaryema21</th>\n",
       "      <th>Binaryema15</th>\n",
       "      <th>Binaryema5</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.797619</td>\n",
       "      <td>0.745238</td>\n",
       "      <td>16514.150000</td>\n",
       "      <td>1498.323810</td>\n",
       "      <td>63.299076</td>\n",
       "      <td>11.288095</td>\n",
       "      <td>0.113520</td>\n",
       "      <td>0.128081</td>\n",
       "      <td>4.256410</td>\n",
       "      <td>0.545164</td>\n",
       "      <td>46475.60</td>\n",
       "      <td>46645.34</td>\n",
       "      <td>46360.00</td>\n",
       "      <td>46360.01</td>\n",
       "      <td>661.70660</td>\n",
       "      <td>3.078130e+07</td>\n",
       "      <td>27929</td>\n",
       "      <td>337.98922</td>\n",
       "      <td>1.572096e+07</td>\n",
       "      <td>46510.740000</td>\n",
       "      <td>46620.040</td>\n",
       "      <td>46500.609170</td>\n",
       "      <td>46524.334603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.086364</td>\n",
       "      <td>48196.449449</td>\n",
       "      <td>48078.186363</td>\n",
       "      <td>48035.797421</td>\n",
       "      <td>49380.744177</td>\n",
       "      <td>49264.803864</td>\n",
       "      <td>51365.355477</td>\n",
       "      <td>11.500328</td>\n",
       "      <td>11.161985</td>\n",
       "      <td>11.147481</td>\n",
       "      <td>12.036069</td>\n",
       "      <td>11.633252</td>\n",
       "      <td>11.401072</td>\n",
       "      <td>1.545594</td>\n",
       "      <td>1.562554</td>\n",
       "      <td>1.542539</td>\n",
       "      <td>1.644936</td>\n",
       "      <td>1.571697</td>\n",
       "      <td>1.511503</td>\n",
       "      <td>24897.966177</td>\n",
       "      <td>20753.002445</td>\n",
       "      <td>22571.536004</td>\n",
       "      <td>20141.160233</td>\n",
       "      <td>21554.559441</td>\n",
       "      <td>19541.657515</td>\n",
       "      <td>1606.423217</td>\n",
       "      <td>1904.300039</td>\n",
       "      <td>1792.862905</td>\n",
       "      <td>1893.415089</td>\n",
       "      <td>1843.454381</td>\n",
       "      <td>1971.350293</td>\n",
       "      <td>563.035330</td>\n",
       "      <td>446.938354</td>\n",
       "      <td>508.380162</td>\n",
       "      <td>396.751881</td>\n",
       "      <td>451.446392</td>\n",
       "      <td>381.105084</td>\n",
       "      <td>14.591515</td>\n",
       "      <td>14.287093</td>\n",
       "      <td>14.232559</td>\n",
       "      <td>15.325942</td>\n",
       "      <td>14.776646</td>\n",
       "      <td>14.424077</td>\n",
       "      <td>0.112792</td>\n",
       "      <td>0.099203</td>\n",
       "      <td>0.104325</td>\n",
       "      <td>0.092270</td>\n",
       "      <td>0.098422</td>\n",
       "      <td>0.092775</td>\n",
       "      <td>0.295329</td>\n",
       "      <td>0.244635</td>\n",
       "      <td>0.261049</td>\n",
       "      <td>0.220459</td>\n",
       "      <td>0.236897</td>\n",
       "      <td>0.213518</td>\n",
       "      <td>4.649767</td>\n",
       "      <td>4.045326</td>\n",
       "      <td>4.227080</td>\n",
       "      <td>3.824881</td>\n",
       "      <td>4.004069</td>\n",
       "      <td>3.789511</td>\n",
       "      <td>1.493411</td>\n",
       "      <td>1.112052</td>\n",
       "      <td>1.227893</td>\n",
       "      <td>0.949826</td>\n",
       "      <td>1.065037</td>\n",
       "      <td>0.895558</td>\n",
       "      <td>6.348281e+07</td>\n",
       "      <td>4.952478e+07</td>\n",
       "      <td>5.385039e+07</td>\n",
       "      <td>4.692252e+07</td>\n",
       "      <td>5.088904e+07</td>\n",
       "      <td>4.823619e+07</td>\n",
       "      <td>42578.818182</td>\n",
       "      <td>34703.190476</td>\n",
       "      <td>37351.990769</td>\n",
       "      <td>34250.219592</td>\n",
       "      <td>36203.694194</td>\n",
       "      <td>34712.753446</td>\n",
       "      <td>667.980733</td>\n",
       "      <td>523.108937</td>\n",
       "      <td>568.490056</td>\n",
       "      <td>496.389445</td>\n",
       "      <td>539.005303</td>\n",
       "      <td>519.356741</td>\n",
       "      <td>3.129876e+07</td>\n",
       "      <td>2.453851e+07</td>\n",
       "      <td>2.663726e+07</td>\n",
       "      <td>2.332155e+07</td>\n",
       "      <td>2.528961e+07</td>\n",
       "      <td>2.443322e+07</td>\n",
       "      <td>19.700818</td>\n",
       "      <td>-84.118036</td>\n",
       "      <td>-36.699431</td>\n",
       "      <td>3.087861</td>\n",
       "      <td>-3952.008488</td>\n",
       "      <td>346.417756</td>\n",
       "      <td>-574.943189</td>\n",
       "      <td>29.295680</td>\n",
       "      <td>-1.298165</td>\n",
       "      <td>-10828.228907</td>\n",
       "      <td>-146.795323</td>\n",
       "      <td>-8.651462e+06</td>\n",
       "      <td>1.107497</td>\n",
       "      <td>0.923348</td>\n",
       "      <td>0.989720</td>\n",
       "      <td>0.990214</td>\n",
       "      <td>0.990647</td>\n",
       "      <td>0.992877</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.633072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>39.326190</td>\n",
       "      <td>3.390476</td>\n",
       "      <td>16099.623810</td>\n",
       "      <td>1672.185714</td>\n",
       "      <td>110.504489</td>\n",
       "      <td>46.107143</td>\n",
       "      <td>0.087327</td>\n",
       "      <td>0.253405</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.633513</td>\n",
       "      <td>46360.01</td>\n",
       "      <td>46413.47</td>\n",
       "      <td>45696.00</td>\n",
       "      <td>45922.00</td>\n",
       "      <td>3127.50381</td>\n",
       "      <td>1.437875e+08</td>\n",
       "      <td>75059</td>\n",
       "      <td>1450.68943</td>\n",
       "      <td>6.667771e+07</td>\n",
       "      <td>46469.963333</td>\n",
       "      <td>46517.136</td>\n",
       "      <td>46217.432522</td>\n",
       "      <td>46414.854289</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92</td>\n",
       "      <td>27</td>\n",
       "      <td>0.100108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48178.173233</td>\n",
       "      <td>48076.911675</td>\n",
       "      <td>48028.907428</td>\n",
       "      <td>49371.226555</td>\n",
       "      <td>49255.414588</td>\n",
       "      <td>51357.505310</td>\n",
       "      <td>14.512476</td>\n",
       "      <td>12.794159</td>\n",
       "      <td>13.346864</td>\n",
       "      <td>13.108289</td>\n",
       "      <td>13.479448</td>\n",
       "      <td>12.364007</td>\n",
       "      <td>1.795136</td>\n",
       "      <td>1.686271</td>\n",
       "      <td>1.690355</td>\n",
       "      <td>1.714186</td>\n",
       "      <td>1.692949</td>\n",
       "      <td>1.576295</td>\n",
       "      <td>24675.783182</td>\n",
       "      <td>21099.259845</td>\n",
       "      <td>22313.158515</td>\n",
       "      <td>20042.453125</td>\n",
       "      <td>21190.897066</td>\n",
       "      <td>19422.966698</td>\n",
       "      <td>1636.866995</td>\n",
       "      <td>1895.659837</td>\n",
       "      <td>1774.912759</td>\n",
       "      <td>1881.602283</td>\n",
       "      <td>1832.036470</td>\n",
       "      <td>1961.034273</td>\n",
       "      <td>567.066258</td>\n",
       "      <td>445.774898</td>\n",
       "      <td>486.235689</td>\n",
       "      <td>386.793989</td>\n",
       "      <td>428.716932</td>\n",
       "      <td>371.774029</td>\n",
       "      <td>18.102747</td>\n",
       "      <td>16.166701</td>\n",
       "      <td>16.727574</td>\n",
       "      <td>16.536661</td>\n",
       "      <td>16.865346</td>\n",
       "      <td>15.516596</td>\n",
       "      <td>0.110745</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.103747</td>\n",
       "      <td>0.092356</td>\n",
       "      <td>0.097683</td>\n",
       "      <td>0.092588</td>\n",
       "      <td>0.292772</td>\n",
       "      <td>0.249328</td>\n",
       "      <td>0.262863</td>\n",
       "      <td>0.222651</td>\n",
       "      <td>0.237998</td>\n",
       "      <td>0.214893</td>\n",
       "      <td>4.328450</td>\n",
       "      <td>4.052087</td>\n",
       "      <td>4.121644</td>\n",
       "      <td>3.781862</td>\n",
       "      <td>3.903797</td>\n",
       "      <td>3.745045</td>\n",
       "      <td>1.396555</td>\n",
       "      <td>1.124832</td>\n",
       "      <td>1.199559</td>\n",
       "      <td>0.943834</td>\n",
       "      <td>1.036269</td>\n",
       "      <td>0.886522</td>\n",
       "      <td>7.234468e+07</td>\n",
       "      <td>5.468189e+07</td>\n",
       "      <td>6.133098e+07</td>\n",
       "      <td>5.100832e+07</td>\n",
       "      <td>5.708226e+07</td>\n",
       "      <td>5.153106e+07</td>\n",
       "      <td>46326.636364</td>\n",
       "      <td>37134.904762</td>\n",
       "      <td>40559.572308</td>\n",
       "      <td>35978.038367</td>\n",
       "      <td>38794.047915</td>\n",
       "      <td>36104.003327</td>\n",
       "      <td>755.511302</td>\n",
       "      <td>574.313600</td>\n",
       "      <td>642.257435</td>\n",
       "      <td>536.674026</td>\n",
       "      <td>599.784245</td>\n",
       "      <td>551.471662</td>\n",
       "      <td>3.527950e+07</td>\n",
       "      <td>2.686729e+07</td>\n",
       "      <td>2.999135e+07</td>\n",
       "      <td>2.515305e+07</td>\n",
       "      <td>2.804882e+07</td>\n",
       "      <td>2.588993e+07</td>\n",
       "      <td>14.284485</td>\n",
       "      <td>-115.697415</td>\n",
       "      <td>-52.956627</td>\n",
       "      <td>10.188234</td>\n",
       "      <td>-4985.186302</td>\n",
       "      <td>367.222254</td>\n",
       "      <td>-984.008583</td>\n",
       "      <td>26.765012</td>\n",
       "      <td>-1.915452</td>\n",
       "      <td>-13137.608289</td>\n",
       "      <td>-190.897291</td>\n",
       "      <td>-1.437937e+07</td>\n",
       "      <td>1.120331</td>\n",
       "      <td>0.823015</td>\n",
       "      <td>0.984131</td>\n",
       "      <td>0.984821</td>\n",
       "      <td>0.985425</td>\n",
       "      <td>0.988481</td>\n",
       "      <td>1.540271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.642624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.369048</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11057.442857</td>\n",
       "      <td>1828.021429</td>\n",
       "      <td>133.780075</td>\n",
       "      <td>6.469048</td>\n",
       "      <td>0.093584</td>\n",
       "      <td>0.222649</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>0.545491</td>\n",
       "      <td>45922.01</td>\n",
       "      <td>46260.44</td>\n",
       "      <td>45811.87</td>\n",
       "      <td>45979.01</td>\n",
       "      <td>1055.66747</td>\n",
       "      <td>4.859122e+07</td>\n",
       "      <td>36692</td>\n",
       "      <td>478.35709</td>\n",
       "      <td>2.202104e+07</td>\n",
       "      <td>46251.580000</td>\n",
       "      <td>46362.846</td>\n",
       "      <td>46078.530953</td>\n",
       "      <td>46303.928703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48159.802603</td>\n",
       "      <td>48075.600968</td>\n",
       "      <td>48021.949062</td>\n",
       "      <td>49361.694240</td>\n",
       "      <td>49245.984234</td>\n",
       "      <td>51349.632204</td>\n",
       "      <td>14.072162</td>\n",
       "      <td>12.618942</td>\n",
       "      <td>12.844038</td>\n",
       "      <td>12.795033</td>\n",
       "      <td>12.938754</td>\n",
       "      <td>12.122801</td>\n",
       "      <td>1.722408</td>\n",
       "      <td>1.632447</td>\n",
       "      <td>1.613099</td>\n",
       "      <td>1.667652</td>\n",
       "      <td>1.616752</td>\n",
       "      <td>1.540905</td>\n",
       "      <td>23801.288214</td>\n",
       "      <td>21023.709688</td>\n",
       "      <td>21653.515571</td>\n",
       "      <td>19742.111264</td>\n",
       "      <td>20515.333452</td>\n",
       "      <td>19134.500358</td>\n",
       "      <td>1639.810761</td>\n",
       "      <td>1830.511606</td>\n",
       "      <td>1769.942474</td>\n",
       "      <td>1876.038300</td>\n",
       "      <td>1831.768801</td>\n",
       "      <td>1956.447623</td>\n",
       "      <td>542.151802</td>\n",
       "      <td>447.568914</td>\n",
       "      <td>466.201592</td>\n",
       "      <td>378.100704</td>\n",
       "      <td>409.054475</td>\n",
       "      <td>363.567341</td>\n",
       "      <td>17.516979</td>\n",
       "      <td>15.883836</td>\n",
       "      <td>16.070237</td>\n",
       "      <td>16.130337</td>\n",
       "      <td>16.172259</td>\n",
       "      <td>15.204612</td>\n",
       "      <td>0.111309</td>\n",
       "      <td>0.100722</td>\n",
       "      <td>0.103592</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.097409</td>\n",
       "      <td>0.092622</td>\n",
       "      <td>0.298386</td>\n",
       "      <td>0.254827</td>\n",
       "      <td>0.261851</td>\n",
       "      <td>0.223563</td>\n",
       "      <td>0.236975</td>\n",
       "      <td>0.215161</td>\n",
       "      <td>4.170496</td>\n",
       "      <td>4.021616</td>\n",
       "      <td>4.014492</td>\n",
       "      <td>3.737251</td>\n",
       "      <td>3.806878</td>\n",
       "      <td>3.700389</td>\n",
       "      <td>1.384898</td>\n",
       "      <td>1.135039</td>\n",
       "      <td>1.163523</td>\n",
       "      <td>0.934275</td>\n",
       "      <td>1.003551</td>\n",
       "      <td>0.874762</td>\n",
       "      <td>7.188079e+07</td>\n",
       "      <td>5.534665e+07</td>\n",
       "      <td>6.118190e+07</td>\n",
       "      <td>5.120104e+07</td>\n",
       "      <td>5.651619e+07</td>\n",
       "      <td>5.142968e+07</td>\n",
       "      <td>46372.181818</td>\n",
       "      <td>37499.666667</td>\n",
       "      <td>40681.150769</td>\n",
       "      <td>36138.395918</td>\n",
       "      <td>38653.911387</td>\n",
       "      <td>36124.279074</td>\n",
       "      <td>749.047102</td>\n",
       "      <td>578.940397</td>\n",
       "      <td>638.110000</td>\n",
       "      <td>537.245217</td>\n",
       "      <td>591.689101</td>\n",
       "      <td>548.950470</td>\n",
       "      <td>3.492695e+07</td>\n",
       "      <td>2.705957e+07</td>\n",
       "      <td>2.976831e+07</td>\n",
       "      <td>2.516110e+07</td>\n",
       "      <td>2.764697e+07</td>\n",
       "      <td>2.575652e+07</td>\n",
       "      <td>13.359474</td>\n",
       "      <td>-142.962820</td>\n",
       "      <td>-71.372555</td>\n",
       "      <td>10.015561</td>\n",
       "      <td>-5483.536566</td>\n",
       "      <td>384.879628</td>\n",
       "      <td>-1253.872127</td>\n",
       "      <td>22.248523</td>\n",
       "      <td>-2.061429</td>\n",
       "      <td>-14632.074914</td>\n",
       "      <td>-174.411608</td>\n",
       "      <td>-1.685552e+07</td>\n",
       "      <td>1.138785</td>\n",
       "      <td>0.815338</td>\n",
       "      <td>0.983716</td>\n",
       "      <td>0.984601</td>\n",
       "      <td>0.985368</td>\n",
       "      <td>0.989145</td>\n",
       "      <td>0.997319</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.495238</td>\n",
       "      <td>1.890476</td>\n",
       "      <td>25029.171429</td>\n",
       "      <td>1882.980952</td>\n",
       "      <td>662.631886</td>\n",
       "      <td>18.276190</td>\n",
       "      <td>0.090903</td>\n",
       "      <td>0.173174</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.432936</td>\n",
       "      <td>45979.01</td>\n",
       "      <td>46439.97</td>\n",
       "      <td>45961.59</td>\n",
       "      <td>46217.90</td>\n",
       "      <td>1002.70049</td>\n",
       "      <td>4.632422e+07</td>\n",
       "      <td>39674</td>\n",
       "      <td>482.29605</td>\n",
       "      <td>2.228102e+07</td>\n",
       "      <td>46087.006667</td>\n",
       "      <td>46262.180</td>\n",
       "      <td>46020.765581</td>\n",
       "      <td>46167.213095</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48141.337561</td>\n",
       "      <td>48074.254240</td>\n",
       "      <td>48014.922321</td>\n",
       "      <td>49352.147234</td>\n",
       "      <td>49236.512801</td>\n",
       "      <td>51341.736161</td>\n",
       "      <td>14.368398</td>\n",
       "      <td>11.919132</td>\n",
       "      <td>13.068302</td>\n",
       "      <td>12.847442</td>\n",
       "      <td>13.042520</td>\n",
       "      <td>12.204610</td>\n",
       "      <td>1.660173</td>\n",
       "      <td>1.605507</td>\n",
       "      <td>1.641120</td>\n",
       "      <td>1.675060</td>\n",
       "      <td>1.635001</td>\n",
       "      <td>1.552959</td>\n",
       "      <td>23579.362554</td>\n",
       "      <td>21350.167928</td>\n",
       "      <td>22093.562472</td>\n",
       "      <td>19998.332506</td>\n",
       "      <td>20816.255983</td>\n",
       "      <td>19337.764878</td>\n",
       "      <td>1692.227489</td>\n",
       "      <td>1828.334926</td>\n",
       "      <td>1768.932371</td>\n",
       "      <td>1872.605753</td>\n",
       "      <td>1835.182944</td>\n",
       "      <td>1953.914290</td>\n",
       "      <td>494.021490</td>\n",
       "      <td>474.011983</td>\n",
       "      <td>486.977631</td>\n",
       "      <td>390.510159</td>\n",
       "      <td>425.959636</td>\n",
       "      <td>373.879911</td>\n",
       "      <td>17.688745</td>\n",
       "      <td>15.130146</td>\n",
       "      <td>16.350543</td>\n",
       "      <td>16.197561</td>\n",
       "      <td>16.312521</td>\n",
       "      <td>15.310528</td>\n",
       "      <td>0.110456</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.103144</td>\n",
       "      <td>0.092867</td>\n",
       "      <td>0.096976</td>\n",
       "      <td>0.092563</td>\n",
       "      <td>0.288082</td>\n",
       "      <td>0.249972</td>\n",
       "      <td>0.256985</td>\n",
       "      <td>0.222452</td>\n",
       "      <td>0.232721</td>\n",
       "      <td>0.213713</td>\n",
       "      <td>3.936230</td>\n",
       "      <td>3.961828</td>\n",
       "      <td>3.911249</td>\n",
       "      <td>3.694877</td>\n",
       "      <td>3.719752</td>\n",
       "      <td>3.658996</td>\n",
       "      <td>1.292016</td>\n",
       "      <td>1.106452</td>\n",
       "      <td>1.118726</td>\n",
       "      <td>0.920144</td>\n",
       "      <td>0.965510</td>\n",
       "      <td>0.859527</td>\n",
       "      <td>6.984983e+07</td>\n",
       "      <td>5.614564e+07</td>\n",
       "      <td>6.079130e+07</td>\n",
       "      <td>5.129117e+07</td>\n",
       "      <td>5.583673e+07</td>\n",
       "      <td>5.125363e+07</td>\n",
       "      <td>46437.636364</td>\n",
       "      <td>38235.047619</td>\n",
       "      <td>40997.175385</td>\n",
       "      <td>36409.913469</td>\n",
       "      <td>38721.917294</td>\n",
       "      <td>36246.683244</td>\n",
       "      <td>723.869733</td>\n",
       "      <td>586.969392</td>\n",
       "      <td>633.559943</td>\n",
       "      <td>537.925198</td>\n",
       "      <td>584.396231</td>\n",
       "      <td>546.652041</td>\n",
       "      <td>3.368766e+07</td>\n",
       "      <td>2.741787e+07</td>\n",
       "      <td>2.953348e+07</td>\n",
       "      <td>2.517777e+07</td>\n",
       "      <td>2.728924e+07</td>\n",
       "      <td>2.563667e+07</td>\n",
       "      <td>22.692087</td>\n",
       "      <td>-153.944946</td>\n",
       "      <td>-88.189989</td>\n",
       "      <td>17.890161</td>\n",
       "      <td>-5569.254985</td>\n",
       "      <td>403.371082</td>\n",
       "      <td>-1073.562226</td>\n",
       "      <td>24.147574</td>\n",
       "      <td>-1.974307</td>\n",
       "      <td>-13457.256279</td>\n",
       "      <td>-132.691139</td>\n",
       "      <td>-1.584262e+07</td>\n",
       "      <td>1.126529</td>\n",
       "      <td>0.827629</td>\n",
       "      <td>0.987350</td>\n",
       "      <td>0.988365</td>\n",
       "      <td>0.989234</td>\n",
       "      <td>0.993311</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.329545</td>\n",
       "      <td>1.281818</td>\n",
       "      <td>22107.427273</td>\n",
       "      <td>2018.227273</td>\n",
       "      <td>479.791704</td>\n",
       "      <td>15.893182</td>\n",
       "      <td>0.095774</td>\n",
       "      <td>0.263250</td>\n",
       "      <td>3.395833</td>\n",
       "      <td>0.893952</td>\n",
       "      <td>46217.91</td>\n",
       "      <td>46576.94</td>\n",
       "      <td>46170.64</td>\n",
       "      <td>46446.10</td>\n",
       "      <td>1055.33575</td>\n",
       "      <td>4.896831e+07</td>\n",
       "      <td>29425</td>\n",
       "      <td>503.52171</td>\n",
       "      <td>2.336430e+07</td>\n",
       "      <td>46039.636667</td>\n",
       "      <td>46190.330</td>\n",
       "      <td>46257.255102</td>\n",
       "      <td>46206.003903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48122.778106</td>\n",
       "      <td>48072.871493</td>\n",
       "      <td>48007.827207</td>\n",
       "      <td>49342.585535</td>\n",
       "      <td>49227.000289</td>\n",
       "      <td>51333.817181</td>\n",
       "      <td>14.456799</td>\n",
       "      <td>11.881377</td>\n",
       "      <td>13.173975</td>\n",
       "      <td>12.852665</td>\n",
       "      <td>13.061655</td>\n",
       "      <td>12.243400</td>\n",
       "      <td>1.584061</td>\n",
       "      <td>1.604960</td>\n",
       "      <td>1.618556</td>\n",
       "      <td>1.657281</td>\n",
       "      <td>1.611455</td>\n",
       "      <td>1.543610</td>\n",
       "      <td>24459.936029</td>\n",
       "      <td>21505.479817</td>\n",
       "      <td>22261.591283</td>\n",
       "      <td>20125.761693</td>\n",
       "      <td>20902.334069</td>\n",
       "      <td>19433.270478</td>\n",
       "      <td>1724.331267</td>\n",
       "      <td>1793.846133</td>\n",
       "      <td>1778.152576</td>\n",
       "      <td>1874.662369</td>\n",
       "      <td>1847.385900</td>\n",
       "      <td>1956.131979</td>\n",
       "      <td>531.765129</td>\n",
       "      <td>491.145712</td>\n",
       "      <td>491.741615</td>\n",
       "      <td>395.430495</td>\n",
       "      <td>429.548440</td>\n",
       "      <td>377.532042</td>\n",
       "      <td>17.624921</td>\n",
       "      <td>15.091297</td>\n",
       "      <td>16.411088</td>\n",
       "      <td>16.167228</td>\n",
       "      <td>16.284565</td>\n",
       "      <td>15.330620</td>\n",
       "      <td>0.111724</td>\n",
       "      <td>0.101225</td>\n",
       "      <td>0.103072</td>\n",
       "      <td>0.093232</td>\n",
       "      <td>0.096896</td>\n",
       "      <td>0.092673</td>\n",
       "      <td>0.276411</td>\n",
       "      <td>0.259173</td>\n",
       "      <td>0.258778</td>\n",
       "      <td>0.224997</td>\n",
       "      <td>0.234757</td>\n",
       "      <td>0.215421</td>\n",
       "      <td>3.986356</td>\n",
       "      <td>4.004486</td>\n",
       "      <td>3.880251</td>\n",
       "      <td>3.688266</td>\n",
       "      <td>3.698158</td>\n",
       "      <td>3.649921</td>\n",
       "      <td>1.272013</td>\n",
       "      <td>1.140682</td>\n",
       "      <td>1.109001</td>\n",
       "      <td>0.924572</td>\n",
       "      <td>0.960739</td>\n",
       "      <td>0.860714</td>\n",
       "      <td>6.920458e+07</td>\n",
       "      <td>5.672918e+07</td>\n",
       "      <td>6.052418e+07</td>\n",
       "      <td>5.148056e+07</td>\n",
       "      <td>5.537884e+07</td>\n",
       "      <td>5.117483e+07</td>\n",
       "      <td>45091.909091</td>\n",
       "      <td>38280.333333</td>\n",
       "      <td>40469.544615</td>\n",
       "      <td>36260.440816</td>\n",
       "      <td>38102.122808</td>\n",
       "      <td>36011.452787</td>\n",
       "      <td>717.097553</td>\n",
       "      <td>593.600185</td>\n",
       "      <td>629.825058</td>\n",
       "      <td>539.442647</td>\n",
       "      <td>579.004596</td>\n",
       "      <td>545.164788</td>\n",
       "      <td>3.332196e+07</td>\n",
       "      <td>2.771586e+07</td>\n",
       "      <td>2.934479e+07</td>\n",
       "      <td>2.523771e+07</td>\n",
       "      <td>2.702758e+07</td>\n",
       "      <td>2.555831e+07</td>\n",
       "      <td>34.139724</td>\n",
       "      <td>-148.097173</td>\n",
       "      <td>-100.346622</td>\n",
       "      <td>29.689397</td>\n",
       "      <td>-5362.498848</td>\n",
       "      <td>413.301410</td>\n",
       "      <td>-835.553274</td>\n",
       "      <td>29.334145</td>\n",
       "      <td>-1.969773</td>\n",
       "      <td>-12360.101519</td>\n",
       "      <td>-91.899346</td>\n",
       "      <td>-1.099462e+07</td>\n",
       "      <td>1.103011</td>\n",
       "      <td>0.872962</td>\n",
       "      <td>0.992019</td>\n",
       "      <td>0.993086</td>\n",
       "      <td>0.993983</td>\n",
       "      <td>0.997910</td>\n",
       "      <td>0.793299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.476874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7263</th>\n",
       "      <td>10.030952</td>\n",
       "      <td>2.557143</td>\n",
       "      <td>12561.900000</td>\n",
       "      <td>906.071429</td>\n",
       "      <td>406.088057</td>\n",
       "      <td>15.145238</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.365355</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>1.120422</td>\n",
       "      <td>20336.22</td>\n",
       "      <td>20465.99</td>\n",
       "      <td>20291.74</td>\n",
       "      <td>20401.53</td>\n",
       "      <td>18928.44019</td>\n",
       "      <td>3.855259e+08</td>\n",
       "      <td>378888</td>\n",
       "      <td>9593.04149</td>\n",
       "      <td>1.953918e+08</td>\n",
       "      <td>20456.926667</td>\n",
       "      <td>20570.376</td>\n",
       "      <td>20362.555890</td>\n",
       "      <td>20503.444373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83</td>\n",
       "      <td>26</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20106.722911</td>\n",
       "      <td>19685.536024</td>\n",
       "      <td>19880.713661</td>\n",
       "      <td>19665.734538</td>\n",
       "      <td>19865.081402</td>\n",
       "      <td>20066.763819</td>\n",
       "      <td>8.747573</td>\n",
       "      <td>7.567017</td>\n",
       "      <td>8.064936</td>\n",
       "      <td>8.075007</td>\n",
       "      <td>8.243993</td>\n",
       "      <td>8.258043</td>\n",
       "      <td>1.893712</td>\n",
       "      <td>1.447170</td>\n",
       "      <td>1.672537</td>\n",
       "      <td>1.956401</td>\n",
       "      <td>1.865205</td>\n",
       "      <td>1.875349</td>\n",
       "      <td>12379.205002</td>\n",
       "      <td>10281.205779</td>\n",
       "      <td>11407.914296</td>\n",
       "      <td>9973.987460</td>\n",
       "      <td>11093.884884</td>\n",
       "      <td>10721.075925</td>\n",
       "      <td>777.943689</td>\n",
       "      <td>780.015474</td>\n",
       "      <td>792.954043</td>\n",
       "      <td>824.985792</td>\n",
       "      <td>812.365083</td>\n",
       "      <td>816.382542</td>\n",
       "      <td>352.799905</td>\n",
       "      <td>363.852343</td>\n",
       "      <td>364.596692</td>\n",
       "      <td>351.897675</td>\n",
       "      <td>354.254811</td>\n",
       "      <td>349.001714</td>\n",
       "      <td>12.534997</td>\n",
       "      <td>10.461357</td>\n",
       "      <td>11.410010</td>\n",
       "      <td>11.987809</td>\n",
       "      <td>11.974402</td>\n",
       "      <td>12.008742</td>\n",
       "      <td>0.095291</td>\n",
       "      <td>0.090120</td>\n",
       "      <td>0.092446</td>\n",
       "      <td>0.088132</td>\n",
       "      <td>0.090132</td>\n",
       "      <td>0.085589</td>\n",
       "      <td>0.205164</td>\n",
       "      <td>0.171433</td>\n",
       "      <td>0.183005</td>\n",
       "      <td>0.162588</td>\n",
       "      <td>0.174267</td>\n",
       "      <td>0.156642</td>\n",
       "      <td>4.320211</td>\n",
       "      <td>4.216202</td>\n",
       "      <td>4.288653</td>\n",
       "      <td>4.047875</td>\n",
       "      <td>4.102877</td>\n",
       "      <td>3.882089</td>\n",
       "      <td>0.857634</td>\n",
       "      <td>0.689712</td>\n",
       "      <td>0.746614</td>\n",
       "      <td>0.643463</td>\n",
       "      <td>0.692566</td>\n",
       "      <td>0.603763</td>\n",
       "      <td>3.200849e+08</td>\n",
       "      <td>2.454396e+08</td>\n",
       "      <td>2.847238e+08</td>\n",
       "      <td>2.338055e+08</td>\n",
       "      <td>2.681329e+08</td>\n",
       "      <td>2.396102e+08</td>\n",
       "      <td>333241.090909</td>\n",
       "      <td>264155.857143</td>\n",
       "      <td>299259.135385</td>\n",
       "      <td>258877.006531</td>\n",
       "      <td>287444.342881</td>\n",
       "      <td>266612.935595</td>\n",
       "      <td>7719.031358</td>\n",
       "      <td>5922.211025</td>\n",
       "      <td>6870.308437</td>\n",
       "      <td>5636.231206</td>\n",
       "      <td>6466.032626</td>\n",
       "      <td>5785.128385</td>\n",
       "      <td>1.586547e+08</td>\n",
       "      <td>1.217557e+08</td>\n",
       "      <td>1.411798e+08</td>\n",
       "      <td>1.161054e+08</td>\n",
       "      <td>1.330367e+08</td>\n",
       "      <td>1.191801e+08</td>\n",
       "      <td>34.477556</td>\n",
       "      <td>-48.895625</td>\n",
       "      <td>-35.908972</td>\n",
       "      <td>19.246396</td>\n",
       "      <td>147193.189036</td>\n",
       "      <td>145.765396</td>\n",
       "      <td>-149.102178</td>\n",
       "      <td>31.022411</td>\n",
       "      <td>-0.556469</td>\n",
       "      <td>-693870.204086</td>\n",
       "      <td>-153.341690</td>\n",
       "      <td>-1.051588e+05</td>\n",
       "      <td>1.055579</td>\n",
       "      <td>0.907324</td>\n",
       "      <td>0.992577</td>\n",
       "      <td>0.989140</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.993042</td>\n",
       "      <td>1.314560</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.619953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7264</th>\n",
       "      <td>9.507143</td>\n",
       "      <td>3.530952</td>\n",
       "      <td>10695.850000</td>\n",
       "      <td>805.514286</td>\n",
       "      <td>264.229345</td>\n",
       "      <td>16.569048</td>\n",
       "      <td>0.075146</td>\n",
       "      <td>0.250643</td>\n",
       "      <td>2.518519</td>\n",
       "      <td>0.631248</td>\n",
       "      <td>20402.60</td>\n",
       "      <td>20533.00</td>\n",
       "      <td>20328.68</td>\n",
       "      <td>20371.87</td>\n",
       "      <td>20988.91618</td>\n",
       "      <td>4.285581e+08</td>\n",
       "      <td>383396</td>\n",
       "      <td>10453.52667</td>\n",
       "      <td>2.134634e+08</td>\n",
       "      <td>20358.013333</td>\n",
       "      <td>20500.570</td>\n",
       "      <td>20383.221150</td>\n",
       "      <td>20435.715408</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78</td>\n",
       "      <td>27</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>20111.858824</td>\n",
       "      <td>19688.328352</td>\n",
       "      <td>19883.491346</td>\n",
       "      <td>19667.244832</td>\n",
       "      <td>19866.872238</td>\n",
       "      <td>20067.380487</td>\n",
       "      <td>8.990829</td>\n",
       "      <td>7.818371</td>\n",
       "      <td>8.208251</td>\n",
       "      <td>8.122332</td>\n",
       "      <td>8.328203</td>\n",
       "      <td>8.301116</td>\n",
       "      <td>2.104522</td>\n",
       "      <td>1.573244</td>\n",
       "      <td>1.811319</td>\n",
       "      <td>2.015084</td>\n",
       "      <td>1.976254</td>\n",
       "      <td>1.932439</td>\n",
       "      <td>12992.051826</td>\n",
       "      <td>10582.509762</td>\n",
       "      <td>11476.362045</td>\n",
       "      <td>9988.693386</td>\n",
       "      <td>11067.349225</td>\n",
       "      <td>10720.206066</td>\n",
       "      <td>769.638635</td>\n",
       "      <td>780.422305</td>\n",
       "      <td>790.760497</td>\n",
       "      <td>824.143656</td>\n",
       "      <td>811.908363</td>\n",
       "      <td>816.007774</td>\n",
       "      <td>357.414598</td>\n",
       "      <td>353.547375</td>\n",
       "      <td>358.771464</td>\n",
       "      <td>349.482088</td>\n",
       "      <td>348.253113</td>\n",
       "      <td>346.078529</td>\n",
       "      <td>13.199872</td>\n",
       "      <td>10.964858</td>\n",
       "      <td>11.830889</td>\n",
       "      <td>12.152500</td>\n",
       "      <td>12.280712</td>\n",
       "      <td>12.165994</td>\n",
       "      <td>0.093947</td>\n",
       "      <td>0.089850</td>\n",
       "      <td>0.091345</td>\n",
       "      <td>0.087751</td>\n",
       "      <td>0.089133</td>\n",
       "      <td>0.085229</td>\n",
       "      <td>0.206405</td>\n",
       "      <td>0.176066</td>\n",
       "      <td>0.189239</td>\n",
       "      <td>0.166651</td>\n",
       "      <td>0.179359</td>\n",
       "      <td>0.159883</td>\n",
       "      <td>4.072803</td>\n",
       "      <td>4.146789</td>\n",
       "      <td>4.157321</td>\n",
       "      <td>3.995939</td>\n",
       "      <td>3.997253</td>\n",
       "      <td>3.835069</td>\n",
       "      <td>0.802128</td>\n",
       "      <td>0.690735</td>\n",
       "      <td>0.742093</td>\n",
       "      <td>0.646061</td>\n",
       "      <td>0.688478</td>\n",
       "      <td>0.604711</td>\n",
       "      <td>3.460033e+08</td>\n",
       "      <td>2.610101e+08</td>\n",
       "      <td>2.997360e+08</td>\n",
       "      <td>2.425508e+08</td>\n",
       "      <td>2.788279e+08</td>\n",
       "      <td>2.461256e+08</td>\n",
       "      <td>351143.545455</td>\n",
       "      <td>276619.619048</td>\n",
       "      <td>309293.301538</td>\n",
       "      <td>264514.336327</td>\n",
       "      <td>293841.120022</td>\n",
       "      <td>270639.937816</td>\n",
       "      <td>8351.050474</td>\n",
       "      <td>6307.014359</td>\n",
       "      <td>7241.301132</td>\n",
       "      <td>5852.376963</td>\n",
       "      <td>6731.865562</td>\n",
       "      <td>5946.107636</td>\n",
       "      <td>1.715141e+08</td>\n",
       "      <td>1.295853e+08</td>\n",
       "      <td>1.486906e+08</td>\n",
       "      <td>1.204690e+08</td>\n",
       "      <td>1.383985e+08</td>\n",
       "      <td>1.224313e+08</td>\n",
       "      <td>33.701986</td>\n",
       "      <td>-58.239611</td>\n",
       "      <td>-40.375100</td>\n",
       "      <td>16.369139</td>\n",
       "      <td>139042.802561</td>\n",
       "      <td>151.782746</td>\n",
       "      <td>-146.260262</td>\n",
       "      <td>39.928764</td>\n",
       "      <td>-0.799279</td>\n",
       "      <td>-715297.237682</td>\n",
       "      <td>-131.783209</td>\n",
       "      <td>-5.653012e+04</td>\n",
       "      <td>1.021652</td>\n",
       "      <td>0.938704</td>\n",
       "      <td>0.992016</td>\n",
       "      <td>0.988944</td>\n",
       "      <td>0.990084</td>\n",
       "      <td>0.993595</td>\n",
       "      <td>1.212615</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7265</th>\n",
       "      <td>17.957143</td>\n",
       "      <td>4.402381</td>\n",
       "      <td>22596.990476</td>\n",
       "      <td>846.535714</td>\n",
       "      <td>217.348828</td>\n",
       "      <td>26.761905</td>\n",
       "      <td>0.104180</td>\n",
       "      <td>0.403896</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>1.187930</td>\n",
       "      <td>20370.92</td>\n",
       "      <td>20425.28</td>\n",
       "      <td>20345.06</td>\n",
       "      <td>20373.64</td>\n",
       "      <td>11827.33569</td>\n",
       "      <td>2.411572e+08</td>\n",
       "      <td>248163</td>\n",
       "      <td>5958.90684</td>\n",
       "      <td>1.215062e+08</td>\n",
       "      <td>20369.873333</td>\n",
       "      <td>20428.836</td>\n",
       "      <td>20383.890119</td>\n",
       "      <td>20366.695247</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>20116.974340</td>\n",
       "      <td>19691.127258</td>\n",
       "      <td>19886.264742</td>\n",
       "      <td>19668.757053</td>\n",
       "      <td>19868.656597</td>\n",
       "      <td>20067.995604</td>\n",
       "      <td>10.259021</td>\n",
       "      <td>8.221787</td>\n",
       "      <td>9.000313</td>\n",
       "      <td>8.506688</td>\n",
       "      <td>8.970132</td>\n",
       "      <td>8.634082</td>\n",
       "      <td>2.377636</td>\n",
       "      <td>1.718083</td>\n",
       "      <td>2.012489</td>\n",
       "      <td>2.107106</td>\n",
       "      <td>2.137996</td>\n",
       "      <td>2.017609</td>\n",
       "      <td>14665.287281</td>\n",
       "      <td>11165.740962</td>\n",
       "      <td>12451.240578</td>\n",
       "      <td>10476.812475</td>\n",
       "      <td>11835.991975</td>\n",
       "      <td>11129.750356</td>\n",
       "      <td>775.342010</td>\n",
       "      <td>781.996129</td>\n",
       "      <td>792.184944</td>\n",
       "      <td>824.888651</td>\n",
       "      <td>814.216853</td>\n",
       "      <td>817.060462</td>\n",
       "      <td>363.593749</td>\n",
       "      <td>355.191916</td>\n",
       "      <td>349.272804</td>\n",
       "      <td>345.166151</td>\n",
       "      <td>339.526161</td>\n",
       "      <td>341.639573</td>\n",
       "      <td>15.014293</td>\n",
       "      <td>11.657954</td>\n",
       "      <td>13.025291</td>\n",
       "      <td>12.720899</td>\n",
       "      <td>13.246124</td>\n",
       "      <td>12.669301</td>\n",
       "      <td>0.096037</td>\n",
       "      <td>0.090778</td>\n",
       "      <td>0.092551</td>\n",
       "      <td>0.088520</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.085882</td>\n",
       "      <td>0.229768</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.207153</td>\n",
       "      <td>0.176757</td>\n",
       "      <td>0.194328</td>\n",
       "      <td>0.168298</td>\n",
       "      <td>4.005371</td>\n",
       "      <td>4.132709</td>\n",
       "      <td>4.062560</td>\n",
       "      <td>3.960747</td>\n",
       "      <td>3.926848</td>\n",
       "      <td>3.804245</td>\n",
       "      <td>0.860937</td>\n",
       "      <td>0.707817</td>\n",
       "      <td>0.780991</td>\n",
       "      <td>0.670686</td>\n",
       "      <td>0.721775</td>\n",
       "      <td>0.624822</td>\n",
       "      <td>3.513945e+08</td>\n",
       "      <td>2.665934e+08</td>\n",
       "      <td>2.995020e+08</td>\n",
       "      <td>2.437006e+08</td>\n",
       "      <td>2.763165e+08</td>\n",
       "      <td>2.459543e+08</td>\n",
       "      <td>352912.090909</td>\n",
       "      <td>281973.809524</td>\n",
       "      <td>308363.575385</td>\n",
       "      <td>264699.239184</td>\n",
       "      <td>290795.912021</td>\n",
       "      <td>269864.870995</td>\n",
       "      <td>8499.964074</td>\n",
       "      <td>6440.682723</td>\n",
       "      <td>7246.012675</td>\n",
       "      <td>5886.287833</td>\n",
       "      <td>6680.334981</td>\n",
       "      <td>5946.548988</td>\n",
       "      <td>1.745082e+08</td>\n",
       "      <td>1.322654e+08</td>\n",
       "      <td>1.487115e+08</td>\n",
       "      <td>1.211070e+08</td>\n",
       "      <td>1.372723e+08</td>\n",
       "      <td>1.223994e+08</td>\n",
       "      <td>33.481296</td>\n",
       "      <td>-65.217215</td>\n",
       "      <td>-45.343523</td>\n",
       "      <td>15.586269</td>\n",
       "      <td>133864.555978</td>\n",
       "      <td>154.544854</td>\n",
       "      <td>-163.647592</td>\n",
       "      <td>38.646697</td>\n",
       "      <td>-0.817219</td>\n",
       "      <td>-730484.467639</td>\n",
       "      <td>-135.168648</td>\n",
       "      <td>-6.383382e+04</td>\n",
       "      <td>1.038410</td>\n",
       "      <td>0.943717</td>\n",
       "      <td>0.991980</td>\n",
       "      <td>0.989255</td>\n",
       "      <td>0.990556</td>\n",
       "      <td>0.994493</td>\n",
       "      <td>0.880098</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.649348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7266</th>\n",
       "      <td>4.788095</td>\n",
       "      <td>1.021429</td>\n",
       "      <td>21378.707143</td>\n",
       "      <td>730.971429</td>\n",
       "      <td>172.471290</td>\n",
       "      <td>6.830952</td>\n",
       "      <td>0.097456</td>\n",
       "      <td>0.310215</td>\n",
       "      <td>4.176471</td>\n",
       "      <td>1.295602</td>\n",
       "      <td>20373.64</td>\n",
       "      <td>20468.89</td>\n",
       "      <td>20356.00</td>\n",
       "      <td>20433.90</td>\n",
       "      <td>11550.60960</td>\n",
       "      <td>2.358457e+08</td>\n",
       "      <td>235484</td>\n",
       "      <td>5953.46508</td>\n",
       "      <td>1.215626e+08</td>\n",
       "      <td>20382.346667</td>\n",
       "      <td>20363.910</td>\n",
       "      <td>20398.346614</td>\n",
       "      <td>20395.231660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78</td>\n",
       "      <td>24</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>20122.069461</td>\n",
       "      <td>19693.932742</td>\n",
       "      <td>19889.033849</td>\n",
       "      <td>19670.271203</td>\n",
       "      <td>19870.434479</td>\n",
       "      <td>20068.609171</td>\n",
       "      <td>10.153030</td>\n",
       "      <td>8.253041</td>\n",
       "      <td>8.767848</td>\n",
       "      <td>8.357073</td>\n",
       "      <td>8.691330</td>\n",
       "      <td>8.501462</td>\n",
       "      <td>2.396753</td>\n",
       "      <td>1.718880</td>\n",
       "      <td>1.968370</td>\n",
       "      <td>2.061658</td>\n",
       "      <td>2.063558</td>\n",
       "      <td>1.983258</td>\n",
       "      <td>16038.543290</td>\n",
       "      <td>12008.585727</td>\n",
       "      <td>13298.631381</td>\n",
       "      <td>10961.189365</td>\n",
       "      <td>12472.172986</td>\n",
       "      <td>11483.162659</td>\n",
       "      <td>784.104545</td>\n",
       "      <td>779.363844</td>\n",
       "      <td>788.540094</td>\n",
       "      <td>820.899107</td>\n",
       "      <td>808.667158</td>\n",
       "      <td>814.091874</td>\n",
       "      <td>364.888050</td>\n",
       "      <td>355.798661</td>\n",
       "      <td>336.177841</td>\n",
       "      <td>339.077899</td>\n",
       "      <td>328.389169</td>\n",
       "      <td>335.806184</td>\n",
       "      <td>14.946537</td>\n",
       "      <td>11.690801</td>\n",
       "      <td>12.704588</td>\n",
       "      <td>12.480389</td>\n",
       "      <td>12.818446</td>\n",
       "      <td>12.467979</td>\n",
       "      <td>0.097764</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>0.093145</td>\n",
       "      <td>0.088999</td>\n",
       "      <td>0.090624</td>\n",
       "      <td>0.086281</td>\n",
       "      <td>0.242825</td>\n",
       "      <td>0.195699</td>\n",
       "      <td>0.217096</td>\n",
       "      <td>0.182932</td>\n",
       "      <td>0.202054</td>\n",
       "      <td>0.173191</td>\n",
       "      <td>3.972463</td>\n",
       "      <td>4.142513</td>\n",
       "      <td>4.064156</td>\n",
       "      <td>3.975110</td>\n",
       "      <td>3.943490</td>\n",
       "      <td>3.817081</td>\n",
       "      <td>0.909986</td>\n",
       "      <td>0.760883</td>\n",
       "      <td>0.826133</td>\n",
       "      <td>0.699102</td>\n",
       "      <td>0.760030</td>\n",
       "      <td>0.647952</td>\n",
       "      <td>3.598099e+08</td>\n",
       "      <td>2.743231e+08</td>\n",
       "      <td>2.988178e+08</td>\n",
       "      <td>2.446915e+08</td>\n",
       "      <td>2.736185e+08</td>\n",
       "      <td>2.456057e+08</td>\n",
       "      <td>356442.363636</td>\n",
       "      <td>288417.476190</td>\n",
       "      <td>306429.458462</td>\n",
       "      <td>264447.982041</td>\n",
       "      <td>287108.451219</td>\n",
       "      <td>268679.323719</td>\n",
       "      <td>8717.300795</td>\n",
       "      <td>6640.637899</td>\n",
       "      <td>7248.011548</td>\n",
       "      <td>5921.057685</td>\n",
       "      <td>6631.876987</td>\n",
       "      <td>5946.787474</td>\n",
       "      <td>1.789170e+08</td>\n",
       "      <td>1.363257e+08</td>\n",
       "      <td>1.486940e+08</td>\n",
       "      <td>1.217723e+08</td>\n",
       "      <td>1.362250e+08</td>\n",
       "      <td>1.223705e+08</td>\n",
       "      <td>37.616040</td>\n",
       "      <td>-66.928370</td>\n",
       "      <td>-49.660492</td>\n",
       "      <td>22.944169</td>\n",
       "      <td>135321.336350</td>\n",
       "      <td>157.566520</td>\n",
       "      <td>-92.285657</td>\n",
       "      <td>40.227400</td>\n",
       "      <td>-0.548158</td>\n",
       "      <td>-717661.040914</td>\n",
       "      <td>-106.811199</td>\n",
       "      <td>-7.790618e+04</td>\n",
       "      <td>1.045109</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>0.991490</td>\n",
       "      <td>0.992896</td>\n",
       "      <td>0.996945</td>\n",
       "      <td>0.776355</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.516430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7267</th>\n",
       "      <td>10.600000</td>\n",
       "      <td>1.764286</td>\n",
       "      <td>15495.466667</td>\n",
       "      <td>663.738095</td>\n",
       "      <td>170.085989</td>\n",
       "      <td>14.128571</td>\n",
       "      <td>0.078864</td>\n",
       "      <td>0.206627</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.671536</td>\n",
       "      <td>20433.90</td>\n",
       "      <td>20438.01</td>\n",
       "      <td>20360.37</td>\n",
       "      <td>20371.72</td>\n",
       "      <td>12193.69642</td>\n",
       "      <td>2.487188e+08</td>\n",
       "      <td>253179</td>\n",
       "      <td>5944.94510</td>\n",
       "      <td>1.212673e+08</td>\n",
       "      <td>20393.136667</td>\n",
       "      <td>20383.432</td>\n",
       "      <td>20397.976601</td>\n",
       "      <td>20395.429401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86</td>\n",
       "      <td>22</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>20127.144185</td>\n",
       "      <td>19696.744804</td>\n",
       "      <td>19891.798667</td>\n",
       "      <td>19671.787280</td>\n",
       "      <td>19872.205882</td>\n",
       "      <td>20069.221187</td>\n",
       "      <td>10.154762</td>\n",
       "      <td>8.219420</td>\n",
       "      <td>8.981073</td>\n",
       "      <td>8.441675</td>\n",
       "      <td>8.818574</td>\n",
       "      <td>8.573825</td>\n",
       "      <td>2.423160</td>\n",
       "      <td>1.718842</td>\n",
       "      <td>1.980528</td>\n",
       "      <td>2.045768</td>\n",
       "      <td>2.043607</td>\n",
       "      <td>1.975708</td>\n",
       "      <td>16160.305411</td>\n",
       "      <td>12117.176094</td>\n",
       "      <td>13641.763022</td>\n",
       "      <td>11198.981119</td>\n",
       "      <td>12673.725898</td>\n",
       "      <td>11621.517969</td>\n",
       "      <td>776.725325</td>\n",
       "      <td>780.723498</td>\n",
       "      <td>779.873089</td>\n",
       "      <td>814.124555</td>\n",
       "      <td>799.005221</td>\n",
       "      <td>808.907261</td>\n",
       "      <td>327.425440</td>\n",
       "      <td>337.453518</td>\n",
       "      <td>323.177392</td>\n",
       "      <td>332.908294</td>\n",
       "      <td>317.835624</td>\n",
       "      <td>330.091695</td>\n",
       "      <td>15.001082</td>\n",
       "      <td>11.657104</td>\n",
       "      <td>12.942129</td>\n",
       "      <td>12.533211</td>\n",
       "      <td>12.905788</td>\n",
       "      <td>12.525240</td>\n",
       "      <td>0.096117</td>\n",
       "      <td>0.090859</td>\n",
       "      <td>0.092274</td>\n",
       "      <td>0.088722</td>\n",
       "      <td>0.089840</td>\n",
       "      <td>0.086026</td>\n",
       "      <td>0.246812</td>\n",
       "      <td>0.198606</td>\n",
       "      <td>0.218397</td>\n",
       "      <td>0.184945</td>\n",
       "      <td>0.202359</td>\n",
       "      <td>0.174344</td>\n",
       "      <td>3.718675</td>\n",
       "      <td>4.142778</td>\n",
       "      <td>3.993860</td>\n",
       "      <td>3.952366</td>\n",
       "      <td>3.897257</td>\n",
       "      <td>3.797526</td>\n",
       "      <td>0.881635</td>\n",
       "      <td>0.770370</td>\n",
       "      <td>0.820397</td>\n",
       "      <td>0.702461</td>\n",
       "      <td>0.754130</td>\n",
       "      <td>0.648765</td>\n",
       "      <td>3.640443e+08</td>\n",
       "      <td>2.736767e+08</td>\n",
       "      <td>2.989510e+08</td>\n",
       "      <td>2.462419e+08</td>\n",
       "      <td>2.719585e+08</td>\n",
       "      <td>2.457131e+08</td>\n",
       "      <td>356418.818182</td>\n",
       "      <td>287496.809524</td>\n",
       "      <td>305749.643077</td>\n",
       "      <td>264984.405714</td>\n",
       "      <td>284846.487805</td>\n",
       "      <td>268144.829798</td>\n",
       "      <td>8818.509015</td>\n",
       "      <td>6638.899041</td>\n",
       "      <td>7244.882583</td>\n",
       "      <td>5956.192795</td>\n",
       "      <td>6586.081528</td>\n",
       "      <td>5946.723944</td>\n",
       "      <td>1.809423e+08</td>\n",
       "      <td>1.362389e+08</td>\n",
       "      <td>1.485654e+08</td>\n",
       "      <td>1.224429e+08</td>\n",
       "      <td>1.352278e+08</td>\n",
       "      <td>1.223325e+08</td>\n",
       "      <td>35.885754</td>\n",
       "      <td>-69.676106</td>\n",
       "      <td>-53.663615</td>\n",
       "      <td>17.644362</td>\n",
       "      <td>131090.611408</td>\n",
       "      <td>158.997389</td>\n",
       "      <td>-254.788480</td>\n",
       "      <td>36.004752</td>\n",
       "      <td>-0.745321</td>\n",
       "      <td>-730075.142941</td>\n",
       "      <td>-105.789351</td>\n",
       "      <td>-9.483240e+04</td>\n",
       "      <td>1.060905</td>\n",
       "      <td>0.930096</td>\n",
       "      <td>0.992758</td>\n",
       "      <td>0.990611</td>\n",
       "      <td>0.992130</td>\n",
       "      <td>0.996344</td>\n",
       "      <td>0.784036</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.635571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7254 rows Ã— 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      favorites  retweets  number_of_followers    following  \\\n",
       "14     9.797619  0.745238         16514.150000  1498.323810   \n",
       "15    39.326190  3.390476         16099.623810  1672.185714   \n",
       "16     5.369048  0.550000         11057.442857  1828.021429   \n",
       "17    14.495238  1.890476         25029.171429  1882.980952   \n",
       "18    13.329545  1.281818         22107.427273  2018.227273   \n",
       "...         ...       ...                  ...          ...   \n",
       "7263  10.030952  2.557143         12561.900000   906.071429   \n",
       "7264   9.507143  3.530952         10695.850000   805.514286   \n",
       "7265  17.957143  4.402381         22596.990476   846.535714   \n",
       "7266   4.788095  1.021429         21378.707143   730.971429   \n",
       "7267  10.600000  1.764286         15495.466667   663.738095   \n",
       "\n",
       "      followers_following_ratio  2x_retweets_+_favorites  polarity  W1 Score  \\\n",
       "14                    63.299076                11.288095  0.113520  0.128081   \n",
       "15                   110.504489                46.107143  0.087327  0.253405   \n",
       "16                   133.780075                 6.469048  0.093584  0.222649   \n",
       "17                   662.631886                18.276190  0.090903  0.173174   \n",
       "18                   479.791704                15.893182  0.095774  0.263250   \n",
       "...                         ...                      ...       ...       ...   \n",
       "7263                 406.088057                15.145238  0.094200  0.365355   \n",
       "7264                 264.229345                16.569048  0.075146  0.250643   \n",
       "7265                 217.348828                26.761905  0.104180  0.403896   \n",
       "7266                 172.471290                 6.830952  0.097456  0.310215   \n",
       "7267                 170.085989                14.128571  0.078864  0.206627   \n",
       "\n",
       "      Bull_ratio  W Score With Bull Ratio      Open      High       Low  \\\n",
       "14      4.256410                 0.545164  46475.60  46645.34  46360.00   \n",
       "15      2.500000                 0.633513  46360.01  46413.47  45696.00   \n",
       "16      2.450000                 0.545491  45922.01  46260.44  45811.87   \n",
       "17      2.500000                 0.432936  45979.01  46439.97  45961.59   \n",
       "18      3.395833                 0.893952  46217.91  46576.94  46170.64   \n",
       "...          ...                      ...       ...       ...       ...   \n",
       "7263    3.066667                 1.120422  20336.22  20465.99  20291.74   \n",
       "7264    2.518519                 0.631248  20402.60  20533.00  20328.68   \n",
       "7265    2.941176                 1.187930  20370.92  20425.28  20345.06   \n",
       "7266    4.176471                 1.295602  20373.64  20468.89  20356.00   \n",
       "7267    3.250000                 0.671536  20433.90  20438.01  20360.37   \n",
       "\n",
       "         Close       Volume  Quote Asset Volume  Number of Trades  \\\n",
       "14    46360.01    661.70660        3.078130e+07             27929   \n",
       "15    45922.00   3127.50381        1.437875e+08             75059   \n",
       "16    45979.01   1055.66747        4.859122e+07             36692   \n",
       "17    46217.90   1002.70049        4.632422e+07             39674   \n",
       "18    46446.10   1055.33575        4.896831e+07             29425   \n",
       "...        ...          ...                 ...               ...   \n",
       "7263  20401.53  18928.44019        3.855259e+08            378888   \n",
       "7264  20371.87  20988.91618        4.285581e+08            383396   \n",
       "7265  20373.64  11827.33569        2.411572e+08            248163   \n",
       "7266  20433.90  11550.60960        2.358457e+08            235484   \n",
       "7267  20371.72  12193.69642        2.487188e+08            253179   \n",
       "\n",
       "      TB Base Volume  TB Quote Volume  3MovingAverage  5MovingAverage  \\\n",
       "14         337.98922     1.572096e+07    46510.740000       46620.040   \n",
       "15        1450.68943     6.667771e+07    46469.963333       46517.136   \n",
       "16         478.35709     2.202104e+07    46251.580000       46362.846   \n",
       "17         482.29605     2.228102e+07    46087.006667       46262.180   \n",
       "18         503.52171     2.336430e+07    46039.636667       46190.330   \n",
       "...              ...              ...             ...             ...   \n",
       "7263      9593.04149     1.953918e+08    20456.926667       20570.376   \n",
       "7264     10453.52667     2.134634e+08    20358.013333       20500.570   \n",
       "7265      5958.90684     1.215062e+08    20369.873333       20428.836   \n",
       "7266      5953.46508     1.215626e+08    20382.346667       20363.910   \n",
       "7267      5944.94510     1.212673e+08    20393.136667       20383.432   \n",
       "\n",
       "      JMJ_3HMoving_averages  JMJ_5HMoving_averages  Actual_Label  \\\n",
       "14             46500.609170           46524.334603           0.0   \n",
       "15             46217.432522           46414.854289           1.0   \n",
       "16             46078.530953           46303.928703           1.0   \n",
       "17             46020.765581           46167.213095           1.0   \n",
       "18             46257.255102           46206.003903           0.0   \n",
       "...                     ...                    ...           ...   \n",
       "7263           20362.555890           20503.444373           0.0   \n",
       "7264           20383.221150           20435.715408           1.0   \n",
       "7265           20383.890119           20366.695247           1.0   \n",
       "7266           20398.346614           20395.231660           0.0   \n",
       "7267           20397.976601           20395.429401           1.0   \n",
       "\n",
       "      Bitcoin_Google_Trend_Score  BTC_Google_Trend_Score  Mkt Sentiment  \\\n",
       "14                            86                      21       0.000000   \n",
       "15                            92                      27       0.100108   \n",
       "16                            99                      30       0.000000   \n",
       "17                            94                      26       0.000000   \n",
       "18                            90                      24       0.000000   \n",
       "...                          ...                     ...            ...   \n",
       "7263                          83                      26       0.037500   \n",
       "7264                          78                      27       0.083333   \n",
       "7265                          78                      26       0.000000   \n",
       "7266                          78                      24       0.144444   \n",
       "7267                          86                      22       0.013889   \n",
       "\n",
       "      Crypto Sentiment  Historically Optimal SMA(s-t)  \\\n",
       "14           -0.086364                   48196.449449   \n",
       "15            0.000000                   48178.173233   \n",
       "16            0.000000                   48159.802603   \n",
       "17            0.000000                   48141.337561   \n",
       "18            0.000000                   48122.778106   \n",
       "...                ...                            ...   \n",
       "7263          0.000000                   20106.722911   \n",
       "7264         -0.083333                   20111.858824   \n",
       "7265          0.214286                   20116.974340   \n",
       "7266          0.200000                   20122.069461   \n",
       "7267          0.050000                   20127.144185   \n",
       "\n",
       "      Historically Optimal SMA(l-t)  Historically Optimal WMA(s-t)  \\\n",
       "14                     48078.186363                   48035.797421   \n",
       "15                     48076.911675                   48028.907428   \n",
       "16                     48075.600968                   48021.949062   \n",
       "17                     48074.254240                   48014.922321   \n",
       "18                     48072.871493                   48007.827207   \n",
       "...                             ...                            ...   \n",
       "7263                   19685.536024                   19880.713661   \n",
       "7264                   19688.328352                   19883.491346   \n",
       "7265                   19691.127258                   19886.264742   \n",
       "7266                   19693.932742                   19889.033849   \n",
       "7267                   19696.744804                   19891.798667   \n",
       "\n",
       "      Historically Optimal WMA(l-t)  Historically Optimal EMA(s-t)  \\\n",
       "14                     49380.744177                   49264.803864   \n",
       "15                     49371.226555                   49255.414588   \n",
       "16                     49361.694240                   49245.984234   \n",
       "17                     49352.147234                   49236.512801   \n",
       "18                     49342.585535                   49227.000289   \n",
       "...                             ...                            ...   \n",
       "7263                   19665.734538                   19865.081402   \n",
       "7264                   19667.244832                   19866.872238   \n",
       "7265                   19668.757053                   19868.656597   \n",
       "7266                   19670.271203                   19870.434479   \n",
       "7267                   19671.787280                   19872.205882   \n",
       "\n",
       "      Historically Optimal EMA(l-t)  Twitter Hourly Favorites SMA(s-t)  \\\n",
       "14                     51365.355477                          11.500328   \n",
       "15                     51357.505310                          14.512476   \n",
       "16                     51349.632204                          14.072162   \n",
       "17                     51341.736161                          14.368398   \n",
       "18                     51333.817181                          14.456799   \n",
       "...                             ...                                ...   \n",
       "7263                   20066.763819                           8.747573   \n",
       "7264                   20067.380487                           8.990829   \n",
       "7265                   20067.995604                          10.259021   \n",
       "7266                   20068.609171                          10.153030   \n",
       "7267                   20069.221187                          10.154762   \n",
       "\n",
       "      Twitter Hourly Favorites SMA(l-t)  Twitter Hourly Favorites WMA(s-t)  \\\n",
       "14                            11.161985                          11.147481   \n",
       "15                            12.794159                          13.346864   \n",
       "16                            12.618942                          12.844038   \n",
       "17                            11.919132                          13.068302   \n",
       "18                            11.881377                          13.173975   \n",
       "...                                 ...                                ...   \n",
       "7263                           7.567017                           8.064936   \n",
       "7264                           7.818371                           8.208251   \n",
       "7265                           8.221787                           9.000313   \n",
       "7266                           8.253041                           8.767848   \n",
       "7267                           8.219420                           8.981073   \n",
       "\n",
       "      Twitter Hourly Favorites WMA(l-t)  Twitter Hourly Favorites EMA(s-t)  \\\n",
       "14                            12.036069                          11.633252   \n",
       "15                            13.108289                          13.479448   \n",
       "16                            12.795033                          12.938754   \n",
       "17                            12.847442                          13.042520   \n",
       "18                            12.852665                          13.061655   \n",
       "...                                 ...                                ...   \n",
       "7263                           8.075007                           8.243993   \n",
       "7264                           8.122332                           8.328203   \n",
       "7265                           8.506688                           8.970132   \n",
       "7266                           8.357073                           8.691330   \n",
       "7267                           8.441675                           8.818574   \n",
       "\n",
       "      Twitter Hourly Favorites EMA(l-t)  Twitter Hourly Retweets SMA(s-t)  \\\n",
       "14                            11.401072                          1.545594   \n",
       "15                            12.364007                          1.795136   \n",
       "16                            12.122801                          1.722408   \n",
       "17                            12.204610                          1.660173   \n",
       "18                            12.243400                          1.584061   \n",
       "...                                 ...                               ...   \n",
       "7263                           8.258043                          1.893712   \n",
       "7264                           8.301116                          2.104522   \n",
       "7265                           8.634082                          2.377636   \n",
       "7266                           8.501462                          2.396753   \n",
       "7267                           8.573825                          2.423160   \n",
       "\n",
       "      Twitter Hourly Retweets SMA(l-t)  Twitter Hourly Retweets WMA(s-t)  \\\n",
       "14                            1.562554                          1.542539   \n",
       "15                            1.686271                          1.690355   \n",
       "16                            1.632447                          1.613099   \n",
       "17                            1.605507                          1.641120   \n",
       "18                            1.604960                          1.618556   \n",
       "...                                ...                               ...   \n",
       "7263                          1.447170                          1.672537   \n",
       "7264                          1.573244                          1.811319   \n",
       "7265                          1.718083                          2.012489   \n",
       "7266                          1.718880                          1.968370   \n",
       "7267                          1.718842                          1.980528   \n",
       "\n",
       "      Twitter Hourly Retweets WMA(l-t)  Twitter Hourly Retweets EMA(s-t)  \\\n",
       "14                            1.644936                          1.571697   \n",
       "15                            1.714186                          1.692949   \n",
       "16                            1.667652                          1.616752   \n",
       "17                            1.675060                          1.635001   \n",
       "18                            1.657281                          1.611455   \n",
       "...                                ...                               ...   \n",
       "7263                          1.956401                          1.865205   \n",
       "7264                          2.015084                          1.976254   \n",
       "7265                          2.107106                          2.137996   \n",
       "7266                          2.061658                          2.063558   \n",
       "7267                          2.045768                          2.043607   \n",
       "\n",
       "      Twitter Hourly Retweets EMA(l-t)  \\\n",
       "14                            1.511503   \n",
       "15                            1.576295   \n",
       "16                            1.540905   \n",
       "17                            1.552959   \n",
       "18                            1.543610   \n",
       "...                                ...   \n",
       "7263                          1.875349   \n",
       "7264                          1.932439   \n",
       "7265                          2.017609   \n",
       "7266                          1.983258   \n",
       "7267                          1.975708   \n",
       "\n",
       "      Twitter Hourly Follower Exposure SMA(s-t)  \\\n",
       "14                                 24897.966177   \n",
       "15                                 24675.783182   \n",
       "16                                 23801.288214   \n",
       "17                                 23579.362554   \n",
       "18                                 24459.936029   \n",
       "...                                         ...   \n",
       "7263                               12379.205002   \n",
       "7264                               12992.051826   \n",
       "7265                               14665.287281   \n",
       "7266                               16038.543290   \n",
       "7267                               16160.305411   \n",
       "\n",
       "      Twitter Hourly Follower Exposure SMA(l-t)  \\\n",
       "14                                 20753.002445   \n",
       "15                                 21099.259845   \n",
       "16                                 21023.709688   \n",
       "17                                 21350.167928   \n",
       "18                                 21505.479817   \n",
       "...                                         ...   \n",
       "7263                               10281.205779   \n",
       "7264                               10582.509762   \n",
       "7265                               11165.740962   \n",
       "7266                               12008.585727   \n",
       "7267                               12117.176094   \n",
       "\n",
       "      Twitter Hourly Follower Exposure WMA(s-t)  \\\n",
       "14                                 22571.536004   \n",
       "15                                 22313.158515   \n",
       "16                                 21653.515571   \n",
       "17                                 22093.562472   \n",
       "18                                 22261.591283   \n",
       "...                                         ...   \n",
       "7263                               11407.914296   \n",
       "7264                               11476.362045   \n",
       "7265                               12451.240578   \n",
       "7266                               13298.631381   \n",
       "7267                               13641.763022   \n",
       "\n",
       "      Twitter Hourly Follower Exposure WMA(l-t)  \\\n",
       "14                                 20141.160233   \n",
       "15                                 20042.453125   \n",
       "16                                 19742.111264   \n",
       "17                                 19998.332506   \n",
       "18                                 20125.761693   \n",
       "...                                         ...   \n",
       "7263                                9973.987460   \n",
       "7264                                9988.693386   \n",
       "7265                               10476.812475   \n",
       "7266                               10961.189365   \n",
       "7267                               11198.981119   \n",
       "\n",
       "      Twitter Hourly Follower Exposure EMA(s-t)  \\\n",
       "14                                 21554.559441   \n",
       "15                                 21190.897066   \n",
       "16                                 20515.333452   \n",
       "17                                 20816.255983   \n",
       "18                                 20902.334069   \n",
       "...                                         ...   \n",
       "7263                               11093.884884   \n",
       "7264                               11067.349225   \n",
       "7265                               11835.991975   \n",
       "7266                               12472.172986   \n",
       "7267                               12673.725898   \n",
       "\n",
       "      Twitter Hourly Follower Exposure EMA(l-t)  \\\n",
       "14                                 19541.657515   \n",
       "15                                 19422.966698   \n",
       "16                                 19134.500358   \n",
       "17                                 19337.764878   \n",
       "18                                 19433.270478   \n",
       "...                                         ...   \n",
       "7263                               10721.075925   \n",
       "7264                               10720.206066   \n",
       "7265                               11129.750356   \n",
       "7266                               11483.162659   \n",
       "7267                               11621.517969   \n",
       "\n",
       "      Twitter Hourly Following Exposure SMA(s-t)  \\\n",
       "14                                   1606.423217   \n",
       "15                                   1636.866995   \n",
       "16                                   1639.810761   \n",
       "17                                   1692.227489   \n",
       "18                                   1724.331267   \n",
       "...                                          ...   \n",
       "7263                                  777.943689   \n",
       "7264                                  769.638635   \n",
       "7265                                  775.342010   \n",
       "7266                                  784.104545   \n",
       "7267                                  776.725325   \n",
       "\n",
       "      Twitter Hourly Following Exposure SMA(l-t)  \\\n",
       "14                                   1904.300039   \n",
       "15                                   1895.659837   \n",
       "16                                   1830.511606   \n",
       "17                                   1828.334926   \n",
       "18                                   1793.846133   \n",
       "...                                          ...   \n",
       "7263                                  780.015474   \n",
       "7264                                  780.422305   \n",
       "7265                                  781.996129   \n",
       "7266                                  779.363844   \n",
       "7267                                  780.723498   \n",
       "\n",
       "      Twitter Hourly Following Exposure WMA(s-t)  \\\n",
       "14                                   1792.862905   \n",
       "15                                   1774.912759   \n",
       "16                                   1769.942474   \n",
       "17                                   1768.932371   \n",
       "18                                   1778.152576   \n",
       "...                                          ...   \n",
       "7263                                  792.954043   \n",
       "7264                                  790.760497   \n",
       "7265                                  792.184944   \n",
       "7266                                  788.540094   \n",
       "7267                                  779.873089   \n",
       "\n",
       "      Twitter Hourly Following Exposure WMA(l-t)  \\\n",
       "14                                   1893.415089   \n",
       "15                                   1881.602283   \n",
       "16                                   1876.038300   \n",
       "17                                   1872.605753   \n",
       "18                                   1874.662369   \n",
       "...                                          ...   \n",
       "7263                                  824.985792   \n",
       "7264                                  824.143656   \n",
       "7265                                  824.888651   \n",
       "7266                                  820.899107   \n",
       "7267                                  814.124555   \n",
       "\n",
       "      Twitter Hourly Following Exposure EMA(s-t)  \\\n",
       "14                                   1843.454381   \n",
       "15                                   1832.036470   \n",
       "16                                   1831.768801   \n",
       "17                                   1835.182944   \n",
       "18                                   1847.385900   \n",
       "...                                          ...   \n",
       "7263                                  812.365083   \n",
       "7264                                  811.908363   \n",
       "7265                                  814.216853   \n",
       "7266                                  808.667158   \n",
       "7267                                  799.005221   \n",
       "\n",
       "      Twitter Hourly Following Exposure EMA(l-t)  \\\n",
       "14                                   1971.350293   \n",
       "15                                   1961.034273   \n",
       "16                                   1956.447623   \n",
       "17                                   1953.914290   \n",
       "18                                   1956.131979   \n",
       "...                                          ...   \n",
       "7263                                  816.382542   \n",
       "7264                                  816.007774   \n",
       "7265                                  817.060462   \n",
       "7266                                  814.091874   \n",
       "7267                                  808.907261   \n",
       "\n",
       "      Twitter Hourly Follower to Following Ratio SMA(s-t)  \\\n",
       "14                                           563.035330     \n",
       "15                                           567.066258     \n",
       "16                                           542.151802     \n",
       "17                                           494.021490     \n",
       "18                                           531.765129     \n",
       "...                                                 ...     \n",
       "7263                                         352.799905     \n",
       "7264                                         357.414598     \n",
       "7265                                         363.593749     \n",
       "7266                                         364.888050     \n",
       "7267                                         327.425440     \n",
       "\n",
       "      Twitter Hourly Follower to Following Ratio SMA(l-t)  \\\n",
       "14                                           446.938354     \n",
       "15                                           445.774898     \n",
       "16                                           447.568914     \n",
       "17                                           474.011983     \n",
       "18                                           491.145712     \n",
       "...                                                 ...     \n",
       "7263                                         363.852343     \n",
       "7264                                         353.547375     \n",
       "7265                                         355.191916     \n",
       "7266                                         355.798661     \n",
       "7267                                         337.453518     \n",
       "\n",
       "      Twitter Hourly Follower to Following Ratio WMA(s-t)  \\\n",
       "14                                           508.380162     \n",
       "15                                           486.235689     \n",
       "16                                           466.201592     \n",
       "17                                           486.977631     \n",
       "18                                           491.741615     \n",
       "...                                                 ...     \n",
       "7263                                         364.596692     \n",
       "7264                                         358.771464     \n",
       "7265                                         349.272804     \n",
       "7266                                         336.177841     \n",
       "7267                                         323.177392     \n",
       "\n",
       "      Twitter Hourly Follower to Following Ratio WMA(l-t)  \\\n",
       "14                                           396.751881     \n",
       "15                                           386.793989     \n",
       "16                                           378.100704     \n",
       "17                                           390.510159     \n",
       "18                                           395.430495     \n",
       "...                                                 ...     \n",
       "7263                                         351.897675     \n",
       "7264                                         349.482088     \n",
       "7265                                         345.166151     \n",
       "7266                                         339.077899     \n",
       "7267                                         332.908294     \n",
       "\n",
       "      Twitter Hourly Follower to Following Ratio EMA(s-t)  \\\n",
       "14                                           451.446392     \n",
       "15                                           428.716932     \n",
       "16                                           409.054475     \n",
       "17                                           425.959636     \n",
       "18                                           429.548440     \n",
       "...                                                 ...     \n",
       "7263                                         354.254811     \n",
       "7264                                         348.253113     \n",
       "7265                                         339.526161     \n",
       "7266                                         328.389169     \n",
       "7267                                         317.835624     \n",
       "\n",
       "      Twitter Hourly Follower to Following Ratio EMA(l-t)  \\\n",
       "14                                           381.105084     \n",
       "15                                           371.774029     \n",
       "16                                           363.567341     \n",
       "17                                           373.879911     \n",
       "18                                           377.532042     \n",
       "...                                                 ...     \n",
       "7263                                         349.001714     \n",
       "7264                                         346.078529     \n",
       "7265                                         341.639573     \n",
       "7266                                         335.806184     \n",
       "7267                                         330.091695     \n",
       "\n",
       "      Twitter Hourly 2x Retweets + Favorites SMA(s-t)  \\\n",
       "14                                          14.591515   \n",
       "15                                          18.102747   \n",
       "16                                          17.516979   \n",
       "17                                          17.688745   \n",
       "18                                          17.624921   \n",
       "...                                               ...   \n",
       "7263                                        12.534997   \n",
       "7264                                        13.199872   \n",
       "7265                                        15.014293   \n",
       "7266                                        14.946537   \n",
       "7267                                        15.001082   \n",
       "\n",
       "      Twitter Hourly 2x Retweets + Favorites SMA(l-t)  \\\n",
       "14                                          14.287093   \n",
       "15                                          16.166701   \n",
       "16                                          15.883836   \n",
       "17                                          15.130146   \n",
       "18                                          15.091297   \n",
       "...                                               ...   \n",
       "7263                                        10.461357   \n",
       "7264                                        10.964858   \n",
       "7265                                        11.657954   \n",
       "7266                                        11.690801   \n",
       "7267                                        11.657104   \n",
       "\n",
       "      Twitter Hourly 2x Retweets + Favorites WMA(s-t)  \\\n",
       "14                                          14.232559   \n",
       "15                                          16.727574   \n",
       "16                                          16.070237   \n",
       "17                                          16.350543   \n",
       "18                                          16.411088   \n",
       "...                                               ...   \n",
       "7263                                        11.410010   \n",
       "7264                                        11.830889   \n",
       "7265                                        13.025291   \n",
       "7266                                        12.704588   \n",
       "7267                                        12.942129   \n",
       "\n",
       "      Twitter Hourly 2x Retweets + Favorites WMA(l-t)  \\\n",
       "14                                          15.325942   \n",
       "15                                          16.536661   \n",
       "16                                          16.130337   \n",
       "17                                          16.197561   \n",
       "18                                          16.167228   \n",
       "...                                               ...   \n",
       "7263                                        11.987809   \n",
       "7264                                        12.152500   \n",
       "7265                                        12.720899   \n",
       "7266                                        12.480389   \n",
       "7267                                        12.533211   \n",
       "\n",
       "      Twitter Hourly 2x Retweets + Favorites EMA(s-t)  \\\n",
       "14                                          14.776646   \n",
       "15                                          16.865346   \n",
       "16                                          16.172259   \n",
       "17                                          16.312521   \n",
       "18                                          16.284565   \n",
       "...                                               ...   \n",
       "7263                                        11.974402   \n",
       "7264                                        12.280712   \n",
       "7265                                        13.246124   \n",
       "7266                                        12.818446   \n",
       "7267                                        12.905788   \n",
       "\n",
       "      Twitter Hourly 2x Retweets + Favorites EMA(l-t)  \\\n",
       "14                                          14.424077   \n",
       "15                                          15.516596   \n",
       "16                                          15.204612   \n",
       "17                                          15.310528   \n",
       "18                                          15.330620   \n",
       "...                                               ...   \n",
       "7263                                        12.008742   \n",
       "7264                                        12.165994   \n",
       "7265                                        12.669301   \n",
       "7266                                        12.467979   \n",
       "7267                                        12.525240   \n",
       "\n",
       "      Twitter Hourly Polarity Score SMA(s-t)  \\\n",
       "14                                  0.112792   \n",
       "15                                  0.110745   \n",
       "16                                  0.111309   \n",
       "17                                  0.110456   \n",
       "18                                  0.111724   \n",
       "...                                      ...   \n",
       "7263                                0.095291   \n",
       "7264                                0.093947   \n",
       "7265                                0.096037   \n",
       "7266                                0.097764   \n",
       "7267                                0.096117   \n",
       "\n",
       "      Twitter Hourly Polarity Score SMA(l-t)  \\\n",
       "14                                  0.099203   \n",
       "15                                  0.100007   \n",
       "16                                  0.100722   \n",
       "17                                  0.100282   \n",
       "18                                  0.101225   \n",
       "...                                      ...   \n",
       "7263                                0.090120   \n",
       "7264                                0.089850   \n",
       "7265                                0.090778   \n",
       "7266                                0.091349   \n",
       "7267                                0.090859   \n",
       "\n",
       "      Twitter Hourly Polarity Score WMA(s-t)  \\\n",
       "14                                  0.104325   \n",
       "15                                  0.103747   \n",
       "16                                  0.103592   \n",
       "17                                  0.103144   \n",
       "18                                  0.103072   \n",
       "...                                      ...   \n",
       "7263                                0.092446   \n",
       "7264                                0.091345   \n",
       "7265                                0.092551   \n",
       "7266                                0.093145   \n",
       "7267                                0.092274   \n",
       "\n",
       "      Twitter Hourly Polarity Score WMA(l-t)  \\\n",
       "14                                  0.092270   \n",
       "15                                  0.092356   \n",
       "16                                  0.092673   \n",
       "17                                  0.092867   \n",
       "18                                  0.093232   \n",
       "...                                      ...   \n",
       "7263                                0.088132   \n",
       "7264                                0.087751   \n",
       "7265                                0.088520   \n",
       "7266                                0.088999   \n",
       "7267                                0.088722   \n",
       "\n",
       "      Twitter Hourly Polarity Score EMA(s-t)  \\\n",
       "14                                  0.098422   \n",
       "15                                  0.097683   \n",
       "16                                  0.097409   \n",
       "17                                  0.096976   \n",
       "18                                  0.096896   \n",
       "...                                      ...   \n",
       "7263                                0.090132   \n",
       "7264                                0.089133   \n",
       "7265                                0.090136   \n",
       "7266                                0.090624   \n",
       "7267                                0.089840   \n",
       "\n",
       "      Twitter Hourly Polarity Score EMA(l-t)  Twitter W1 Score SMA(s-t)  \\\n",
       "14                                  0.092775                   0.295329   \n",
       "15                                  0.092588                   0.292772   \n",
       "16                                  0.092622                   0.298386   \n",
       "17                                  0.092563                   0.288082   \n",
       "18                                  0.092673                   0.276411   \n",
       "...                                      ...                        ...   \n",
       "7263                                0.085589                   0.205164   \n",
       "7264                                0.085229                   0.206405   \n",
       "7265                                0.085882                   0.229768   \n",
       "7266                                0.086281                   0.242825   \n",
       "7267                                0.086026                   0.246812   \n",
       "\n",
       "      Twitter W1 Score SMA(l-t)  Twitter W1 Score WMA(s-t)  \\\n",
       "14                     0.244635                   0.261049   \n",
       "15                     0.249328                   0.262863   \n",
       "16                     0.254827                   0.261851   \n",
       "17                     0.249972                   0.256985   \n",
       "18                     0.259173                   0.258778   \n",
       "...                         ...                        ...   \n",
       "7263                   0.171433                   0.183005   \n",
       "7264                   0.176066                   0.189239   \n",
       "7265                   0.183100                   0.207153   \n",
       "7266                   0.195699                   0.217096   \n",
       "7267                   0.198606                   0.218397   \n",
       "\n",
       "      Twitter W1 Score WMA(l-t)  Twitter W1 Score EMA(s-t)  \\\n",
       "14                     0.220459                   0.236897   \n",
       "15                     0.222651                   0.237998   \n",
       "16                     0.223563                   0.236975   \n",
       "17                     0.222452                   0.232721   \n",
       "18                     0.224997                   0.234757   \n",
       "...                         ...                        ...   \n",
       "7263                   0.162588                   0.174267   \n",
       "7264                   0.166651                   0.179359   \n",
       "7265                   0.176757                   0.194328   \n",
       "7266                   0.182932                   0.202054   \n",
       "7267                   0.184945                   0.202359   \n",
       "\n",
       "      Twitter W1 Score EMA(l-t)  Twitter Hourly Bull Ratio SMA(s-t)  \\\n",
       "14                     0.213518                            4.649767   \n",
       "15                     0.214893                            4.328450   \n",
       "16                     0.215161                            4.170496   \n",
       "17                     0.213713                            3.936230   \n",
       "18                     0.215421                            3.986356   \n",
       "...                         ...                                 ...   \n",
       "7263                   0.156642                            4.320211   \n",
       "7264                   0.159883                            4.072803   \n",
       "7265                   0.168298                            4.005371   \n",
       "7266                   0.173191                            3.972463   \n",
       "7267                   0.174344                            3.718675   \n",
       "\n",
       "      Twitter Hourly Bull Ratio SMA(l-t)  Twitter Hourly Bull Ratio WMA(s-t)  \\\n",
       "14                              4.045326                            4.227080   \n",
       "15                              4.052087                            4.121644   \n",
       "16                              4.021616                            4.014492   \n",
       "17                              3.961828                            3.911249   \n",
       "18                              4.004486                            3.880251   \n",
       "...                                  ...                                 ...   \n",
       "7263                            4.216202                            4.288653   \n",
       "7264                            4.146789                            4.157321   \n",
       "7265                            4.132709                            4.062560   \n",
       "7266                            4.142513                            4.064156   \n",
       "7267                            4.142778                            3.993860   \n",
       "\n",
       "      Twitter Hourly Bull Ratio WMA(l-t)  Twitter Hourly Bull Ratio EMA(s-t)  \\\n",
       "14                              3.824881                            4.004069   \n",
       "15                              3.781862                            3.903797   \n",
       "16                              3.737251                            3.806878   \n",
       "17                              3.694877                            3.719752   \n",
       "18                              3.688266                            3.698158   \n",
       "...                                  ...                                 ...   \n",
       "7263                            4.047875                            4.102877   \n",
       "7264                            3.995939                            3.997253   \n",
       "7265                            3.960747                            3.926848   \n",
       "7266                            3.975110                            3.943490   \n",
       "7267                            3.952366                            3.897257   \n",
       "\n",
       "      Twitter Hourly Bull Ratio EMA(l-t)  \\\n",
       "14                              3.789511   \n",
       "15                              3.745045   \n",
       "16                              3.700389   \n",
       "17                              3.658996   \n",
       "18                              3.649921   \n",
       "...                                  ...   \n",
       "7263                            3.882089   \n",
       "7264                            3.835069   \n",
       "7265                            3.804245   \n",
       "7266                            3.817081   \n",
       "7267                            3.797526   \n",
       "\n",
       "      Twitter Hourly Weighted Bull Ratio SMA(s-t)  \\\n",
       "14                                       1.493411   \n",
       "15                                       1.396555   \n",
       "16                                       1.384898   \n",
       "17                                       1.292016   \n",
       "18                                       1.272013   \n",
       "...                                           ...   \n",
       "7263                                     0.857634   \n",
       "7264                                     0.802128   \n",
       "7265                                     0.860937   \n",
       "7266                                     0.909986   \n",
       "7267                                     0.881635   \n",
       "\n",
       "      Twitter Hourly Weighted Bull Ratio SMA(l-t)  \\\n",
       "14                                       1.112052   \n",
       "15                                       1.124832   \n",
       "16                                       1.135039   \n",
       "17                                       1.106452   \n",
       "18                                       1.140682   \n",
       "...                                           ...   \n",
       "7263                                     0.689712   \n",
       "7264                                     0.690735   \n",
       "7265                                     0.707817   \n",
       "7266                                     0.760883   \n",
       "7267                                     0.770370   \n",
       "\n",
       "      Twitter Hourly Weighted Bull Ratio WMA(s-t)  \\\n",
       "14                                       1.227893   \n",
       "15                                       1.199559   \n",
       "16                                       1.163523   \n",
       "17                                       1.118726   \n",
       "18                                       1.109001   \n",
       "...                                           ...   \n",
       "7263                                     0.746614   \n",
       "7264                                     0.742093   \n",
       "7265                                     0.780991   \n",
       "7266                                     0.826133   \n",
       "7267                                     0.820397   \n",
       "\n",
       "      Twitter Hourly Weighted Bull Ratio WMA(l-t)  \\\n",
       "14                                       0.949826   \n",
       "15                                       0.943834   \n",
       "16                                       0.934275   \n",
       "17                                       0.920144   \n",
       "18                                       0.924572   \n",
       "...                                           ...   \n",
       "7263                                     0.643463   \n",
       "7264                                     0.646061   \n",
       "7265                                     0.670686   \n",
       "7266                                     0.699102   \n",
       "7267                                     0.702461   \n",
       "\n",
       "      Twitter Hourly Weighted Bull Ratio EMA(s-t)  \\\n",
       "14                                       1.065037   \n",
       "15                                       1.036269   \n",
       "16                                       1.003551   \n",
       "17                                       0.965510   \n",
       "18                                       0.960739   \n",
       "...                                           ...   \n",
       "7263                                     0.692566   \n",
       "7264                                     0.688478   \n",
       "7265                                     0.721775   \n",
       "7266                                     0.760030   \n",
       "7267                                     0.754130   \n",
       "\n",
       "      Twitter Hourly Weighted Bull Ratio EMA(l-t)  \\\n",
       "14                                       0.895558   \n",
       "15                                       0.886522   \n",
       "16                                       0.874762   \n",
       "17                                       0.859527   \n",
       "18                                       0.860714   \n",
       "...                                           ...   \n",
       "7263                                     0.603763   \n",
       "7264                                     0.604711   \n",
       "7265                                     0.624822   \n",
       "7266                                     0.647952   \n",
       "7267                                     0.648765   \n",
       "\n",
       "      Quote Asset Volume SMA(s-t)  Quote Asset Volume SMA(l-t)  \\\n",
       "14                   6.348281e+07                 4.952478e+07   \n",
       "15                   7.234468e+07                 5.468189e+07   \n",
       "16                   7.188079e+07                 5.534665e+07   \n",
       "17                   6.984983e+07                 5.614564e+07   \n",
       "18                   6.920458e+07                 5.672918e+07   \n",
       "...                           ...                          ...   \n",
       "7263                 3.200849e+08                 2.454396e+08   \n",
       "7264                 3.460033e+08                 2.610101e+08   \n",
       "7265                 3.513945e+08                 2.665934e+08   \n",
       "7266                 3.598099e+08                 2.743231e+08   \n",
       "7267                 3.640443e+08                 2.736767e+08   \n",
       "\n",
       "      Quote Asset Volume WMA(s-t)  Quote Asset Volume WMA(l-t)  \\\n",
       "14                   5.385039e+07                 4.692252e+07   \n",
       "15                   6.133098e+07                 5.100832e+07   \n",
       "16                   6.118190e+07                 5.120104e+07   \n",
       "17                   6.079130e+07                 5.129117e+07   \n",
       "18                   6.052418e+07                 5.148056e+07   \n",
       "...                           ...                          ...   \n",
       "7263                 2.847238e+08                 2.338055e+08   \n",
       "7264                 2.997360e+08                 2.425508e+08   \n",
       "7265                 2.995020e+08                 2.437006e+08   \n",
       "7266                 2.988178e+08                 2.446915e+08   \n",
       "7267                 2.989510e+08                 2.462419e+08   \n",
       "\n",
       "      Quote Asset Volume EMA(s-t)  Quote Asset Volume EMA(l-t)  \\\n",
       "14                   5.088904e+07                 4.823619e+07   \n",
       "15                   5.708226e+07                 5.153106e+07   \n",
       "16                   5.651619e+07                 5.142968e+07   \n",
       "17                   5.583673e+07                 5.125363e+07   \n",
       "18                   5.537884e+07                 5.117483e+07   \n",
       "...                           ...                          ...   \n",
       "7263                 2.681329e+08                 2.396102e+08   \n",
       "7264                 2.788279e+08                 2.461256e+08   \n",
       "7265                 2.763165e+08                 2.459543e+08   \n",
       "7266                 2.736185e+08                 2.456057e+08   \n",
       "7267                 2.719585e+08                 2.457131e+08   \n",
       "\n",
       "      # of Hourly Trades SMA(s-t)  # of Hourly Trades SMA(l-t)  \\\n",
       "14                   42578.818182                 34703.190476   \n",
       "15                   46326.636364                 37134.904762   \n",
       "16                   46372.181818                 37499.666667   \n",
       "17                   46437.636364                 38235.047619   \n",
       "18                   45091.909091                 38280.333333   \n",
       "...                           ...                          ...   \n",
       "7263                333241.090909                264155.857143   \n",
       "7264                351143.545455                276619.619048   \n",
       "7265                352912.090909                281973.809524   \n",
       "7266                356442.363636                288417.476190   \n",
       "7267                356418.818182                287496.809524   \n",
       "\n",
       "      # of Hourly Trades WMA(s-t)  # of Hourly Trades WMA(l-t)  \\\n",
       "14                   37351.990769                 34250.219592   \n",
       "15                   40559.572308                 35978.038367   \n",
       "16                   40681.150769                 36138.395918   \n",
       "17                   40997.175385                 36409.913469   \n",
       "18                   40469.544615                 36260.440816   \n",
       "...                           ...                          ...   \n",
       "7263                299259.135385                258877.006531   \n",
       "7264                309293.301538                264514.336327   \n",
       "7265                308363.575385                264699.239184   \n",
       "7266                306429.458462                264447.982041   \n",
       "7267                305749.643077                264984.405714   \n",
       "\n",
       "      # of Hourly Trades EMA(s-t)  # of Hourly Trades EMA(l-t)  \\\n",
       "14                   36203.694194                 34712.753446   \n",
       "15                   38794.047915                 36104.003327   \n",
       "16                   38653.911387                 36124.279074   \n",
       "17                   38721.917294                 36246.683244   \n",
       "18                   38102.122808                 36011.452787   \n",
       "...                           ...                          ...   \n",
       "7263                287444.342881                266612.935595   \n",
       "7264                293841.120022                270639.937816   \n",
       "7265                290795.912021                269864.870995   \n",
       "7266                287108.451219                268679.323719   \n",
       "7267                284846.487805                268144.829798   \n",
       "\n",
       "      TB Base Volume SMA(s-t)  TB Base Volume SMA(l-t)  \\\n",
       "14                 667.980733               523.108937   \n",
       "15                 755.511302               574.313600   \n",
       "16                 749.047102               578.940397   \n",
       "17                 723.869733               586.969392   \n",
       "18                 717.097553               593.600185   \n",
       "...                       ...                      ...   \n",
       "7263              7719.031358              5922.211025   \n",
       "7264              8351.050474              6307.014359   \n",
       "7265              8499.964074              6440.682723   \n",
       "7266              8717.300795              6640.637899   \n",
       "7267              8818.509015              6638.899041   \n",
       "\n",
       "      TB Base Volume WMA(s-t)  TB Base Volume WMA(l-t)  \\\n",
       "14                 568.490056               496.389445   \n",
       "15                 642.257435               536.674026   \n",
       "16                 638.110000               537.245217   \n",
       "17                 633.559943               537.925198   \n",
       "18                 629.825058               539.442647   \n",
       "...                       ...                      ...   \n",
       "7263              6870.308437              5636.231206   \n",
       "7264              7241.301132              5852.376963   \n",
       "7265              7246.012675              5886.287833   \n",
       "7266              7248.011548              5921.057685   \n",
       "7267              7244.882583              5956.192795   \n",
       "\n",
       "      TB Base Volume EMA(s-t)  TB Base Volume EMA(l-t)  \\\n",
       "14                 539.005303               519.356741   \n",
       "15                 599.784245               551.471662   \n",
       "16                 591.689101               548.950470   \n",
       "17                 584.396231               546.652041   \n",
       "18                 579.004596               545.164788   \n",
       "...                       ...                      ...   \n",
       "7263              6466.032626              5785.128385   \n",
       "7264              6731.865562              5946.107636   \n",
       "7265              6680.334981              5946.548988   \n",
       "7266              6631.876987              5946.787474   \n",
       "7267              6586.081528              5946.723944   \n",
       "\n",
       "      TB Quote Volume SMA(s-t)  TB Quote Volume SMA(l-t)  \\\n",
       "14                3.129876e+07              2.453851e+07   \n",
       "15                3.527950e+07              2.686729e+07   \n",
       "16                3.492695e+07              2.705957e+07   \n",
       "17                3.368766e+07              2.741787e+07   \n",
       "18                3.332196e+07              2.771586e+07   \n",
       "...                        ...                       ...   \n",
       "7263              1.586547e+08              1.217557e+08   \n",
       "7264              1.715141e+08              1.295853e+08   \n",
       "7265              1.745082e+08              1.322654e+08   \n",
       "7266              1.789170e+08              1.363257e+08   \n",
       "7267              1.809423e+08              1.362389e+08   \n",
       "\n",
       "      TB Quote Volume WMA(s-t)  TB Quote Volume WMA(l-t)  \\\n",
       "14                2.663726e+07              2.332155e+07   \n",
       "15                2.999135e+07              2.515305e+07   \n",
       "16                2.976831e+07              2.516110e+07   \n",
       "17                2.953348e+07              2.517777e+07   \n",
       "18                2.934479e+07              2.523771e+07   \n",
       "...                        ...                       ...   \n",
       "7263              1.411798e+08              1.161054e+08   \n",
       "7264              1.486906e+08              1.204690e+08   \n",
       "7265              1.487115e+08              1.211070e+08   \n",
       "7266              1.486940e+08              1.217723e+08   \n",
       "7267              1.485654e+08              1.224429e+08   \n",
       "\n",
       "      TB Quote Volume EMA(s-t)  TB Quote Volume EMA(l-t)  14 period RSI  \\\n",
       "14                2.528961e+07              2.443322e+07      19.700818   \n",
       "15                2.804882e+07              2.588993e+07      14.284485   \n",
       "16                2.764697e+07              2.575652e+07      13.359474   \n",
       "17                2.728924e+07              2.563667e+07      22.692087   \n",
       "18                2.702758e+07              2.555831e+07      34.139724   \n",
       "...                        ...                       ...            ...   \n",
       "7263              1.330367e+08              1.191801e+08      34.477556   \n",
       "7264              1.383985e+08              1.224313e+08      33.701986   \n",
       "7265              1.372723e+08              1.223994e+08      33.481296   \n",
       "7266              1.362250e+08              1.223705e+08      37.616040   \n",
       "7267              1.352278e+08              1.223325e+08      35.885754   \n",
       "\n",
       "            MACD      SIGNAL  14 period STOCH %K            MFV  \\\n",
       "14    -84.118036  -36.699431            3.087861   -3952.008488   \n",
       "15   -115.697415  -52.956627           10.188234   -4985.186302   \n",
       "16   -142.962820  -71.372555           10.015561   -5483.536566   \n",
       "17   -153.944946  -88.189989           17.890161   -5569.254985   \n",
       "18   -148.097173 -100.346622           29.689397   -5362.498848   \n",
       "...          ...         ...                 ...            ...   \n",
       "7263  -48.895625  -35.908972           19.246396  147193.189036   \n",
       "7264  -58.239611  -40.375100           16.369139  139042.802561   \n",
       "7265  -65.217215  -45.343523           15.586269  133864.555978   \n",
       "7266  -66.928370  -49.660492           22.944169  135321.336350   \n",
       "7267  -69.676106  -53.663615           17.644362  131090.611408   \n",
       "\n",
       "      14 period ATR          MOM  14 period MFI       ROC            OBV  \\\n",
       "14       346.417756  -574.943189      29.295680 -1.298165  -10828.228907   \n",
       "15       367.222254  -984.008583      26.765012 -1.915452  -13137.608289   \n",
       "16       384.879628 -1253.872127      22.248523 -2.061429  -14632.074914   \n",
       "17       403.371082 -1073.562226      24.147574 -1.974307  -13457.256279   \n",
       "18       413.301410  -835.553274      29.334145 -1.969773  -12360.101519   \n",
       "...             ...          ...            ...       ...            ...   \n",
       "7263     145.765396  -149.102178      31.022411 -0.556469 -693870.204086   \n",
       "7264     151.782746  -146.260262      39.928764 -0.799279 -715297.237682   \n",
       "7265     154.544854  -163.647592      38.646697 -0.817219 -730484.467639   \n",
       "7266     157.566520   -92.285657      40.227400 -0.548158 -717661.040914   \n",
       "7267     158.997389  -254.788480      36.004752 -0.745321 -730075.142941   \n",
       "\n",
       "      20 period CCI  14 period EMV       VIm       VIp     ema50     ema21  \\\n",
       "14      -146.795323  -8.651462e+06  1.107497  0.923348  0.989720  0.990214   \n",
       "15      -190.897291  -1.437937e+07  1.120331  0.823015  0.984131  0.984821   \n",
       "16      -174.411608  -1.685552e+07  1.138785  0.815338  0.983716  0.984601   \n",
       "17      -132.691139  -1.584262e+07  1.126529  0.827629  0.987350  0.988365   \n",
       "18       -91.899346  -1.099462e+07  1.103011  0.872962  0.992019  0.993086   \n",
       "...             ...            ...       ...       ...       ...       ...   \n",
       "7263    -153.341690  -1.051588e+05  1.055579  0.907324  0.992577  0.989140   \n",
       "7264    -131.783209  -5.653012e+04  1.021652  0.938704  0.992016  0.988944   \n",
       "7265    -135.168648  -6.383382e+04  1.038410  0.943717  0.991980  0.989255   \n",
       "7266    -106.811199  -7.790618e+04  1.045109  0.935714  0.993939  0.991490   \n",
       "7267    -105.789351  -9.483240e+04  1.060905  0.930096  0.992758  0.990611   \n",
       "\n",
       "         ema15      ema5   normVol  Binaryfavorites  Binaryretweets  \\\n",
       "14    0.990647  0.992877  0.595745                1               0   \n",
       "15    0.985425  0.988481  1.540271                1               1   \n",
       "16    0.985368  0.989145  0.997319                0               0   \n",
       "17    0.989234  0.993311  0.814469                1               1   \n",
       "18    0.993983  0.997910  0.793299                0               0   \n",
       "...        ...       ...       ...              ...             ...   \n",
       "7263  0.990100  0.993042  1.314560                1               1   \n",
       "7264  0.990084  0.993595  1.212615                0               1   \n",
       "7265  0.990556  0.994493  0.880098                1               1   \n",
       "7266  0.992896  0.996945  0.776355                0               0   \n",
       "7267  0.992130  0.996344  0.784036                1               1   \n",
       "\n",
       "      Binarynumber_of_followers  Binaryfollowing  \\\n",
       "14                            0                0   \n",
       "15                            0                1   \n",
       "16                            0                1   \n",
       "17                            1                1   \n",
       "18                            0                1   \n",
       "...                         ...              ...   \n",
       "7263                          1                0   \n",
       "7264                          0                0   \n",
       "7265                          1                1   \n",
       "7266                          0                0   \n",
       "7267                          0                0   \n",
       "\n",
       "      Binaryfollowers_following_ratio  Binary2x_retweets_+_favorites  \\\n",
       "14                                  0                              0   \n",
       "15                                  1                              1   \n",
       "16                                  1                              0   \n",
       "17                                  1                              1   \n",
       "18                                  0                              0   \n",
       "...                               ...                            ...   \n",
       "7263                                1                              1   \n",
       "7264                                0                              1   \n",
       "7265                                0                              1   \n",
       "7266                                0                              0   \n",
       "7267                                0                              1   \n",
       "\n",
       "      Binarypolarity  BinaryW1 Score  BinaryBull_ratio  \\\n",
       "14                 1               0                 1   \n",
       "15                 0               1                 0   \n",
       "16                 1               0                 0   \n",
       "17                 0               0                 1   \n",
       "18                 1               1                 1   \n",
       "...              ...             ...               ...   \n",
       "7263               0               1                 0   \n",
       "7264               0               0                 0   \n",
       "7265               1               1                 1   \n",
       "7266               0               0                 1   \n",
       "7267               0               0                 0   \n",
       "\n",
       "      BinaryW Score With Bull Ratio  BinaryOpen  BinaryHigh  BinaryLow  \\\n",
       "14                                1           0           0          0   \n",
       "15                                1           0           0          0   \n",
       "16                                0           0           0          1   \n",
       "17                                0           1           1          1   \n",
       "18                                1           1           1          1   \n",
       "...                             ...         ...         ...        ...   \n",
       "7263                              1           1           1          1   \n",
       "7264                              0           1           1          1   \n",
       "7265                              1           0           0          1   \n",
       "7266                              1           1           1          1   \n",
       "7267                              0           1           0          1   \n",
       "\n",
       "      BinaryClose  BinaryVolume  BinaryQuote Asset Volume  \\\n",
       "14              0             0                         0   \n",
       "15              0             1                         1   \n",
       "16              1             0                         0   \n",
       "17              1             0                         0   \n",
       "18              1             1                         1   \n",
       "...           ...           ...                       ...   \n",
       "7263            1             0                         0   \n",
       "7264            0             1                         1   \n",
       "7265            1             0                         0   \n",
       "7266            1             0                         0   \n",
       "7267            0             1                         1   \n",
       "\n",
       "      BinaryNumber of Trades  BinaryTB Base Volume  BinaryTB Quote Volume  \\\n",
       "14                         0                     1                      1   \n",
       "15                         1                     1                      1   \n",
       "16                         0                     0                      0   \n",
       "17                         1                     1                      1   \n",
       "18                         0                     1                      1   \n",
       "...                      ...                   ...                    ...   \n",
       "7263                       0                     0                      0   \n",
       "7264                       1                     1                      1   \n",
       "7265                       0                     0                      0   \n",
       "7266                       0                     0                      1   \n",
       "7267                       1                     0                      0   \n",
       "\n",
       "      Binary3MovingAverage  Binary5MovingAverage  BinaryJMJ_3HMoving_averages  \\\n",
       "14                       0                     0                            0   \n",
       "15                       0                     0                            0   \n",
       "16                       0                     0                            0   \n",
       "17                       0                     0                            0   \n",
       "18                       0                     0                            1   \n",
       "...                    ...                   ...                          ...   \n",
       "7263                     0                     0                            0   \n",
       "7264                     0                     0                            1   \n",
       "7265                     1                     0                            1   \n",
       "7266                     1                     0                            1   \n",
       "7267                     1                     1                            0   \n",
       "\n",
       "      BinaryJMJ_5HMoving_averages  Binary14 period RSI  BinaryMACD  \\\n",
       "14                              0                    0           0   \n",
       "15                              0                    0           0   \n",
       "16                              0                    0           0   \n",
       "17                              0                    1           0   \n",
       "18                              1                    1           1   \n",
       "...                           ...                  ...         ...   \n",
       "7263                            0                    1           0   \n",
       "7264                            0                    0           0   \n",
       "7265                            0                    0           0   \n",
       "7266                            1                    1           0   \n",
       "7267                            1                    0           0   \n",
       "\n",
       "      BinarySIGNAL  Binary14 period STOCH %K  BinaryMFV  Binary14 period ATR  \\\n",
       "14               0                         0          0                    0   \n",
       "15               0                         1          0                    1   \n",
       "16               0                         0          0                    1   \n",
       "17               0                         1          0                    1   \n",
       "18               0                         1          1                    1   \n",
       "...            ...                       ...        ...                  ...   \n",
       "7263             0                         1          0                    1   \n",
       "7264             0                         0          0                    1   \n",
       "7265             0                         0          0                    1   \n",
       "7266             0                         1          1                    1   \n",
       "7267             0                         0          0                    1   \n",
       "\n",
       "      BinaryMOM  Binary14 period MFI  BinaryROC  BinaryOBV  \\\n",
       "14            0                    0          0          0   \n",
       "15            0                    0          0          0   \n",
       "16            0                    0          0          0   \n",
       "17            1                    1          1          1   \n",
       "18            1                    1          1          1   \n",
       "...         ...                  ...        ...        ...   \n",
       "7263          1                    0          0          1   \n",
       "7264          1                    1          0          0   \n",
       "7265          0                    0          0          0   \n",
       "7266          1                    1          1          1   \n",
       "7267          0                    0          0          0   \n",
       "\n",
       "      Binary20 period CCI  Binary14 period EMV  BinaryVIm  BinaryVIp  \\\n",
       "14                      1                    0          0          0   \n",
       "15                      0                    0          1          0   \n",
       "16                      1                    0          1          0   \n",
       "17                      1                    1          0          1   \n",
       "18                      1                    1          0          1   \n",
       "...                   ...                  ...        ...        ...   \n",
       "7263                    1                    1          0          1   \n",
       "7264                    1                    1          0          1   \n",
       "7265                    0                    0          1          1   \n",
       "7266                    1                    0          1          0   \n",
       "7267                    1                    0          1          0   \n",
       "\n",
       "      Binaryema50  Binaryema21  Binaryema15  Binaryema5  Prediction  \n",
       "14              0            0            0           0    0.633072  \n",
       "15              0            0            0           0    0.642624  \n",
       "16              0            0            0           1    0.636700  \n",
       "17              1            1            1           1    0.590953  \n",
       "18              1            1            1           1    0.476874  \n",
       "...           ...          ...          ...         ...         ...  \n",
       "7263            1            1            1           1    0.619953  \n",
       "7264            0            0            0           1    0.662075  \n",
       "7265            0            1            1           1    0.649348  \n",
       "7266            1            1            1           1    0.516430  \n",
       "7267            0            0            0           0    0.635571  \n",
       "\n",
       "[7254 rows x 179 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed7e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictDF.to_csv('predictions/StackingPrediction_LR_ABC_LDA_GBC_to_LR.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
